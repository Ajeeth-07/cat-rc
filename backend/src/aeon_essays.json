[
  {
    "title": "For years, I practised medicine with cool certainty, comfortable with life-and-death decisions. Then, one day, I couldn’t",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/for-an-anaesthesiologist-intuition-stands-between-life-and-death",
    "publishedDate": "",
    "content": "Several years ago, I left my medical practice for a long vacation. On the morning of my first day back, my alarm went off. I pushed the button in and, for a few minutes, lay with the light off. Then, one at a time, I lowered my feet to the floor. The slow process that would transform me back into an anaesthesiologist had begun.\n\nBut something was wrong. I felt uneasy about my ability to perform my duties as a physician. Some kind of inner harmony was gone. Before my vacation, I had enjoyed the pleasure of working along a single groove, endlessly repeating surgical cases with unwearied regularity, and making snap decisions with confidence. The unexpected had never really startled me, and, at times, I even hoped for something out of the ordinary. Now something was different – off. I was filled with doubt, born of I knew not where, to which I returned unceasingly. How was this possible? One day I was perfectly fine, and now, after just a few weeks away, confidence and sureness were gone. Simply put, I had lost my professional intuition. Although that explanation may seem imprecise, intuition is real, and, without it, experts lose their bearings. What had once seemed sure and certain for them becomes a question for enquiry.\n\nResearchers have long recognised intuition’s relevance to professional judgment. In 1938, the businessman Chester Barnard wrote the now-classic book The Functions of the Executive, in which he described two distinct forms of managerial decision-making, logical and non-logical, with intuition an example of the latter. In 1957, the scholar Herbert Simon coined the phrase ‘bounded rationality’ to illustrate the same division. The psychologist Daniel Kahneman spoke of two systems for decision-making: ‘System 1’ for intuition, and ‘System 2’ for deliberate and analytical thought. In the 1960s, the neuroscientist Roger Sperry attributed logic and reason to the brain’s left side, and intuition to the brain’s right. More recently, the division has been used to distinguish between personality types, as in the Myers-Briggs personality model, where a person can be ‘intuitive’ or ‘analytical’.\n\nIntuition was pigeonholed in this way not merely to try to understand it, but also to control it. For, within the secular world, intuition is the sole survivor from those primitive days when people credited human behaviour to mystical and spiritual forces, and science was inseparable from divine doctrine. Most of those forces were elbowed out of existence in modern life, and consigned to the religious sphere. But intuitive thinking was too useful a professional tool to simply be tossed aside. Even today, more than 60 per cent of CEOs rely on intuition, or ‘gut feeling’, to guide their decisions. Under some circumstances, 90 per cent of intuitive decisions prove correct. Nevertheless, the concept of intuition threatens science, upon which much of modern professional life is based. Science uses logic, observation and measurement to find truth, while intuition, derived from the word intueri, which means ‘to look within’, seeks truth through inner contemplation. The latter method harkens back to a dark age before clarity and frankness came to dominate the realm of thought. Thus, while given its due, intuition had to be contained within a well-ordered system that downplayed its connection with mystical thinking.\n\n‘Intuition is nothing more and nothing less than recognition’ prompted by a cue, Simon wrote. In the mind sit memories stored away in oblivion, which, when cued, return to consciousness because a temporary use for them has been found. Simultaneously, a feeling of certitude arises. There is nothing miraculous about any of this, researchers insist.\n\nYet, it is when professionals lose their intuition that its mystical value shines through. For, in tough cases, when facts are lacking and the path forward is unclear, intuition arrives like a revelation. Intuition is an article of faith we assent to when reason has reached its limits. Belief in that revelation is what puts intuition on an altogether different plane of cognitive experience. There can be no relation between intuition and reason, not because they work through different sides of the brain. Instead, it is like the difference between the infinite and the finite; the infinite is out of all proportion to the finite, so that no comparison or analogy can be established between them.\n\nProfessionals who lose their intuition also sometimes lose their will to act. Faced with a difficult case, the feeling of certitude that accompanies intuition helps turn thought into action. Without it, doctors like me might hesitate to do anything at all.\n\nTo accept intuition as revelation, it’s as if the world had not yet undergone division. Belief and knowledge unite, mystery ceases to be anathema, and the science of decision-making, no longer parsed between rational and irrational, acknowledges an element that surpasses human comprehension, on par with the universe or eternity.\n\nI entered the hospital that morning with a shiver, and greeted my first patient, a woman going for in vitro fertilisation (IVF). I knew I would never mention my feeling of doubt to her. Yet, in the back of my mind, I feared I would never part with it.\n\nShe had a history of asthma. She had also been caught chewing gum, which was a problem. Gum chewing raises the risk of aspiration under anaesthesia, because it causes the stomach to secrete gastric juice. I debated whether to delay the case. On the one hand, doing so was almost unheard of, as egg retrieval must occur within a narrow time frame and prospective mothers are loathe to miss it. On the other hand, aspiration carries the risk of an asthma attack, pneumonia or even death.\n\nIn the past, I would have made the decision easily. How is that, I wondered? In regard to gum chewing, there is no exact rule. Yet, everything I used to do in the operating room, I used to do thoughtlessly; the right course seemed plain and simple. Now, no matter how much thought I gave to the matter, and however much I examined it from every possible angle, my mind kept floundering in a welter of indecision and doubt.\n\nAnxiety aroused in me wild imaginings that neither my knowledge nor my clinical experience could dispel\n\nThe patient cried when I told her we might have to delay things. The surgeon rolled his eyes. He was not out of line. As a surgeon, he valued anaesthesiologists for their acquiescence and adaptability, yet he also thought my concern too theoretical. Perhaps he was right. Life for a doctor is full of pitfalls; it is dangerous to be foolish, and it is also dangerous to be intelligent and overthink things; dangerous to others, and, no less, to oneself.\n\nI asked another anaesthesiologist for her opinion. She laughed, and said of course she would proceed. Although she probably thought less of me because of my confusion, I enjoyed our short moment together. Somehow, she was a link to my old life, when I had my intuition, and decisions flowed naturally and easily from my mind.\n\nAfter deciding to proceed, I thought briefly about giving the patient a spinal anaesthetic, so she would be numb from the waist down but awake. That would lessen her chance of aspirating. But using a spinal anaesthetic in an IVF case was almost unheard of, so I elected to put her to sleep. Yet, I had another decision to make. Should I insert a breathing tube in her airway while she was under anaesthesia? Doing so would protect against aspiration. Then again, I never had to do this for any IVF case in the past. There must have been a good reason why not. I just couldn’t remember it. What confidence I must have had in those days! I thought. I floated out loud the idea of placing a tube to see what others in the room might say. ‘You’re joking, aren’t you?’ said the nurse. I nervously smiled back and nodded, hoping to conceal the fact that I had been undecided, and pretending to be just another doctor who knew his business.\n\nThe science of decision-making can account for several of these events on perfectly rational grounds. The neuroscientist and psychologist Joel Pearson, author of The Intuition Toolkit (2024), has an acronym, SMILE, to explain them. I had violated three of his rules. The S stands for self-awareness, especially one’s emotional state. In my case, fear and anxiety had compromised my ability to think intuitively. The L stands for low-probability. In my case, I had overestimated the chances of an anaesthetic complication, similar to how people overestimate the chance of being struck by lightning during a gentle rain, which had confounded my intuition (what Pearson calls ‘misintuition’). The E stands for environment, where intuition works best in a familiar and predictable environment. In my case, being away from the hospital had caused my environment to seem unfamiliar.\n\nYet, science is incapable of creating a chiaroscuro for mental happenings. Science demands order and light, and has no place for that twilight realm where the transition from rational thought to spiritual confusion begins.\n\nThat morning, I asked myself why my medical intuition, which, over the years, had struck its roots deep into the ground, had begun suddenly to hop about from place to place. The answer was terrible: although I still had all my professional knowledge and clinical experience, something had given way in my mind, a crack had opened, and I stared through a narrow black fissure into a void. I read five journal articles that discussed gum chewing and anaesthesia. Yet, the circumstances in each study were slightly different from my own case; also, the studies disagreed with one another. Even when clustered together to intensify their warm and comforting effect, to make medical practice seem well-ordered and predictable, five journal articles offer no guarantee, no sure thing. They are like stars in the night sky that shine bright, and warm the heart, and seem to give life secure limits, but nevertheless exist in that unnamable Nothing, the alien, cold and dark universe where people come face to face with the unknown. Nothing had really happened, but my anxiety aroused in me wild imaginings, and neither my knowledge nor my clinical experience could dispel them. The anaesthesia monitors seemed to become transparent, like shadowy lights in darkness. And, when that darkness enveloped the operating room, I trembled before the infinite.\n\nThe great 20th-century violinist, Yehudi Menuhin, described a similar experience when, as a young man, he lost his professional intuition, causing his violin playing to suffer. In his memoir Unfinished Journey (1976), he wrote: ‘What had happened was a break in sequence. Between musical vision and its communication, a transition hitherto made intuitively, there occurred … a rupture which brought all to naught. Intuition was no longer to be relied on; the intellect would have to replace it.’ But intellect was not enough. He continued to play badly. ‘How many people walk badly, breathe badly, chew badly, digest badly? Such is the price of a civilisation which has left intuition behind and not yet discovered the way back,’ he moaned.\n\nHow do professionals find their intuition after losing it? How can they give themselves what they do not have? Some psychologists say intuition can be coached or taught. In The Power of Intuition (2003), the research psychologist Gary Klein says the intuitive method can be rationally communicated to others, and enhanced through conscious effort.\n\nMenuhin’s intuition finally returned to him when he accepted the limits of his own reason\n\nYet, the experience of another great intuitive violinist, Jascha Heifetz, suggests otherwise. By his own admission, he failed to understand the intuitive process, let alone teach it to others. He analysed himself religiously, sometimes watching videos of his rehearsals in slow motion to study his hands moving eight times slower. But, as the violinist Erick Friedman said about Heifetz and intuition: ‘He doesn’t understand the sources of his own genius. That is a part of his dilemma.’ Nor was he able to teach it to others. He was considered a poor instructor. One violinist said of him: ‘Heifetz is intuitive, and it is impossible to communicate intuition.’ Of all his pupils, only one became a soloist; the others became players in an orchestra where, Heifetz admitted, they just ‘contribute to the noise’.\n\nThat morning, I had tried to reclaim my old intuitive self by reviewing, in my mind, some of my old decisions. But discovering their secret ingredient proved impossible. Those older decisions were built on a continuous flux of emotions, ideas and memories, none of them beginning or ending, but all extending into each other, leading to an impulse that had moved me a certain way. When turning back and looking for that impulse, to understand it and reclaim it, I found it was gone, for the impulse was not a thing but a direction of movement, which, while simple, is indescribable.\n\nThe mystery led Menuhin into the metaphysical. He wrote, the ‘harsh whips of determination cannot drive one to performing pitch … one is led there by the quiet exercise of principles grasped by the mind and absorbed by the body over stubborn years of faith’ (my italics). He travelled to India. He absorbed the Hindu belief in one force binding all creation. He gravitated toward beliefs in hatha yoga. His intuition finally returned to him, he suggested, when he accepted the limits of his own reason. ‘When we are faced with 10 different factors, all acting upon each other and among them creating some astronomical total of variables, reason is defeated and only intuition can cope,’ he wrote. For whatever reason, on his return from India, he regained his intuition; he believed in it again, although he gave up the notion that he could understand what he believed.\n\nThe searches of Menuhin and Heifetz are eerily analogous to the path people trod centuries ago in their quest to understand some higher reality, when they questioned, hoped, studied and prayed for better understanding, and when knowledge and belief were one. Theirs was called a spiritual search. While science may try to show the absurdity of this game of God-chasing, professionals must resort to it at times; it is an inevitable phase in their mental life. Science and method can give knowledge, but knowledge alone is not enough to lead a professional into the recesses of their own mind, even when they want to go there. No synthesis of our bits of knowledge will ever equal the truth about intuition at which they aim.\n\nAfter my patient lay down on the operating-room table, I prepared to insert her intravenous. Yet, the instant I readied the needle, self-consciousness arose in me. I thought of all the precision needed to get an intravenous in, and how fragile any person’s plan for doing so really is. I began to second-guess my angle of approach. I thought about how I had not placed an intravenous in weeks. Clenching my teeth, I took aim and plunged the needle with unnecessary force into the spot. A mound of blue blood swelled just underneath the skin. I had missed the vein. I tried and missed again. The nurse stared at me wide-eyed. Fortunately, I succeeded on the third try.\n\nHere, the science of decision-making offers a good explanation for what happened. Pearson describes intuition as the learned, positive use of unconscious information for better decisions or action. To perform well, we must sometimes operate reflexively – what Pearson calls ‘blindaction’ – as when flinging out a foot to deflect a soccer ball. My error was to rely too much on conscious thought to place the intravenous, which caused my rhythm to break down.\n\nThe operation began. While working, the surgeon and nurse talked happily about how they had spent their weekend. The conversation could have amused me if I had wanted to be amused. But I did not want to be amused. I was worried about what might go wrong. The anaesthetic was amusement enough, occupation enough, torture enough.\n\nOne voice encouraged me to hold off injecting a medicine, while another prodded me to do so\n\nThe case finished 20 minutes later. While emerging from anaesthesia, the patient coughed, her airway obstructed, and her oxygen level fell. She was in laryngospasm, most likely because gastric juice had dripped out of her stomach and touched her vocal cords, causing them to snap shut – the very thing I had worried about. My best option was to give her a short-acting muscle paralytic to relax her vocal cords.\n\nBut she had a large burn scar on her leg, and the drug is contraindicated in such situations because of a possible dangerous release of potassium from the muscles. How large did the scar have to be to provoke such a release? The textbooks were not that precise. I had dealt with this conundrum before, always making the right choice and never having to resort to the safer, but longer-acting, paralytic. I wondered how I had reached those decisions. I was living backwards; I was a better doctor in the past, and from that past I was trying to extract the mindset that had produced such good decisions.\n\nIt seemed stupid not to give the short-acting drug. But a feeling within me advised otherwise. I could not make out the nature of the feeling. It was akin to a stomach upset, just as nauseating and terrible. There was fear; the drug might provoke a dangerous potassium release. There was also a queasy sensation of regret for not having used a spinal anaesthetic. Then there was embarrassment for having the complication in the first place. The feeling inside me was strong, and I couldn’t disentangle it from the more general feeling that this was an unusual situation that demanded use of the longer-acting paralytic.\n\nI also found myself yielding to the delightful idea that, if I simply did nothing and left things alone, everything would be fine. Although laryngospasm is a dangerous complication, it sometimes disappears on its own. Just let things be, another voice inside me said. Yet, I parried all the good reasons for doing so with other reasons for treating the problem. For example, the patient might suffer a lack of oxygen before the laryngospasm broke. One voice encouraged me to hold off injecting a medicine, while another prodded me to do so. I had no idea which inner voice to listen to.\n\nI sighed, I hoped, I feared, I doubted. The case lay motionless for 30 seconds, not under any command. Then, impulsively, I injected the longer-acting paralytic. Because the drug would paralyse the patient for an hour, I had to insert a breathing tube and bring her to the recovery room re-anaesthetised. There, everyone knew what I had done. I forced myself to look them in the face. When I did, I met their reproachful glances. I waited at the patient’s side for the drug to wear off. When she woke up, I removed the tube and went back to the lounge feeling very distressed.\n\nAccording to Pearson’s SMILE acronym, this new set of problems arose because I had violated the I, which stands for impulsive thinking that is mistaken for intuition. People imagine hearing within them a commanding inner voice that they wrongly credit to intuition. Still, science alone fails to expose the mistake’s deep structure. Because I lacked intuition and certitude, a void opened up inside me, which other voices rushed to fill. When we lack certitude, our minds censor, rationalise, and create ideas that seem to be relevant, but which are, in fact, invented by us. When a person is fiercely intent on hearing a particular something, it is hard not to acquiesce even slightly to just a particle of that something. And, because we cannot tolerate the idea of being unable to understand ourselves, we ineluctably make a system out of that something, and organise our thoughts to make it seem coherent. Anchored in personal bias, prejudice, vanity, hope or a moment of wilful ignorance, that system then masquerades as intuition, causing us to act impulsively.\n\nIn a difficult situation, of course, the natural thing to do is to not act, thereby avoiding injury to oneself or one’s client. Nevertheless, action is sometimes necessary, and, for that, one needs the will. When certitude cannot arise through reason to motivate the will, it arises instead through a leap of faith in one’s intuition.\n\nDwight D Eisenhower exemplified this point on the eve of the D-Day invasion. The weather was bad. Delaying the invasion risked mission secrecy, as well as cutting into the time the Allies had to campaign during good summer weather. Yet, proceeding amid wind and rain risked the absence of air cover during the landings. Another issue was whether to activate the airborne operation that would drop forces behind enemy lines. Several of his advisors had warned him that the paratroopers would be massacred. Yet, cancelling the airborne operation put the shore landings at risk. It was a ‘soul-racking problem’, Eisenhower later admitted.\n\n‘I took the problem to no one else. Professional advice and counsel could do no more,’ he wrote in his memoirs, Crusade of Europe (1948). Inside his mind, he heard different voices, all competing for his allegiance. It was as in centuries before, except then it was the gods, the unseen powers, who supposedly revealed themselves to generals in the form of whispering voices, filling their minds with fears and hopes, threatening and exhorting. He reluctantly postponed the invasion by a day. He seemed uncertain whether to proceed at all. He paced up and down the room, ‘hands clasped behind his back, chin on his chest’. Finally, after thinking things through as best he could, he reached a decision – intuitively. He looked up at his commanders and said: ‘OK, boys. We will go.’\n\nScience may explain why intuition fades, but why it returns seems mysterious\n\nWhether he had judged rightly or wrongly, Eisenhower at the time didn’t know. Nevertheless, he believed in his intuition; he had faith in it. And, with that faith came the will, that uprush of volcanic energy, that discharge from the unconscious needed to get things done. Once Eisenhower made his decision, there was no second-guessing himself, no regrets, no vacillation; just an underlying strength of determination that radiated outward to his advisors, and then to the thousands of soldiers under his command. An aroused will brings everything inside that person under one yoke, giving him or her the courage of conviction vital to action when the way forward is unclear.\n\nI, on the other hand, had acted impulsively that morning, then I had second-guessed myself in a way Eisenhower had not, which is telling. ‘Coolness’ is often the quality that distinguishes the professional with intuition from the one who lacks it. It is easiest to observe in the person’s facial expression. The expression in the former invariably reads: ‘I’ve done all I could do; now I’ll wait and see what happens; while waiting, perhaps I’ll think of something else.’ In the latter it reads: ‘Was I wrong? Oh, why did I do that? I should have done otherwise. Or maybe not?’\n\nWhen I lost my intuition that morning, it was as if something had hit me, or simply settled on me softly like a bird settling on a tree; I had grown wooden and inflexible, and lost all feeling, as if I had turned into a tree myself. Fortunately, the malady took its normal course, and my intuition returned to me by the end of the day. How or why, I do not know. Science may explain why intuition fades, but why it returns seems mysterious.\n\nThe most recognisable change, as my intuition returned, was my attitude toward doubt. During the IVF case, it was as if doubt were a person who kept tapping me on the shoulder, interrupting my thoughts, and saying: ‘I know you’re busy, and I don’t want to bother you, but I have been here all this time, and I don’t intend to leave you.’ Every time I was on the cusp of making a decision, doubt was there to convince me otherwise. I wasted much time and energy that morning wrestling with doubt.\n\nI resented doubt’s presence. I imagined myself once practising medicine doubt-free, as if the days before my vacation had been happy youth, something to be remembered as dear and pleasant because now they had passed away. But, over time, it dawned on me that there had never been a time as a doctor when I had been free of doubt. Imagining there had been was an illusion.\n\nAs I grew comfortable again living and practising medicine with doubt at my side, I felt the tide of self-consciousness flowing out of me, like a child at play. The change in mindset is comparable to what happens in chemistry when dropping liquid into what appears to be a clear, watery solution. Nothing changes, then suddenly the solution becomes saturated. One more drop and the contents come alive; instead of water, one has gleaming crystals – in my case, intuition. When the change occurred, it was as if doubt had whispered to me: ‘So now you know, I am always here. Go where you will inside the hospital, I will travel with you. I am your master, but you are also mine.’\n\nScience tends to view life as an empty, soulless place, an inert, unspiritual void. Even when it recognises irrational forces, it aspires to tame them by placing them inside a model, keeping the primal beast imprisoned behind the iron bars of knowledge. But, for an expert professional, real life is quite otherwise. The expert’s mind is not binary, nor is it wholly amenable to understanding through knowledge gained piecemeal in research. Instead, it is permeated by invisible waves that can only be sensed by the inner faculties. It is full of mysterious streams and tensions that constantly touch and enliven each other. Intuition is just another name for this heavenly harmony.\n\n",
    "scrapedDate": "2025-03-03T13:29:53.928Z",
    "wordCount": 4345
  },
  {
    "title": "Byung-Chul Han’s relentless critiques of digital capitalism reveal how this suffocating system creates hollowed-out lives",
    "category": "Childhood and adolescence",
    "url": "https://aeon.co/essays/thought-tinkering-the-korean-german-philosopher-byung-chul-han",
    "publishedDate": "",
    "content": "I came across Byung-Chul Han towards the end of the previous decade, while writing a book about the pleasures and discontents of inactivity. My first researches into our culture of overwork and perpetual stimulation soon turned up Han’s The Burnout Society, first published in German in 2010. Han’s descriptions of neoliberalism’s culture of exhaustion hit me with that rare but unmistakable alloy of gratitude and resentment aroused when someone else’s thinking gives precise and fully formed expression to one’s own fumbling intuitions.\n\nByung-Chul Han in Barcelona, Spain, in 2018. Photo by Album/Archivo ABC/Inés Baucells\n\nAt the heart of Han’s conception of a burnout society (Müdigkeitsgesellschaft) is a new paradigm of domination. The industrial society’s worker internalises the imperative to work harder in the form of superego guilt. Sigmund Freud’s superego, a hostile overseer persecuting us from within, comes into being when the infantile psyche internalises the forbidding parent. In other words, the superego has its origin in figures external to us, so that, when it tells us what to do, it is as though we are hearing an order from someone else. The achievement society of our time, Han argues, runs not on superego guilt but ego-ideal positivity – not from a ‘you must’ but a ‘you can’. The ego-ideal is that image of our own perfection once reflected to our infantile selves by our parents’ adoring gaze. It lives in us not as a persecutory other but as a kind of higher version of oneself, a voice of relentless encouragement to do and be more.\n\nWith this triumph of positivity, the roughness of the demanding boss gives way to the smoothness (a key Han term) of the relentlessly encouraging coach. On this view, depression is the definitive malaise of the achievement society: the effect of being always made to feel that we’re running hopelessly behind our own ego-ideal, exhausting ourselves in the process.\n\nThe figure of the achievement subject gives rise to some of Han’s most vivid evocations of psychic and bodily debilitation:\n\nReading this passage now, I recall how startlingly true it seemed to me on first reading. It sent me back to the early years of my professional academic life, the permanent background hum of anxious frustration, as research – at once the first and the most distant professional priority, the job’s one indisputable signal of achievement – was forever subordinated to the workaday demands of teaching, marking and committee meetings. In the scarce hours outside of those duties, I’d return to work on an article and quickly realise that I needed to comb a dozen more sources before I could begin to write it. Abruptly, I became aware of how tired I was; able neither to work nor refrain from it, I’d lie suspended in a state of weary wakefulness. That hollowed-out achievement self, ‘at war with itself’, was all too familiar.\n\nHan’s critique of contemporary life centres on its fetish of transparency; the compulsion to self-exposure driven by social media and fleeting celebrity culture; the reduction of selfhood to a series of positive data-points; and the accompanying hostility to the opacity and strangeness of the human being. This may explain why autobiographical reflection barely figures in Han’s writings: he is doubtless wary of becoming just another voice seeking to be heard in among the cacophony of opinion.\n\nBorn in Seoul in 1959, as a child Han tinkered with wires and chemicals in his bedroom, emulating his civil engineer father, who had worked on large public projects in South Korea. But these experiments came to an end after he triggered a chemical explosion in his bedroom that almost blinded him, leaving physical scars he still bears. He went on to study metallurgy.\n\nBut Han’s reading and thinking were drawing him increasingly towards Europe, and the study of philosophy. At 22, he left South Korea for Germany, telling his parents that he was continuing his scientific studies (‘they wouldn’t have allowed me to study philosophy,’ he told El País in 2023). Han arrived in Germany with almost no knowledge of the language. Yet over the years he effected a remarkable self-transformation, from Korean technophile metallurgy student to émigré German philosopher and social critic. Now, he told an interviewer in Der Zeit, his tinkering is done with the stuff of thought rather than ‘wires or soldering irons’. The metaphor conveys a sense of thinking as more an environment than an activity, a distinctly German conception of the thinker’s vocation.\n\nHan’s affinity with German thought and culture runs deep, especially in regard to its ambiguous status of Germany as at once the philosophical home of the Enlightenment and of its comprehensive critique. He is very much in the tradition of the Frankfurt School, developing for the age of digital capitalism a new chapter of its enquiry into the ‘dialectic of Enlightenment’ – that disturbing interplay between progress and atavism, and creative making and traumatic explosion, that has shaped the passage to modernity.\n\nHan’s writerly voice is melancholic in the Freudian sense of being sealed inside its own grief\n\nThese small intimations of the man and his life reverberate through his thought and prose. The tinkerer is a playful figure, bringing different chemical elements and physical forces into new and unpredictable kinds of contact. But for the boy Han, the play ended in horror that transfers directly to the later activity of thinking: ‘Thinking is also tinkering, and thinking can produce explosions. Thinking is the most dangerous activity, maybe more dangerous than the atomic bomb.’\n\nHan clarifies that his own thinking is dangerous not because it foments violence, but because it discloses a world that is ‘merciless, mad and absurd’. He is writing from inside the experience of what T W Adorno calls ‘damaged life’, in the subtitle to Minima Moralia (1951) – a book Han often quotes – or the disintegration, under advanced consumer capitalism, of cultural forms and institutions and the accompanying deformation of individual consciousness and personal relationships.\n\nHan writes as though from the damage of a near-fatal explosion – at once the conflagration in his childhood bedroom and the more generalised explosion of previous forms of life. And the damage is irreparable: ‘The time in which there was such a thing as the Other is over,’ he writes in The Expulsion of the Other (2016). Han’s writerly voice is melancholic in the strict Freudian sense of being sealed inside its own grief, conveying an absolute conviction in the consignment of self and world to a course of destruction as inevitable as it is irreversible.\n\nMusic is central to Han’s identification with German cultural tradition. He has told of his pleasure in singing Franz Schubert’s Winterreise (1827), a song cycle whose beauty is inextricably bound to its bleakness. Grieving a love lost, the singer wanders through a nocturnal winter landscape, riven by loneliness while longing for a death that will not come. Not a bad approximation, perhaps, of the Han that comes off the pages of his books, walking dejectedly through the winter of civilisation, alert to the traces of all that has been lost: the continuity of time, the grain of beauty, the tensions of eros, the substantiality of selfhood.\n\nPerhaps the other personal pleasures to which Han has alluded in interviews – tending to his garden, good food in high-end restaurants, a somewhat tentative sociability – should be seen in the context of these losses: a determination to cleave to the world of refined sensation that is being so inexorably eroded by virtual life. I’m not suggesting that Han’s books are explicitly lachrymose. Their manifest tone is more one of dry-eyed anger, rendered melancholic by the absence of any outlet or remedy for it. Under his gaze, the political, financial and technological sectors are thieves to whom we have willingly handed over our lives and selves, along with any capacity for dissent or resistance.\n\nLike his Frankfurt School predecessors, Han sees capitalism’s penetration into the deepest reaches of psychic and cultural life as the key to this phenomenon. The Burnout Society insists that power today works not through repression and persecution but by sly and insidious means of ‘self-exploitation’. In a self-administered regime of this kind, revolution is almost literally unthinkable: ‘Burnout and revolution are mutually exclusive,’ he writes later, in Capitalism and the Death Drive (2019).\n\nHan’s enquiries into the different regions of contemporary experience, including work, time, love and art, yield a remarkably consistent project of thought, a relentless critique of the spiritual and political privations of digital capitalism. The troubling question for anyone who reads widely in Han’s corpus is whether this tenaciously sustained consistency ends up becoming a symptom of what it critiques? That is, does the unbroken negativity of Han’s descriptions, his unwillingness to find anything other than loss and degradation in the forms of contemporary experience, end up reproducing the one-dimensional logic of digital capitalism itself?\n\nOne of the weirder recent innovations of the tourism and leisure industry is the immersive art experience, in which viewers are invited to stand or lounge around cavernous dark spaces bordered by giant screens, onto which are projected digitally manipulated reproductions of great paintings. Vincent van Gogh’s or Claude Monet’s brush strokes, Piet Mondrian’s colour blocks, Salvador Dalí’s melting vistas – they all float across the screens, bursting into life and disintegrating into virtual piles on the floor, before rising in swirling maelstroms to combine and recombine on the walls.\n\nEnter one of these attractions after reading Han, and it will look rather more sinister than an elaborate exercise in kitsch gimmickry, since he believes that the cultural symptoms of digital capitalism effectively degrade the very nature of experience. Han regularly invokes Walter Benjamin’s distinction between the two senses of experience concentrated in the German words Erfahrung and Erlebnis. Erfahrung denotes an experience of what philosophy calls the negative – that which is irreducibly other to consciousness. As an encounter with the new and unknown, Erfahrung is intrinsically transformative, writes Han in The Palliative Society (2020), ‘a painful process of transformation that contains an element of suffering, of undergoing something.’\n\nArt can provoke such an experience. A poem or play or painting may be what Franz Kafka called ‘the axe for the frozen sea inside us’, calling into question the ways we see, think and feel, even the way we live. It’s the kind of encounter Mark Rothko might have been alluding to when he noted that ‘a lot of people break down and cry when confronted with my pictures …’ Looked at through Han’s sensibility, Rothko’s paintings seem to cut straight through the smooth artifices of digital life, restoring contact with the tremulous realities of bodily and spiritual life from which we have so long been exiled.\n\nTo digitalise a painting is to decompose it, to deprive it of ground\n\nFor a work of art to have this effect, it must in some sense resist us, cause a disturbance of our familiar modes of language and perception. To be receptive to this kind of disturbance requires certain basic experiential conditions; we must be in an environment that permits lingering, an open-ended remaining in its presence. The paradox of lingering is that it fosters an intimacy that conveys the artwork’s irreducible strangeness. When a painting draws us towards it, we find it eludes us the closer we try to get to it. This is why we can find ourselves gazing at it for so long, often in a kind of stupefaction.\n\nImmersive Van Gogh, its creators claim, puts us inside the paintings, into a new, tactile proximity to their composition and texture. But it does so by annihilating what Han in The Scent of Time (2009) calls the ‘temporal gravitation’ of the originals, unmooring them from any location in space or time. A painting derives its meaning from the fixed relation of its spatial textural and chromatic elements, of, say, this thick band of yellow to that underlying wisp of black. This is what we call its composition. To digitalise a painting is to decompose it, to deprive it of ground.\n\nUnder the rule of digital capitalism, time itself is severed from any ‘narrative or teleological tension’, that is, from any discernible purpose or meaning, and so, like the digital paintings in an immersive show, it ‘disintegrates into points which whizz around without any sense of direction.’ In such a regime of time, there is no possibility of Erfahrung, which depends on a sense of narrative continuum and duration. There is only the proliferation of its pale counterpart Erlebnis: the discrete event that ‘amuses rather than transforms’, as Han would later put it in The Palliative Society.\n\nThe thrust of Han’s writing is, above all, philosophical. Social and cultural life are occasions for addressing metaphysical questions. As such, the surface symptoms of digital culture are secondary to its ontological premises. Like Martin Heidegger, on whose concept of Stimmung, or mood, he wrote his 1994 PhD thesis (as well as a 1999 introduction to Heidegger), he seeks to unearth the underlying metaphysics of our present-day culture. In particular, and again like Heidegger, Han is concerned with how the environment of a hyper-accelerated culture conditions the fundamental relationship between consciousness and the world.\n\nThe Burnout Society crystallised the critique of the self-exploitative logic of contemporary capitalism that Han has been elaborating ever since. Prior to that, his output had been significantly more variegated; there were books on death, Far Eastern philosophy and a study of the concept of power in the Continental philosophical tradition. However, What Is Power? (2005) is intriguing for its adumbration of a non-coercive notion of power that uncannily anticipates his conception of digital capitalism’s burnout society.\n\nThe power of capital is in the voluntary surrender of its subjects to their own exploitation\n\nBecause power so often involves coercion, Han argues, there has been a tendency to see them as inextricable. But it is only when power is poor in mediation, felt as alien to our own lives and interests, that it resorts to threatened or actual violence. Whereas when power is at the ‘highest point of mediation’ – when it seems to speak from a recognition of its subjects’ needs and desires – it is more likely to receive those subjects’ willing consent. One could conceive of a power, therefore, that has no sanctions at its disposal, but which is nonetheless rendered absolute by its subjects’ full identification with it.\n\nThe less it relies on the threat of punitive measures to back it up, the more power maximises itself. ‘An absolute power,’ writes Han, ‘would be one that never became apparent, never pointed to itself, one that rather blended completely into what goes without saying.’ This is precisely what happens in digital capitalism’s burnout society, where the power of capital consists not in its power to oppress but in the voluntary surrender of its subjects to their own exploitation.\n\nHan draws on the German-American theologian Paul Tillich’s conception of power as ipsocentric, that is, as Han puts it, centred around ‘a self whose intentionality consists of willing-itself’, cultivating and bolstering its own status. God is the ultimate embodiment of power because, in the words of G W F Hegel, ‘he is the power to be Himself’. This will to persist in one’s own existence, to cling to one’s own selfhood, is the basic premise of the Western mode of being. We can discern it at work in the empty narcissism of social media and the culture of self-display in which we’re all enjoined to participate. Self-exploitation is, in a sense, a twisted variant on the Cartesian cogito: I am seen therefore I am. In making myself perpetually visible, I may empty myself out, lose the last vestiges of my interiority. But, in cleaving to the bare bones of a self-image, some form of my existence survives.\n\nThe fundamental basis of this erosion of meaningful experience, argues Han, is felt at the level of temporality. The accelerated time of digital capitalism effectively abolishes the practice of ‘contemplative lingering’. Life is felt not as a temporal continuum but as a discontinuous pile-up of sensations crowding in on each other. One of the more egregious consequences of this new temporal regime is the atomisation of social relations, as other people are reduced to interchangeable specks in the same sensory pile-up. Trust between people, grounded in both the assumption of mutual continuity and reliability, and in a sense of knowing the other as singular and distinct, is inexorably corroded: ‘Social practices such as promising, fidelity or commitment, which are temporal practices in the sense that they commit to a future and thus limit the horizon of the future, thus founding duration, are losing all their importance.’\n\nThis corrosion of fidelity and commitment is especially evident, Han argues, in the conduct of love and relationships. Love rests on a willingness to risk not knowing, since time changes both the lovers and the world in ways they cannot anticipate. In this regard, love is the exemplary experience of the negative, a refusal of conceptual and categorical knowledge.\n\nAs Han conceives it, love has nothing to do with the cosily sentimental coupling promoted by consumer culture, in which the loved object is reduced to a narcissistic projection of the self. It is rather an encounter with radical otherness, with the pain and madness – both are implied in the word passion – that comes of risking oneself. Fixated on comfort, on the reduction of the lover to a known and unthreatening quantity, ‘Modern love lacks all transcendence and transgression,’ writes Han in The Agony of Eros (2012).\n\nThe smooth artwork travels through the perceptual field with the ease of a milkshake down the digestive tract\n\nTranscendence and transgression are twin dimensions of the negative: both involve going above and beyond the already known. Just as they are being extirpated from the erotic, so they are also losing their place in the aesthetic. Contemporary art, Han argues in Saving Beauty (2015), has become the expressive organ of a ‘society of positivity’, as manifested in the ‘smooth’ aesthetic common to iPhones, Brazilian waxes and Jeff Koons sculptures. What these apparently disparate objects have in common is the impervious gloss of their surfaces.\n\nHan specifically targets Koons in whose work ‘there exists no disaster, no injury, no ruptures, also no seams.’ By ‘seams’ he means those traces of the labour and suffering that went into its making: glitches in the easy passage from the work to its consumption. More broadly, says Han: ‘The smooth object deletes its Against. Any form of negativity is removed.’ Such negativity, or resistance, presents an obstacle to ‘accelerated communication’. This might be at the level of the material – the rough grain of the sculptor’s stone, the impasto thickness of paint, the dissonances of poetic or musical language. Or it may belong more to the substance of the work, an alienation of imagery, composition, form. Either way, relieved of any such interruption, the smooth artwork travels through its viewer’s perceptual field with the ease of a milkshake slipping down the digestive tract.\n\nThis hollowed-out flatness is equally evident in a related crisis of digital capitalism, the exhaustion of narrative forms as bearers of social meaning. In The Crisis of Narration (2023), Han echoes a now-familiar analysis. He ascribes the rise of populist nationalist movements to their leaders’ canny if cynical recognition of a public yearning for ‘meaning and identity’ in a world in which temporality has been eroded in such a way that it reduces the calendar to ‘a meaningless schedule of appointments’ and lays waste to any sense of continuity, or community.\n\nConsumer culture, with its compulsion for novelty and perpetual stimulation, likewise erodes the bonds of shared experience that engender meaningful narratives. The fire around which human beings would once have gathered to hear stories has been displaced by the digital screen, ‘which separates people as individual consumers.’ Time, love, art, work, narrative; these are the key zones of experience hollowed out by the disintegrative logic of digital capitalism. Each is a rich store of transformative encounter, or Ehrfahrung, which the ‘non-time’ of the present has reduced to empty instances of Erlebnis.\n\nIt is in Vita Contemplativa (2022) that Han ventures furthest beyond the confines of polemic to envision an alternative to the enervated politics and culture of the achievement society. The book mounts a philosophical defence of inactivity, conceived less in opposition to activity than as a possibility within it. Han cites a late fragment by Nietzsche on ‘inventive people’, which proposes that the authentically new can come into being only where there is sufficient time and freedom to think, apart from the imperatives of purpose and productivity.\n\nThis yet-to-exist Nietzschean community of the inventive echoes the German poet Novalis’s utopian imagining of a ‘republic of the living’. Novalis’s ideal of poetry is far more than a discrete literary form. It is radically expansive. For Novalis and the German Romantics, poetry is ‘a medium of unification, reconciliation and love.’ The poem’s capacity to find an image of the whole in an apparently discrete object serves as a kind of promise of the ultimate unity of part and whole, finite and infinite.\n\nThis utopian horizon is intimately bound up with the nature of poetry as a non-purposive activity. Because it has no instrumental aim, nothing in particular ‘to do’, it is capacious enough to draw into itself all of the human and non-human world, what Novalis calls ‘the world family’, without exclusion or exception.\n\nPart of the beauty of this utopic vision is surely its impossibility, and Han knows better than to propose a programme for its realisation – not least because this would require an instrumental shift from the contemplative to the active. But this impossibility leaves his work split between the unremitting darkness of the world’s reality, and the pure light of its ideal, with very little sense of any passage between the two sides of this split.\n\nOne need not have any special affinity for Koons to notice the sheer finality of Han’s condemnation of his art\n\nThis gap between the hopelessness of the existing world and the messianic perfection of an imagined one hints at a significant, if also very interesting flaw in Han’s thinking and writing, namely its tendency towards absolutist descriptions and conceptions. ‘The time in which there was such a thing as the Other is over.’ ‘The unconscious plays no part in depression.’ ‘[A] total abolition of remoteness is underway.’ These statements, each from a different book, have in common their foreclosure of any space through which another experience might intrude – a space where one might hear intimations of the Other or the unconscious or remoteness.\n\nIn this regard, they risk colluding with the suffocating conditions they describe. Han’s prose can read at times as though impelled by an inverse smoothness, a pure negativity that crowds out the possibility of otherness with a determination that mirrors uncannily the compulsory positivity he decries. In other words, it is liable to merge into the very malaise it’s lamenting.\n\nWhen set alongside two of his most insistent and important reference points, Benjamin and Adorno, it is hard to avoid contrasting the minute and exacting attention that those earlier writers bestow on individual phenomena with the summary judgment with which Han despatches them. One need not have any special affinity for Koons, for instance, to notice the sheer finality of Han’s condemnation of his art. Indeed, he doesn’t differentiate between any of Koons’s works, as though each was too bereft of singularity to warrant close analysis: ‘[Hi]s art,’ writes Han, ‘does not require any judgement, interpretation or hermeneutics, no reflection or thought.’ Koons’s floating basketballs, gargantuan animal topiary pieces and pornographic self-portraiture are only instantiations of the same banality. As Han puts it: ‘Koons says that an observer of his work should only emit a simple “Wow”.’\n\nBut pull Koons’s work away from Han’s unforgiving judgment, and it is far from clear that it abolishes the negative. Is the mirrored surface of his featureless bear silhouette merely a smooth affirmation of pop-cultural positivity? Doesn’t its very blankness present to us as an impermeable opacity? In one sense, it bears out Han’s observation that Koons’s art refuses interpretation, but not in the sense that Han himself intends. Doesn’t the sheer thisness of the piece, its silent mockery of any symbolic decoding, constitute its own negativity?\n\nRecalling that startle of recognition in my first encounter with The Burnout Society only amplifies my suspicion that Han’s polemic has become formulaic and, as such, a species of the very inattention he decries. I find myself wishing he would desist at least once from broad-brush essays on the fundamental logic of large-scale social conditions and instead zero in on a single object or phenomenon – an artwork, a place, a person. If attunement to otherness is disappearing, why not seek to revive it rather than mourn it?\n\nAs it happens, there is a strain in Han’s work that at least points to this possibility, namely his writings on the cultural tradition into which he was born. In the tellingly titled Absence (2007), Han describes the very different mode of selfhood and relationship nurtured in Far Eastern philosophy, culture and language. In contrast to the Western self’s tenacious attachment to its own desire, Han presents a self that seeks its own ‘emptying’ – ‘A wanderer is without an I, without a self, without a name.’ Where the substantiality of the Western self requires its maximal differentiation from the world – the divine power to be oneself – the Eastern self aims at a kind of oceanic merger with the world.\n\nThe marine adjective is not arbitrarily chosen. Han relates the 4th-century BCE Chinese philosopher Zhuangzi’s tale of a giant fish that lives in a dark sea of the north and transforms into a giant bird. Had this fish-bird not been giant, it would have had to muster a heroic individuality and marshal the full force of its will against the sky and sea. But its colossal size instead enables it to be borne effortlessly by the force of the waves and winds. By analogy, the mind that sets itself against the world sees their relationship only in oppositional terms. If the world is a hostile, overbearing sea, then the mind is an embattled little fish struggling to marshal all its power and cunning to avoid being beached by its currents. But if the fish is commensurate in scale with the sea, it can yield to rather than fight the waves: ‘If the mind is the sea, the sea poses no threat.’\n\nThis difference in the philosophical underpinning of selfhood extends to broader cultural differences between West and East, for example, the atmospheres of their respective cities. Western cities tend to set clear boundaries between different kinds of space, creating ‘a feeling of narrowness’. Whereas, despite their noise and congestion, the spaces and denizens of Eastern cities more typically flow into one another to live in a kind of friendly proximity: ‘They do not have much to do with each other. Rather, they empty themselves into an in-different closeness.’\n\nIt mediates between the East’s indifferent friendliness and the West’s passionate friendship\n\nFar Eastern rituals of greeting express a similarly generalised and empty friendliness. When the Western individual looks into the eyes of the other and grasps his hand, she is speaking as one bounded and differentiated self to another. This creates what Han calls a full ‘dialogical space’ spilling over with gazes, persons and words.\n\nThe Eastern bow is rather intended to empty the greeting of content, to render both its subject and its object absent to one another. Participants in a bow ‘look nowhere’, as though greeting no one in particular: ‘The grammar of bowing has no nominative or accusative, neither a subjugating subject nor a subjugated object, neither active nor passive … This absence of cases constitutes its friendliness.’ This is a friendliness distinct from the passions of friendship, where the friend is chosen on the basis of their singularity. To bring another into the inclusive zone of my friendship implies an accompanying exclusion, a choosing of this rather than that person’s companionship and love. The friendliness of the bowing ritual ciphers instead a radical universality – a love relieved of any of the prejudices of subjectivity.\n\nHan believes the German Romantic tradition to be the bearer of a similar albeit distinct conception of universal friendliness, in which all human beings may become ‘fellow citizens in a republic of the living’. It is a conception that mediates between the East’s indifferent friendliness and the West’s passionate friendship, between the universality and the singularity of others.\n\nIt seems to me that, if the German tradition carries Han’s preferred ideal of universality, it is Far Eastern thought, language and culture that enable a more playful and alive appreciation for the particular, insinuating shade and colour into prose that can seem increasingly monochromatic in tone. We might think of these two strains as the interplay of the poet and the tinkerer in showing an evident pleasure in observation and association. To quote Han, tempura batter transforms pieces of vegetable or fish into ‘a crisp agglomeration of emptiness’; in the Zen stone garden, ‘nature shines in emptiness and absence.’ Unlike the emptiness of the consumerist West that Han decries for being imposed from above by corporate masters, the emptiness of the Zen garden or the cities of the Far East is organic to the culture.\n\nHan’s 2023 El Pais interview ends with his suggestion, after the recorder has been turned off, that he and the interviewer relocate to his favourite Italian restaurant. Eating a dish of fish soup, he relaxes, jokes around, takes all the pleasure in free-flowing conversation that seemed absent in the formal interview setup. What might such an infusion of vitality and play do for his writing? Han would likely object that such glimmers of positivity would only blunt the negative edge of his thought. But I can’t help wondering if the opposite is the case.\n\n",
    "scrapedDate": "2025-03-03T13:29:55.590Z",
    "wordCount": 5010
  },
  {
    "title": "Early childhood development interventions in the Global South is a huge industry built on highly questionable assumptions",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/the-problem-with-parenting-interventions-in-the-global-south",
    "publishedDate": "",
    "content": "When the Belgian professor of psychology Robert Maistriaux wrote the above in 1955 to describe African children, he was not doing anything unusual. His words confirmed what European colonisers then wished to hear: that colonised people were somewhat cognitively deficient and needed to be rescued from themselves. The fact that ‘science’ supported this view gave only more legitimacy to the colonial project. In the 1950s, such scientific claims – linking brain deficits to inadequate childcare – seemed uncontentious.\n\nIn July 2024, the cover of The Economist portrayed a golden globe in the shape of a brain against a pink background. ‘How To Raise The World’s IQ’ boasted the title, introducing the issue of how to improve children’s brain development globally through improved nutrition and mental stimulation. It features research on malnutrition, responsive care and brain development. A careful reader won’t take long to realise that the ‘world’ mentioned in the title does not, however, represent an abstract, universal category of humanity. Here, ‘how to make the world brainier’ means how to make a certain part of the world brainier. That part of the world is the Global South.\n\nYou know that the developing brain is a truly popular topic when even The Economist, hardly a child-focused magazine, dedicates a cover to it. From colonisers in the past to the economists of today, this obsession with children’s brains – and especially with the brains of Brown, poor children – seems to continue. If anything, it has only increased – no doubt thanks to the increasing popularity of neuroscience and brain-scanning technology. This is evident in the field of early childhood development (ECD) interventions – a multibillion-dollar industry – where brain-focused programmes have gained prominence. No longer merely about ensuring physical health, many early childhood interventions have the explicit goal of improving brain development. As UNICEF puts it: ‘too many children are still missing out on the “eat, play, and love” their brains need to develop …’ Together with the World Bank, the World Health Organization (WHO), corporate foundations and NGOs, UNICEF has also placed children’s optimal brain development at the core of its agenda.\n\nIn 2018, these UN organisations launched the Nurturing Care Framework that seeks, in part, to implement interventions to improve children’s brain development in the Global South. These consist of advising and training parents in childcare practices thought to be conducive to optimal emotional and cognitive development or, in other words, to a thriving brain. These interventions are based on two simple premises. First is the idea that parental behaviour in the first years of life can alter the basic architecture of a child’s brain. As UNICEF’s Early Moments campaign claims: ‘In this formative stage of life, a baby’s brain can form more than 1 million new brain connections every single second – a pace never repeated again.’ The second is the belief that a very particular type of childcare – described as ‘nurturing’ – is conducive to sturdy brain circuits. A series of counselling cards from the WHO and UNICEF illustrates what this entails: talking and singing (even before birth), parent-child play, frequent eye contact, etc. No one would be surprised by such advice. It is the same that parents across the world can find on the internet, in popular science magazines, in the mainstream media, and even on the blog of a multinational food company like Nestlé.\n\nThe reasoning behind early childhood interventions in the Global South seems straightforward: better parenting will enhance children’s brains, which will then in turn help children become academically successful, productive and well-adjusted adults. If generations of children can be improved this way, then entire countries could achieve economic growth, peace and democracy. The UNICEF slogan ‘Building brains, building futures’ succinctly encapsulates this reasoning. What makes parenting interventions even more attractive is that they seem like a relatively simple, cost-effective way to foster development across the Global South. Due to the expected cumulative benefits across the lifespan, for instance, the World Bank estimates that ‘for every $1 spent on early childhood development interventions, the return on investment can be as high as $13.’ Or, as the Nurturing Care Framework puts it, with a poetic touch: ‘If we change the beginning of the story, we change the whole story.’\n\nAnd what could be wrong about changing the beginning of the story if this produces better outcomes for children and society? Who could argue against teaching parents to play with their children, talk frequently with them, and respond sensitively to their emotional needs? Why should we not help children, especially those from poor countries, to reach their full potential, develop better brains, and thus have a brighter future? It seems like a no-brainer.\n\nHowever, these questions make sense only if you start from two interrelated assumptions. One is that there is something fundamentally wrong with how parents in the Global South raise their children. The other is that issues like poverty, low income, unemployment, political instability or wars are somehow traceable to individual deficits. The questions thus make sense only if you accept, as Maistriaux and his colleagues thought at the time, that the problems of poor people in the Global South are their fault. Or, in this case, of their own faulty brains.\n\nThese assumptions are barely hidden in the scientific literature on the topic. For example, a 2022 ECD study published in The Lancet estimates that 74.6 per cent of all toddlers in low- and middle-income countries, 92.1 per cent of toddlers in sub-Saharan Africa, and even 99.5 per cent of the toddlers in Chad do not receive even ‘minimally adequate care’. According to these staggering statistics, almost nobody in the sub-Saharan region can provide appropriate care for their children. Another influential article in The Lancet estimates that around 200 million children under five in low- to middle-income countries ‘fail to reach their developmental potential’. These children will ‘do poorly in school and subsequently have low incomes, high fertility, and provide poor care for their children, thus contributing to the intergenerational transmission of poverty.’ Nationwide parenting interventions would, in theory, break this cycle of poverty by creating the first, fully equipped ‘breakthrough generation’. One does not need to be Frantz Fanon to see similarities with colonial narratives. While today race has been replaced by poverty, the attribution of deficits remains the same.\n\nThis story is undoubtedly attractive. The privileged of the Global North and South can find some solace in the idea that they have little responsibility for social and economic inequality. If bad childcare and poor brain development are major contributing factors to poverty, then it is mainly parents’ job to overcome it. The causal link between economic status and brain capacity also implies that the privileged owe their prosperity primarily to their optimally developed brains. Finally, early interventions offer an opportunity of doing something allegedly transformative with relatively little money and effort. While good intentions might animate these efforts, the rampant deficit view of people in the Global South has ramifications that go well beyond the sphere of early childhood. If even developmental science and prestigious UN organisations suggest that poor people in the Global South are prone to cognitive deficits, socioemotional maladjustments, criminality and high fertility, why should European countries welcome them? Xenophobic anti-immigration policies – already on the rise everywhere in countries of the Global North – seem to find legitimacy in such thinking.\n\nProponents of child removal argued that Native people failed to provide adequate home environments\n\nBut it’s not just a matter of representation. This intrinsic deficit view raises serious questions about the politics of interventions and the rigour of the scientific evidence supporting them. This is all the more important given the intrusive nature of interventions that aim to radically alter parent-child relationships. Indeed, it was not so long ago when similar claims served to justify brutal colonial projects.\n\nA good reminder is the case of the Indigenous child removals, which saw Indigenous children in the United States, Canada and Australia systematically removed from their families and placed in boarding schools or with white middle-class families. The practice continued throughout the 20th century, with the last federally funded residential school – Kivalliq Hall in Canada – closing in 1997. In their time, these schools were considered an entirely benevolent endeavour, good for them and for society. The rationale was similar to contemporary early interventions: since, apparently, ‘[I]ndigenous women did not know how to properly care for their children’, others needed to step in. The historian Margaret Jacobs has pointed out that, in the American West and in Australia, ‘many white women reformers believed it was essential to remove [I]ndigenous children, particularly girls, from their families to protect them from what white women perceived to be sexual exploitation and abuse.’ To this day, Black and Native American families are heavily overrepresented in child protection investigations and child removals.\n\nAmong other things, proponents of child removal criticised the use of the cradleboard, a baby carrier with a rigid frame, by Indigenous mothers in North America, or argued that Native people failed to provide adequate home environments. The endeavour was framed in terms of a civilising mission to produce better citizens. As an advocate argued in 1890: ‘No uncivilised people are elevated till the mothers are reached. The civilisation must begin in the homes.’ The violent and devastating consequences of Indigenous child removals are today widely recognised. But for thousands of children and families, it was too late.\n\nEarly childhood interventions today do not call for systematic child removal nor are they explicitly on a civilising mission. Their objective is innocuously described as ‘enabling young children to achieve their full developmental potential’ or ensuring optimal (brain) development. Instead of removing children from their parents’ influence, they focus on improving parents’ behaviour. Its claims are purportedly based on neutral, state-of-the-art scientific evidence. However, this science ignores two crucial questions. Who gets to define what human potential and optimal development are? And how are parenting practices and children’s abilities assessed?\n\nThe way children are cared for varies across cultures. How you feed, talk, play with and educate children is inextricably linked to local values and moral goals. Child-rearing practices vary, depending on the cultural, social, economic and environmental context, without this meaning that one form is necessarily better or worse than others. These insights of cross-cultural research are largely ignored in the scientific literature that guides early childhood interventions. These instead draw on ideas and measures of optimal development in mainstream developmental psychology, a discipline largely based on research by and with Western, especially anglophone middle-class (aka WEIRD), subjects.\n\nThis leads to an inevitable result: anything deviating from the Western norm is automatically depicted as negative. Take as an example the Lancet’s ECD study, which suggested that the vast majority of parents in the Global South don’t provide adequate care for their toddlers. This relied mainly on data from UNICEF’s Multiple Indicator Cluster Survey (MICS). The following questions measure the quality of early stimulation and learning: does the child attend an organised learning or early educational programme? Does the child’s household have at least one book and at least one toy? The parents who responded negatively to one of these questions were classified as providing ‘minimally adequate’ care. Those who answered no to both were seen as failing to provide early stimulation and learning opportunities.\n\nUsing such questions to assess early stimulation is just like using English to evaluate non-English speakers. They reflect what counts as proper stimulation in a typical Western, urban middle-class environment – those who do not use this grammar can only be found to use no grammar at all. This is because the questions don’t allow caregivers to tell us about the other multiple learning opportunities children encounter in their everyday lives. Toddlers who do not attend a formal educational programme may, for example, be routinely engaged in practices of ‘observing and pitching in’. Such experiences help young children develop attention, responsibility and cooperative behaviour. Were we to assess Western middle-class parents by asking how frequently they encourage toddlers to observe and pitch in in communal activities, they would likely be judged as performing poorly.\n\nIf parents don’t act as expected from urban middle-class Western parents, they are then inadequate caregivers\n\nChildren without a toy or a book at home may still have a rich world available for playful exploration. In many societies, parents allow their toddlers to spend all day outdoors, freely and independently exploring the real world with their siblings, cousins and peers. This provides abundant early stimulation and contributes to the development of different social skills, including autonomy and responsibility. In these contexts, parents don’t need to arrange playdates (another question used in the MICS to evaluate toddlers’ amount of peer contact). Were we to assess Western middle-class parents by asking if they let their toddlers venture alone into the streets of New York, they wouldn’t fare very well.\n\nDue to its Western, urban middle-class bias, early childhood science implicitly promotes Western, urban middle-class skills and behaviours in caregivers and children across the world. In this context, a child has reached their full potential when they behave and think like a Western, urban middle-class child. In much of the literature, this cultural bias is glossed over, and findings are presented as if they were entirely objective: if parents don’t act as expected from urban middle-class Western parents, they are then inadequate caregivers. If children do not perform well on tests measuring Eurocentric skills, based on procedures familiar to Western upper middle-class kids, they can be depicted as developmentally retarded.\n\nThe dominance of the Euro-American middle class in early childhood interventions is a direct reflection of its dominant cultural and political position. Given this hegemony, one could argue that perhaps it’s not a bad idea to give children across the world a chance to become proficient in this way of being. If English is the most important language, isn’t it good to learn English? To some extent, interventions follow precisely this logic. An important goal, for instance, is to improve children’s school readiness. The early stimulation, the educational toys and books at home, the intense parent-child verbal interactions: aren’t these suggestions helpful in preparing young children for academic success? Perhaps yes, but only under certain conditions. It may work for the elites in the Global South, who can afford intensive, time-consuming parent-child interactions, and have access to private international schools and the means to send their children to study abroad (ie, in the West). It’s questionable whether such targeted early interventions would be helpful for poor families, who may not even have access to a school that serves their needs and aspirations.\n\nBut structural issues are not even contemplated by those proposing these Western ideas. Instead of discussing whether the Western parenting style they promote is even useful under different circumstances, or whether an education system needs improvement, the starting assumption here is one of deficit.\n\nOne only needs to look at the most glamorous (and demeaning) argument of all: that people in the Global South need to become ‘brainier’. This focus on the brain is ironic since, among all types of scientific evidence, brain-based claims are the most scientifically elusive. There is no research showing that poor people in the Global South have stunted brains, as that issue of The Economist suggested. Most research about the effects of deprivation on the developing brain comes from studies of children adopted out of Romanian orphanages after the fall of Nicolae Ceausescu in 1989, where they’d had minimal human contact. Most children in the world will never experience such extreme conditions. And yet, the spectre of the stunted brain pervades public imagination. It is a spectre powerful enough for The Economist to devote an entire issue to the ‘problem’ of stunted brains in the Global South, without showing any evidence of brain studies in these countries.\n\nInstead of making grand claims based on scant evidence, perhaps we should stay with what we already know about early childhood development in the Global South: that is, quite little. In the past, similarly unsubstantiated deficit arguments about non-European children and their families justified deeply problematic interventions. We cannot make these mistakes again. Sweeping statements about stunted brains and suboptimal development should be looked at with suspicion, not make the headlines of world-leading magazines. Most important, rather than implementing top-down solutions, and conducting expensive, large-scale surveys and intervention trials, early childhood interventions should place local caretakers’ expertise, needs and perspectives at the centre of their efforts. Parents everywhere are aware of what their children need – and probably have a good sense of how to improve things based on the specific circumstances in which they live. There is no single path to achieving a thriving brain – and no single meaning to it. It is time to acknowledge this.\n\n",
    "scrapedDate": "2025-03-03T13:29:57.287Z",
    "wordCount": 2800
  },
  {
    "title": "The consulting office reflects the personality of the therapist, while also subtly shaping the experience of their patients",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/whats-the-meaning-of-the-objects-an-analyst-puts-on-display",
    "publishedDate": "",
    "content": "My psychoanalyst’s consulting room was brown and functional. There wasn’t much inside it and its walls were bare aside from Claude Monet’s The Poppy Field near Argenteuil (1873). It shows a woman in smart 19th-century clothes walking through the grass holding a parasol. Impressionism – once revolutionary – has been reproduced so much it has become neutral, inoffensive; the perfect container for the anxieties of waiting rooms and clinical offices. At that time, my mood coloured everything I saw: the scene felt grey and sombre, although, looking at it now, I notice the flashes of blue sky. It was located behind my head, in my analyst’s eyeline, so I would see it only as I approached the couch. But I spent many of my early sessions holding it in my mind’s eye. I thought often about why it had been chosen to hang there. I wasn’t ready to share my despair with this person whom I did not yet trust. So I returned to the picture obsessively as a clue as to who my analyst was and what I might expect to happen in this unfamiliar place.\n\nThe artworks in a consulting room shouldn’t matter. Psychoanalysis is the talking cure: its medium is language. Sigmund Freud’s iconic couch was given to him as a present by a patient in 1890; Freud later made it standard practice for patients to recline. He found it was easier for them (and for him) to look inwards, to speak what they found there freely, if they gazed at the ceiling rather than at someone’s face.\n\nBut the space in which the conversation happens is important. I don’t believe I am the only person who has found themselves talking while also wondering internally how a particular print, painting or picture got there. Objects prompt associations, memories, thoughts, feelings, fantasies, imaginings – everything, in short, that is the raw material of psychoanalytic work. By virtue of being placed in the vessel of meaning-making that is the consulting room, artworks of all kinds – whatever they are – gain new value.\n\nTo understand the significance of art in the psychoanalyst’s office, it’s necessary to consider first Freud’s own space. Freud was a devoted collector who assembled more than 2,000 antiquities, all of which were shipped – together with his couch – to 20 Maresfield Gardens in Hampstead, north London (now the Freud Museum) when the Freuds fled Vienna from the Nazis in 1938. Neolithic tools, ancient Egyptian vessels, Greek and Roman figurines, a bronze model of a porcupine were on his desk. The objects gave him immense satisfaction. They drove the development of his theories, feeding his interest in the ways that the past is preserved within the present, in particular how behaviour from childhood is repeated in adulthood. Freud diverted his patients’ attention from his ceiling to show them his favourite things, and he deliberately exploited his possessions’ power to evoke. In Tribute to Freud (1956), the American poet H D, analysed by Freud in 1933-4, recalled him putting an ivory statue of Vishnu into her hands. She felt compelled and repelled by its ‘extreme beauty’.\n\nFigurines and artefacts on Freud’s desk. Courtesy the Freud Museum, London\n\nHow a patient responds to the psychoanalyst’s art depends on their personal history; it will always be particular. But psychoanalysts must engage with a problem universal to their profession: how to set up their room, ready for patients to come and to project and to fantasise. The British Psychoanalytic Society, divided into Freudians, Kleinians and Independents, does not offer any guidance on the matter. Clinicians are free to set up their rooms as they like, just as they come to their own accommodation with Freud’s ideas, each finding their own space within the profession. In the autumn of 2024, I spoke to a psychoanalyst from each branch of the Society about the artworks and objects in their offices. Here are three case studies of how the psychoanalyst’s theoretical approach and personality – their tastes, preferences, histories – leave physical traces on the consulting room.\n\nDr T’s dog lays her head on my knee and looks up at my face, confident of my adoration. He calls her to him. She trots off at once to sit by his feet, settling down on the carpet momentarily before he decides to send her out for tracking in mud on her paws.\n\nI’ve begun my investigation with a Freudian, hoping to follow the lineage of psychoanalysis directly. It’s an early evening in November, cold and dark on the other side of the bay window. We’re in Dr T’s well-lit consulting room in a brick house in Battersea, south London, with a bell marked ‘Office’ by its front door. There is a piano, its lid closed. Dr T gets up to show me the glacier-blue glassware that lines the mantelpiece above a marble fireplace. His own analyst collected glass too, he tells me, his hand on a vase that is squat and reassuringly weighty in form. There is a small statue of Freud on the mantlepiece beside the glass. It’s a copy of the statue of Freud that stands outside the Tavistock Clinic, made by the Croatian sculptor Oscar Nemon around 1970.\n\nSigmund Freud bronze statue (c1970) by Oscar Nemon, in the grounds of the Tavistock Clinic, London. Photo courtesy Wikimedia\n\nWe move right to the bookshelves. Dr T brings down what he calls ‘bits of history’ from them, one by one. A rock cracked into a neat geometric shape by the movement of sea ice. A silver salt cellar in the shape of a Viking ship. A miniature shoe made out of brass (‘Needs polishing.’)\n\nDr T tells me he didn’t set out to create ‘my little Freud space’, although, as I point out, the collection of little antiques on the shelves reminds me of Maresfield Gardens. At first, he felt nervous about putting his personal possessions on show. ‘Years ago, I was much more cautious, but now I’m not so bothered. Look at that mantelpiece – it’s cluttered.’\n\nThere are pictures on every wall. ‘They just happen to be prints I like. Clearly, it must represent something about me, because I’ve chosen them – I’m looking around now to remind me – yes, I did choose them all,’ he says. ‘They speak to me in some way. Even if I didn’t know at the moment that I acquired them, I didn’t know what they were speaking, something evolves in my personal relationship with them over the years.’\n\nA nimbus floats above their head. Is the figure a builder, an angel, both, neither?\n\nAt the top of the couch is an abstract black-and-white print that, it takes me a moment to work out, shows a waterfall. Its position directly above the patient’s head suggests the cascade of drives and desires that Freud believed made up the mind – perhaps also the flow of speech in free association, the cornerstone of Freud’s technique, where patients are encouraged to share words, thoughts and memories spontaneously, without censorship or criticism.\n\nWhen prone on the couch, patients can’t see the waterfall. They look instead at a 1985 print by the Scottish artist Bruce McLean, one of a series of 90. Dr T chose it at the Tate Gallery (now Tate Britain) as a leaving present from a clinic he worked at where he had been very happy. That year, 1985, the Tate had put on McLean’s performance Good Manners and Physical Violence, which parodied good manners by repeating ostensibly polite gestures until they became aggressive. At the time, Dr T didn’t know who McLean was. But he knew the print ‘felt different’ to the others.\n\nGood Manners and Physical Violence (1985) by Bruce McLean. Courtesy Department for art\n\nThe image’s background is a bright teal, rendered in thick brushstrokes. A figure scratched out in white lines stands in front of a black rectangle, which evokes a door. A ladder leans to the left of the door. The figure carries a long roll of something under their arm. (In Good Manners, McLean used similar props to represent the everyday hard labour of decorum.) A nimbus floats above their head. Is the figure a builder, an angel, both, neither? The loose outline of a large face – closed eyes, nose, lips – are layered over the top of the top of the enigmatic scene.\n\nWhen Dr T brought it back to the clinic to show his colleagues their present, ‘people gathered around speculating on its “meaning”.’ An elderly colleague in the group stated confidently: ‘I think it’s about death.’ Dr T didn’t ask him to extrapolate, but says the claim stayed with him. Its validity has become more apparent to him over the years, as he’s begun to imagine the image’s subject passing through the door and into another dimension.\n\n‘I have one particular patient who says: “I really don’t like that” and there it is. He has to look at it every time he’s here,’ he tells me. ‘When a patient comments on the room, they’re commenting on the person of the analyst.’ It is, Dr T suggests, a way of asking the analyst: So, who are you exactly? ‘Or announcing their perception of them, without addressing the analyst directly, but by displacement.’\n\nSuch encounters are inevitable, Dr T thinks. He makes reference to the epistemological instinct, Freud’s term for the innate appetite for knowledge: ‘the instinct to know, to find out, which can be a positive thing – you know, of enquiry and wanting to learn – but it can also be intrusive and have a more destructive aspect to it.’ It’s one of the currents that feeds into the waterfall.\n\nWhile Freud’s passion for objects was uncontained, one branch of his intellectual successors proposed that analysts should show much more restraint in how they furnish the consulting room – the Kleinians, who work with the ideas of the Austrian-British analyst Melanie Klein.\n\nBob Hinshelwood worked as a Kleinian analyst in London from 1972 to 2002. He’s since retired and now lives in Norfolk, so I meet him over a video call, which he takes sitting on a sofa upholstered in a woven beige fabric. The plain wall above his head takes up half of the frame. The screen’s reflected light catches occasionally against his glasses, forming white panes in front of his eyes as he gossips about goings-on in early 20th-century Vienna with a lively immediacy. We discuss the early analysts’ entanglements in the lives of their patients, and Hinshelwood brings up Freud’s one-time ally Sándor Ferenczi who, he says with a big laugh, ‘wasn’t sure whether to marry his patient or marry his patient’s mother!’ (He married the mother.)\n\nAs he explains to me, Freud proposed that the ego developed during childhood as the child passed through the psychosexual stages – oral, anal, phallic. Klein revised this: drawing on her experiences treating children, she proposed that mental and emotional development begins from infancy, propelled by the conflicting feelings of envy and gratitude that the baby feels towards their mother.\n\nBased on their earliest experiences, Klein said, infants form an internal relational system that they carry into adulthood, unconsciously applying it to the world. Klein’s writing, which attempts to reconstruct the baby’s inner world, is engaging but difficult. Hinshelwood’s book A Dictionary of Kleinian Thought (1989) communicated the concepts from her work clearly; his Clinical Klein (1994) mapped out how they could be applied in the consulting room.\n\nSome Kleinians believe that to access the unconscious the consulting office space needs to be clear, uncluttered. During a series of clinical supervisions conducted in Brazil (and later published), Klein’s analysand Donald Meltzer, a prominent analyst in his own right, told the group that they should have nothing personal in their rooms, only a comfortable couch and chair. Meltzer later changed his mind, reflecting that he had made the recommendation following a bereavement, and that the advice was a product of his desolate state of mind. But, by then, the idea had taken hold.\n\nThe names on the spines could be read clearly from a distance. Patients said titles aloud occasionally\n\nHinshelwood’s consulting room was in Notting Hill, west London. It was in a side annex of his family house, accessed through a basement entrance. The close proximity between his work and living spaces meant, he says, that patients always had a ‘sense of something up there’ – Hinshelwood’s life, his family – as opposed to what was happening ‘down there’ in the room. Hinshelwood didn’t mind this. ‘To try to conceal oneself behind an ambiance of work and depersonalisation I would say is inappropriate. You always are a person, however your patient treats you.’\n\nHis room was filled with books. On the couch, patients’ eyes rested on a bookcase mounted to the wall. It was filled with ‘learned biographies’ of philosophers like Ludwig Wittgenstein and Bertrand Russell. The names on the spines could be read clearly from a distance. Patients said titles aloud occasionally, but Hinshelwood didn’t make much of this, he tells me.\n\nHe only had one artwork: ‘a little print … quite a nice little picture.’ But it was ‘really quite unobtrusive’, he stresses. It hung on the staircase that led up to his office where he wrote his notes on patients afterwards. Only he could see it from his chair by the couch; the patient couldn’t. And Hinshelwood doesn’t remember anyone walking up to it to have a proper look.\n\nThe print was produced in London in 1833, number 65 in a series of sketches of peoples’ heads. It shows a dark-haired woman in a teal dress with enormous puffed sleeves – the crisp fabric looks like it might rustle satisfyingly when she moves about – and a scooped back that shows the back of her smooth white neck. Her head is turned back towards the viewer, her eyes coquettish. Perhaps she’s at the opera: she stands behind an ornate balcony, the velvety curtains behind her parting suggestively in fleshy-looking folds. (Hinshelwood emails me later: ‘It’s a bit seductive and suggestive – and perhaps libidinal?’)\n\nAt the print’s focal point, the woman holds each hand in an O shape, her fingertips pressed together to represent glasses or binoculars. Underneath, in capitals: ‘DO YOU UNDERSTAND?’ Hinshelwood taps out the sentence’s syllables in the air with his hand as he relays the caption to me.\n\nSo it was for you, not your patients, I say. ‘It was for me. Yes! It was to remember I was supposed to be an understanding psychoanalyst. Understanding. That’s the focus of my life, I suppose. Patients always come to analysis divided between whether they really want an understanding of what it is they don’t want to understand – what remains unconscious – on the one hand, or whether they want help in supporting their defences against something they feel they can’t bear.’\n\nSo, the print confronted Hinshelwood with the question that drove his work. But it was a question to which he knew he could never answer Yes.\n\nThere is a danger with taking an ‘intellectual approach’ to peoples’ ‘heartfelt anxieties and conflicts’, Hinshelwood says. Being confident you’ve comprehended the patient can prevent you from connecting with them. Hinshelwood tells me about the British analyst Wilfred Bion – Klein’s student – who in 1967 wrote influentially that analysts should listen to their patients without memory or desire: each session must have ‘no history and no future’. By this, he meant that, once the analyst begins to theorise, their own unconscious assumptions and expectations take hold; they lose sight of the patient’s reality.\n\nThis means understanding is a state that must be strived towards, but never arrived at. Curiosity. Never certainty. ‘Understanding is a never-done process when we are focusing on the unconscious mind … We are never done with understanding.’\n\nBefore I meet Jonathan Sklar, he tells me he’s happy to talk but will be ‘silently discreet’ when necessary. My chair cracks loudly as I shift about on it in his consulting room in West Hampstead, northwest London. The creaking chair is the same that patients occupy when they come for a consultation, the first meeting, where the fit between patient and analyst is tried out. There is a blue rug, a couch with a neat row of white cushions, two chairs, a desk, many books. Statues and small objects from China, Peru, Egypt and Africa, most of them old, are on the shelves.\n\nBy this point, the rooms are acquiring a sense of seriality. I can see how Freud’s visual – as well as his verbal and textual – legacy are repeated. ‘It’s very Freudian but it’s my Freudian … It’s what’s accrued after 42 years of being a psychoanalyst.’ How long has he been in this particular room? ‘Some while.’\n\nSklar began his career as an analyst in 1983. He’s done much to develop psychoanalytic training in eastern Europe and has written widely, including about trauma and history in Landscapes of the Dark (2011) and about creativity in The Soft Power of Culture: Art, Transitional Space, Death and Play (2024).\n\nHe is from the Independent school, whose most prominent theorists include Donald Winnicott, who developed his ideas by working with families experiencing dislocation after the Second World War. Independents tend to have an open-minded, experience-based relationship with theory, taking from both Freud and Klein as appropriate to the situation. They give weight to the mother’s consistent holding and handling, which, they believe, is what enables the baby to develop a coherent sense of self: Winnicott’s ‘good enough’ mother succeeds if she is reasonably attentive to the child, no more. I wonder if a consulting room can play the same role as the ‘good enough’ mother, providing a stable, but not rigid, environment for exploration.\n\n‘I thought, that’s a very good picture that can restfully be – to be noticed, or not be noticed’\n\nI tell Sklar I am curious about what meaning art has in the consulting room and he begins by explaining the layout of his office. ‘It’s very important to locate where the door is. So sitting there, I’m not in your way to get out the door … I have set it up with a freedom for the patient, because I’m not here to impede … The patient says “I can’t go past you.” Well, they can.’\n\nHe points out a square painting that has been with him since the beginning of his work. A textured cream wall bisects the canvas. On one side of the wall is a tiled floor on which two small potted plants sit. On the other side is a dense mat of vegetation. At least four different kinds of trees and plants curl and reach, their vitality communicated in sweeping, energetic strokes. In the distance is the upper storey of a Mediterranean-looking building, blue shutters at its windows.\n\n‘When I saw that picture a very long time ago, I liked it for me. And I thought, well, that’s a very good picture that can restfully be – to be noticed, or not be noticed. And very often it is noticed.\n\n‘Everybody who comes into this consulting room has a wall of some sort. A small one. A big one. A wall followed by another wall followed by another wall but at some point – for me – it’s a measure of, you have to actually get into the other place which is verdant. To have a more creative life you need to transcend that wall somehow. Which, in a way, is the dilemma of the patient. Whatever their problem, they can’t get to where they want to go or want to be.’\n\nAnd when is the picture mentioned by patients? Sklar won’t tell me that. But, he says, it is ‘absolutely extraordinary’ when the patient notices something of relevance in the room. ‘The object to be found is found. When it happens, the patient has a real sense of Gosh. I found that here. It’s a moment where there isn’t so much of a wall. You can get through.’\n\n‘This man thought this humble little thing was worthy of an artistic eye. That’s very relevant to psychoanalysis’\n\nOn the other side of the window and above Sklar’s head is a copperplate etching of a plant from Michael Landy’s series ‘Nourishment’ (2002). It was Landy’s first project after his Break Down (2001), in which he catalogued everything he owned – it came to 7,227 items – and then destroyed them. ‘Nourishment’ is a collection of etchings of weeds he found growing in London’s car parks, pavements and brownfield sites.\n\nCreeping Buttercup (2002) by Michael Landy, etching on paper, from Nourishment series © Michael Landy. Published by Paragon Press. Courtesy the artist, Thomas Dane Gallery and Paragon | Contemporary Editions Ltd, London. Photo: Richard Thomas & Prudence Cumming\n\n‘I was interested in the paucity of the object,’ Sklar says. ‘It’s something so humble. It’s not even to be looked at as you walk down the streets. And this man thought that this humble little innocuous thing was worthy of an artistic eye. And I think that’s very relevant to psychoanalysis, to some patients who come for analysis. That they think there is nothing much to them. And it’s probably not true. Or it’s hidden. Concealed.’\n\n‘It’s also the idea that someone coming into this room might think they have to find some big thing that they don’t know about. Some big idea or some deeply hidden thing. But it might be something minuscule, out of the sightline even.’\n\nIt’s complicated to have real flowers in the room though, Sklar says (although he likes tulips). ‘Because flowers are always a memento mori. As soon as they’re cut, their dying is enhanced.’ But, he adds, ‘there’s nothing wrong in that, because part of analysis is to have some recognition that one has to give up life at some point also.’\n\nI cross the room to lie on the couch. Stretching out, I notice the wall right in front of me is blank. ‘I’m all for that blankness as well,’ Sklar says. ‘Everywhere is not filled. I have plenty of other pictures that I could put there, but I didn’t want to do that. I wanted to have an empty space for thinking. The empty frame that can be filled somehow.’\n\nFrom the couch I can now see the row of antiques on a shelf on the opposite side of the room, behind the consultation chair. Like Dr T, Sklar has the Tavistock Freud statue. His is one of 15 maquettes (scale models) made as preparation by Nemon, which have since been circulated among analysts: ‘In time, I was given one.’\n\nSklar’s maquette manifests how the consulting room’s artworks are a medium through which the psychoanalyst works out their relationship with their profession’s past, how they handle the things that came before. Of course, Freud is there. But Sklar takes pleasure in showing me that he has positioned him according to his own liking. The model Freud is placed deliberately on an angle, so his head is turned away from Jonathan’s chair. ‘His presence is there, but he’s not eyeballing me. He’s being discreet.’\n\nIt strikes me, by contrast, that eyeballing is exactly what I’ve done. I have pried into spaces that are normally private, and subjected analysts to my gaze. But a consulting room is not complete without its patient. They give its contents their meaning by layering their personal history into the space, using it to form their own internal picture of the office in which they talk. A psychoanalyst might suggest that what I’ve provided here are really three case studies of myself – a record of whether the sky looked blue or grey that day.\n\n",
    "scrapedDate": "2025-03-03T13:29:59.272Z",
    "wordCount": 3964
  },
  {
    "title": "Since states are founded on violence and expulsion, their existence is always bound up in thorny questions about justice",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/what-exactly-does-it-mean-to-say-a-state-has-a-right-to-exist",
    "publishedDate": "",
    "content": "Demands to recognise the ‘right of a state to exist’ ring from op-ed pages to US Congressional committee rooms. This demand is most frequently encountered in the context of Israel and the framing of its wars with Palestinians and other regional forces. The rhetorical force of the question is obvious. If Israel is facing a challenge to its very ‘right to exist’ from its enemies, then criticism of Israel’s military actions must remain muted and qualified. This framing also implies a premise that existing states have a presumptive right to exist, or that to deny the right of state X or Y to exist is morally repugnant because it implies not the juridical dissolution of a state but the destruction of the people living within it.\n\nThis framing can feel like pure political rhetoric not meant to entertain a serious response or debate. It is meant to preempt any actual public debate over the past and present of Israel/Palestine. However, the demand to recognise a state’s right to exist raises a real but overlooked philosophical question: what exactly does it mean to say that a state has a right to exist?\n\nNote that the statement ‘state X has a right to exist’ is not synonymous with the statement ‘the citizens of state X have the following rights’ – to life, to civil and human rights, to cultural flourishing. The assertion locates the possession of a right in the state itself and, moreover, it stresses the right not to derivative authority (to tax, to enforce laws, to control borders) but to the prior right to exist per se. Is this a coherent claim to be pressed on behalf of any state?\n\nStates come into and pass out of existence all the time. Here is an incomplete list of states that have existed since the Second World War that no longer exist: Czechoslovakia, Yugoslavia, the Soviet Union, South Yemen, South Vietnam, West and East Germany, and the United Arab Republic. The further back in time we go, the more defunct states we find: the Third Reich, the Ottoman Empire, Prussia, the Austro-Hungarian Empire, the Russian Empire, the Polish-Lithuanian Commonwealth, the Holy Roman Empire, and so on. Other states still exist under the same name but have contracted and no longer rule the same territory (for example, Pakistan, Sudan).\n\nSo if ‘state X has a right to exist’ means that ‘if state X existed and then no longer existed, there has been some wrong done to state X’ then it follows that there is some wrong or injustice done to those states in question in that they no longer exist. I suspect few today would hold that the dissolution of Czechoslovakia, the Soviet Union, Yugoslavia or the United Arab Republic entailed a violation of the right of those states per se to exist. The idea that states are entities or persons with rights to exist separate from the rights of persons living within those states seems untenable. States often come in and out of existence with no wrong being done to the people in question per se. It thus follows that to question the right of state X qua state X to exist is not necessarily morally repugnant.\n\nThe demand to recognise the right of a state to exist is not the same as the claim ‘All persons have a right to live under a state.’ The claim is that a specific state, with a specific identity, has a right to exist. When does such a state exist? And how can we differentiate between the claims: (a) ‘persons have a right to live under a state’ and (b) ‘a specific state with a specific identity has a right to exist’?\n\nA state is a sovereign administrative apparatus that governs a given territory, controlling entry to and exit from that territory, and possessing supreme power within that territory. What is, however, meant by a state having a specific identity such that the same people and territory could be ruled by different states at different times? States have markers of unique identity, such as names and flags. But states are also defined through their constitutions – not just technical matters of the organisation and division of powers, but the way in which states represent certain populations while expressing certain identities, values and goals, and distribute membership, rights and representation.\n\nIt seems absurd to assert that the Bourbon Monarchy and the French Republic were the same state\n\nStates can cease to exist through significant-enough change in any of these fundamental aspects of their particular identity: the territory and population they rule, but also their fundamental constitutional order or regime type. The examples I have given are all quite obvious. Certain sovereign state apparatuses with specific names and constitutions ceased to govern the territories in question, and the names of those prior countries were removed from the map. In some cases, the entire previous administrative apparatus was disassembled and replaced.\n\nThis can be harder to see in cases of regime change when the territory governed by a state does not fundamentally change. But I submit that, in many cases of profound-enough revolution, it is implausible to say that the same state governs the territory and population before and after the transformation. Consider some of the most obvious cases: England in 1649, France in 1789, China in 1911 and again in 1949, Russia in 1917, and Iran in 1979. In all of these revolutions, the land and people more or less remained the same, as well as the common, vernacular name of the country, at least at the time of the revolution itself. But the regime-change in these paradigmatic cases was almost total at the level of ideology, legitimation, administration, legality and organisation of power. It seems absurd to assert that the Bourbon Monarchy and the French Republic were the same state. Certainly, if a Royalist had demanded reformers or revolutionaries between 1789-92 recognise the right of the state to exist, what they would have meant was not some state over the territory of France but specifically the monarchy.\n\nNow consider two other cases: Zimbabwe circa 1980 and South Africa between 1993-97. From 1965-79, Rhodesia existed (albeit not internationally recognised) on the same territory with the same population as Zimbabwe since 1980. With the end of colonialism and the emergence of a postcolonial order, there was continuity in territory and population, but a change in name as well as its fundamental constitution and moral foundation. Given the depth of this transformation, I regard it as most plausible to claim that Rhodesia ceased to exist as a state, and that Rhodesia and Zimbabwe are not the same state.\n\nA new name makes it easier to see the transformation, but the name change is a symptom rather than the actual determining factor. Between 1961-93, the Republic of South Africa existed (under two distinct constitutions) as a state with a specific territory and citizenry. Of course, these were the decades of apartheid. In 1993, as part of the transition from apartheid, the Republic of South Africa adopted an interim constitution that fundamentally changed the racial basis of the political order, adopting a permanent constitution in 1996 on the basis of racial equality.\n\nDid the state that existed in South Africa between 1961-93 cease to exist with the adoption of a new, racially egalitarian constitution? On the one hand, there was a continuity in the territory, population and name between 1961-93 and 1993-present. An entity called ‘The Republic of South Africa’ never ceased to exist. There is certainly some continuity in the state administrative apparatus and social order. However, I think it is wrong to say that the states that existed between 1961-93 and 1993-present are the same state. Certainly, if the apartheid regime had offered to release Nelson Mandela from prison on the condition that he and the ANC ‘recognise South Africa’s right to exist’, what they would have meant was not some state over the territory of South Africa but the existing regime.\n\nIn other words, the identity of a particular state is to a large extent constituted by its regime. A state changes its existence through a deep-enough change in its essence (its regime). The various waves of democratic transitions from fascist or communist regimes on this view ended the previous states and created new ones. Even where some state called ‘Romania’, ‘Bulgaria’, ‘Hungary’ or ‘Portugal’ continued to exist (and this would be no small matter for nationalists), the specific state governing those territories and people changed. If the preceding is plausible, to call for a state’s non-existence is often to call for regime change rather than harm to a specific population. In fact, it is often to call for the liberation of that very population.\n\nSo states can disappear in processes of reconstitution without harm or wrong being done. Let’s call this ‘wrongless state destruction’. But obviously not every transformation, destruction, dissolution or recombination of a state is harmless or wrongless. It may be the case that, all things considered, the continued existence of a particular state is the morally best state of affairs. If so, can we specify the conditions in which a state has a right to exist? In order to answer that question, we first have to clarify what we mean by a ‘right’.\n\nOne of the dominant theories of what a right is, the interest theory of rights, was elaborated by the philosopher Joseph Raz. Raz gives us the following definition of a right: ‘“X has a right” if and only if X can have rights, and, other things being equal, an aspect of X’s wellbeing (his interest) is a sufficient reason for holding some other person(s) to be under a duty.’ This gives us the conceptual components of rights-talk: to talk about a right, we need to clarify: (a) which agent we are concerned with, (b) what interest of that agent is at stake and why it is so vital or weighty, (c) which other agents may be in a position to impact that interest positively or negatively, and (d) what kinds of duties those other agents may thus be under. But these all generate questions and problems, rather than easily establishing what rights exist.\n\nThe very notion of a state having a right to exist raises two obvious dilemmas. First, is a state such a thing that can have rights and is the state’s interest or wellbeing the morally relevant factor in its existence? Second, and more importantly, which specific other agents are thought to be under a duty to recognise its existence and what does that duty require of them?\n\nFor anyone but anarchists, states can clearly have rights derived from the interests of their citizens. If states can have derivative rights (to impose security, tax, enforce laws, enter treaties), it is plausible that they can have rights to exist derived from their citizens’ interests in such an administrative apparatus existing.\n\nWho is under an obligation, and in which conditions, to recognise a specific state’s right to exist?\n\nConsider the following argument. Persons have rights to such goods as security, economic stability, rule of law, and human and political rights. These rights can be secured only through a sovereign agency with the power to secure those goods. Therefore, persons have a right to live under a state and thus a state has a right to exist derived from the interests of the persons subject to it.\n\nConsider a further argument. Persons have a right to live under a state and thus a state has a right to exist derived from the interests of the persons subject to it. Other agents wrong the persons living under a state by trying to destroy the sovereign agency that guarantees them important moral goods. Therefore, other agents are under a moral duty to not destroy this state.\n\nI find these two arguments quite plausible. However, what these arguments do not establish yet is which other agents are under which duties to recognise the right of a state justified on these grounds to exist. Who is under an obligation, and in which conditions, to recognise a specific state’s right to exist?\n\nThere are two agents clearly under such a duty. First is the state itself, specifically its rulers or officials. If a state is legitimate only if it guarantees the people under it the above-mentioned goods and rights, then the rulers and officials of a state are under an obligation to not violate said goods and rights of that people. Second are other states or powers who are bound to not undermine or harm those fundamental interests of the people ruled by a state.\n\nThe deceptive simplicity of the question of a state’s right to exist is a function of the fact that states are frequently threatened by other states for those other states’ interests. Leaving aside, for example, deep questions of (say) Ukraine’s relations with all of its citizens, it seems obvious that Ukraine has a right vis-à-vis Russia to exist. Even if some abstract entity called the ‘Ukrainian state’ is not the ultimate rights-holder over the people and territory ruled by that state until 2014 (when Crimea was annexed), a fortiori the Russian state is not the rights-holder over the people and territory of Ukraine.\n\nHowever, other states are not the only other agents against whom states claim rights to exist. They claim these rights against persons and populations. In some of these cases, it is far less obvious that these other agents (other persons) lack the interests or standing to seek the dissolution or radical transformation of a state, or that they are under a moral duty to recognise that state’s right to exist.\n\nThe two arguments I introduced above explaining the interests that can justify the existence of a state assume without argument the existence of a population on a given territory that has a shared set of interests and a possible shared will in desiring sovereign agency. Those arguments overlooked three important elements. First, they establish a population’s right to a state, but not to a particular state with a particular cultural or ethnic identity. Second, they did not address how specific populations come to live in particular territories in the first place. Finally, they did not address how specific territories come to be juridically demarcated from other territories.\n\nPopulations (demoi) are frequently formed as a result of profound, often extremely violent, demographic changes within a territory. States are almost always formed by the arbitrary and violent demarcation of a territory without the democratic consent of all populations who might have legitimate interests in this decision. Political theorists often refer to this as the ‘boundary problem’ or ‘demos problem’. Democrats want the legitimacy of a constitutional order to derive from the actual, tacit or hypothetical consent of the people to be governed in a certain way. But the delineation of the people itself is not something that can usually be decided democratically.\n\nThe examples here are too numerous to count. How did a ‘democratic people’ emerge in the settler colonial states of the Americas and Australasia but through various forms of violence, land theft, genocide, expulsion, exclusion or confinement? How were ethnic majorities established in the successor states of multiethnic empires except through frequent ethnic cleansing, population exchanges or forcible assimilation? Even in contexts where extreme violence or ethnic cleansing was not the immediate cause or consequence of state formation, the drawing of boundaries when territories are partitioned or delineated invariably involves arbitrary decisions to include or exclude that cannot be democratic all the way down.\n\nOne might respond that if almost no states are founded in moments of perfect, non-violent consent by a population that includes no deep exclusions or forced inclusions over a clear territory not disputed by any other group of people (let’s call this the ‘Iceland Standard’), then almost no state is perfectly legitimate and therefore almost no state is uniquely illegitimate. But that would be implausible and dismissive of the inherent rights of persons to justice. It is much more plausible to say that all states are ongoing projects of establishing legitimacy and approaching something like justice and consent-worthiness.\n\nThe statute of limitations on historical justice seems impossibly vague and indeterminate\n\nIf almost all states are formed through historical injustice, a state’s ‘right to exist’ is relative to the outstanding claims of justice against it. There is no single answer to how historical injustice should be remedied. Sometimes partition, secession or the formation of new borders is the most democratically plausible and least harmful answer. Sometimes, it is restoration or transfer of sovereignty over land and its resources. Sometimes, it is the right of displaced populations to return to their former homes. Sometimes, it is democratisation or regime-change within a given state. Some of these answers will require ending existing states or reconfiguring them, and some will require justice-based reforms within the territorial and constitutional confines of the existing state.\n\nIt must be noted that there are some complex philosophical questions at stake that cannot be fully resolved here. I will acknowledge one in particular: the ubiquitous problem of vagueness. Vagueness usually points to the problem of identifying when one thing (not a heap of sand) passes into another thing (a heap of sand) when no single change (the addition of a single grain) clearly marks this transformation. Vagueness afflicts two problems within the present discussion.\n\nFirst, we might say that some historical injustices are alive and salient but some are so far in the past that seeking to reverse them seems to be only to call for new and greater injustices. But what exactly the statute of limitations on historical justice is seems impossibly vague and indeterminate. The legacy of North American slavery is certainly still salient, but what about the conquest of Constantinople or the expulsion of Muslims and Jews from Andalusia?\n\nSecond, there is also a certain vagueness in identifying when a state’s internal regime-change constitutes the definitive end of one state and the emergence of another. Some constitutional changes are profound enough to do this, but not all, and it might not be possible to say exactly what changes result in this ontological transformation from one state to another. But this does not affect the claim that states never have fully resolved and permanent rights to exist in their present form that create duties of acquiescence or recognition in all populations affected by them.\n\nSince most states are founded in some form of violence, exclusion, expulsion or injustice, it is morally incoherent to say that persons or populations are thus under a duty to recognise that historical event as legitimate. At a minimum, defenders of the state would have to show that restitution for that historical injustice has been made or that preserving the present status quo is least likely to produce further injustices or atrocities.\n\nThe rhetorical force of the demand to recognise the right of a state to exist is that states cease to exist only when their populations are destroyed in large part or in whole. But that is false. States can cease to exist by being incorporated into another state, being partitioned into smaller states, or by significant-enough regime-change that transforms the essence of a state. All of these processes can happen with greater or lesser degrees of violence.\n\nIt doesn’t follow that if a state ceases to exist with no significant loss of life on the part of the host population that, therefore, no wrong has been done. States can be conquered rapidly by foreign states with relatively little loss of life, but with great harm to the conquered people’s civil, political and human rights. Regimes that lack legitimacy can be toppled by revolutions that subsequently go on to harm and wrong the people they govern to a greater extent than even the previous illegitimate regime. People(s) can be harmed and wronged by the loss of their state per se and by harm to other interests.\n\nBut people can be harmed without being wronged. And people can be harmed and wronged with those harms and wrongs being justified, all things considered. Enslavers and other elites of the Confederacy were harmed by Reconstruction, but they were not wronged. Germans expelled from Czechoslovakia after the Second World War may have been harmed and (individually) wronged but perhaps this wrong was justified, all things considered, given the legacy of the Nazi occupation.\n\nNot all instances of state destruction are harmless, wrongless or moral improvements\n\nThe crucial point is to maintain the emphasis on the fact that profound-enough changes in a state’s governing system and ideology can constitute the end of one state and the beginning of another. In the case of states that govern significant populations without giving them equal civil and political rights, or that enjoy demographic majorities because of ethnic cleansing or partition, redressing those historical injustices may result in the creation of what must be called new states. Yet this need not involve any atrocities against existing populations. Again, the paradigmatic case is South Africa, which underwent a profound regime change without mass atrocities. It is thus coherent, and often morally correct, to deny that a state has a right to exist without implying the destruction of the people living within that state.\n\nOf course, not all instances of state destruction are harmless, wrongless or moral improvements. States destruction can lead to a great many atrocities or other humanitarian catastrophes. At this very moment, as I finish this essay, the world is awakening to the collapse of the Assad regime in Syria, while a great many questions remain about what awaits that long-suffering and tormented country from its new rulers.\n\nRectifying historical wrongs or political injustices must be balanced against their humanitarian costs and a responsibility for the future being created. This, I submit, is a more honest way of speaking about the right of states to exist. That is, to say something like: ‘The humanitarian risks of correcting historical injustice and restoring the rights of those whose control over their land was illegitimately taken away are simply too great. Justice cannot be served except at a great humanitarian cost, most likely to both populations.’ That statement has as much wisdom or validity as any moral compromise in conditions of asymmetric power, domination and uncertainty, but at least reflects the moral honesty and decency of holding in view that particular states exist today as a result of very specific and very recent historical injustices and atrocities. Yet this view is never the final word. It never extinguishes the possibility of another world where oppression and dispossession give way to justice, dignity and equality. After all, we take for granted today many social and political realities that seemed beyond anyone’s imagination barely a generation ago.\n\n",
    "scrapedDate": "2025-03-03T13:30:01.511Z",
    "wordCount": 3797
  },
  {
    "title": "We can now create compelling experiences of talking with our dead. Is this ghoulish, therapeutic or something else again?",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/are-chatbots-of-the-dead-a-brilliant-idea-or-a-terrible-one",
    "publishedDate": "",
    "content": "In 1970, a 57-year-old man died of heart disease at his home in Queens, New York. Fredric Kurzweil, a gifted pianist and conductor, was born Jewish in Vienna in 1912. When the Nazis entered Austria in 1938, an American benefactor sponsored Fred’s immigration to the United States and saved his life. He eventually became a music professor and conductor for choirs and orchestras around the US. Fred took almost nothing with him when he fled Europe – but, in the US, he saved everything. He saved official documents about his life, lectures, notes, programmes, newspaper clippings related to his work, letters he wrote and letters he received, and personal journals.\n\nFor 50 years after Fred died, his son, Ray, kept these records in a storage unit. In 2018, Ray worked with his daughter, Amy, to digitise all the original writing from his father. He fed that digitised writing to an algorithm and built a chatbot that simulated what it was like to have a conversation with the father he missed and lost too soon. This chatbot was selective, meaning that it responded to questions with sentences that Fred actually wrote at some point in his life. Through this chatbot, Ray was able to converse with a representation of his father, in a way that felt, Ray said: ‘like talking to him.’ And Amy, who co-wrote this essay and was born after Fred died, was able to stage a conversation with an ancestor she had never met.\n\n‘Fredbot’ is one example of a technology known as chatbots of the dead, chatbots designed to speak in the voice of specific deceased people. Other examples are plentiful: in 2016, Eugenia Kuyda built a chatbot from the text messages of her friend Roman Mazurenko, who was killed in a traffic accident. The first Roman Bot, like Fredbot, was selective, but later versions were generative, meaning they generated novel responses that reflected Mazurenko’s voice. In 2020, the musician and artist Laurie Anderson used a corpus of writing and lyrics from her late husband, Velvet Underground’s co-founder Lou Reed, to create a generative program she interacted with as a creative collaborator. And in 2021, the journalist James Vlahos launched HereAfter AI, an app anyone can use to create interactive chatbots, called ‘life story avatars’, that are based on loved ones’ memories. Today, enterprises in the business of ‘reinventing remembrance’ abound: Life Story AI, Project Infinite Life, Project December – the list goes on.\n\nThese apps and algorithms are part of a growing class of technologies that marry artificial intelligence (AI) with the data that people leave behind. These technologies will become more sophisticated and accessible as the parameters and popularity of large language models increase and as personal data expands into the seeming permanence of the cloud. To some, chatbots of the dead are useful tools that can help us grieve, remember, and reflect on those we’ve lost. To others, they are dehumanising technologies that conjure a dystopian world. They raise ethical questions about consent, ownership, memory and historical accuracy: who should be allowed to create, control or profit from these representations? How do we understand chatbots that seem to misrepresent the past? But for us, the deepest concerns relate to how these bots might affect our relationship to the dead. Are they artificial replacements that merely paper over our grief? Or is there something distinctively valuable about chatting with a simulation of the dead?\n\nThe bonds we form with others infuse our lives with shared meaning and warmth, but these connections are threatened by time and the rupture of death. Profound loss is inevitable. This is why all human cultures have developed rituals, stories and other resources for dealing with the death of loved ones. In Book 11 of Homer’s Odyssey, also known as the Nekyia (or Book of the Dead), Odysseus journeys to the underworld and finds his dead mother, Anticlea, who has become a shade – a spirit he cannot embrace.\n\nThe ancient Egyptians’ own Book of the Dead – a collection of texts originally written on papyrus scrolls and the walls of tombs – offers incantations to guide lost souls safely into the afterlife where they might be granted immortality. Alongside such ancient stories and spells are ongoing rituals that engage the dead. In some Asian cultures, the gates of the underworld are said to open during ‘Ghost Month’, and the spirits of dead ancestors freely wander the human realm. Likewise, the Mexican Dia de Los Muertos is a time of remembrance and celebration when the living and dead can commune. Other rituals are less public. In Jewish culture, sitting shiva – which takes place in the seven days immediately after the death of a loved one – facilitates a structured space for coming together around the memory of the deceased. In Japanese culture, sacred objects like the butsudan, a small domestic Buddhist shrine for honouring deceased family members, support continuing bonds with dead ancestors.\n\nThese beliefs and cultural practices offer hope that the living might one day fulfil their desire to commune with the dead. They sustain people through grief by promoting the sense that the human community transcends loss, and that love is not destroyed as long as memory remains.\n\nWhen someone dies, the rational person is expected to close the curtain on that relationship and move on\n\nBut in the industrialised West, communities of support are collapsing. Our spiritual institutions, practices and beliefs have been denuded by centuries of disenchantment. This may suggest that technological modernity, with its commitment to scientific rationalism, is to blame for the Western world’s often-impoverished relationship to death.\n\nTo be rational in the industrialised West is to grant primacy to what can be seen, touched and measured. When someone dies, the rational person is expected to close the curtain on that relationship and move on. Instead of offering new spiritual resources, our technocultures seem to provide us with a vast array of ingenious instruments to help us turn away from death: we have medicine to postpone, entertainment to soothe, drugs to stupefy, and other technologies to help us avoid and ignore. The average person is at once inundated with stylised images of people dying while being sequestered from real, meaningful experiences with death. All the while, the technological advances that saturate our lives with increasing wealth and power whisper of a way beyond death, of new eschatologies of mechanical transcendence.\n\nYet, for all its dynamism, this technological ingenuity has not quelled the suspicion that there are no technical solutions to spiritual problems. It is often said that, rather than providing resources that meet our needs, our instruments merely help us deny the reality of death. To some, these technologies are nothing more than embalming tools, made to mask the inevitable pain of profound loss. And among these many tools, what could be more pernicious than a chatbot that speaks in the voice of those who are, in reality, gone forever?\n\nSince its inception, chatbot technology has been associated with illusion, and subject to suspicion. One of the first and most well-known chatbots, ELIZA, was created in the 1960s by the MIT professor Joseph Weizenbaum, a computer pioneer-turned-critic who later decried AI as an ‘index of the insanity of our world’. ELIZA was designed to imitate a psychotherapist, employing keyword detection and preprogrammed rules to ask users basic questions that prompted verbalisation and reflection. Although rudimentary by today’s standards, people found ELIZA compelling. When Weizenbaum’s secretary asked for alone time with the chatbot, the professor took it as evidence that she erroneously thought the machine had a mind. As a result, Weizenbaum came to believe that even simple chatbots like ELIZA ‘could induce powerful delusional thinking in quite normal people.’ Today, chatbots employ machine learning rather than preprogrammed rules and can hold sophisticated conversations by making inferences, responding to questions, tracking context and conversational presuppositions, and more. To a much greater degree than ELIZA, contemporary chatbots can produce a compelling conversational experience similar to a human interlocutor.\n\nAlthough chatbots have been around for a long time, chatbots of the dead are a relatively new innovation made possible by recent advances in programming techniques and the proliferation of personal data. On a basic level, these chatbots are created by combining machine learning with personal writing, such as text messages, emails, letters and journals, which reflect a person’s distinctive diction, syntax, attitudes and quirks. There are various ways this combination can be achieved. One resource-intensive method involves creating a new chatbot by training a language model on someone’s personal writing. A technically simpler method involves instructing a pretrained chatbot, like ChatGPT, to utilise personal data that is inserted into the context window of a conversation. Both methods enable a chatbot to speak in ways that resemble a dead person by ‘selectively’ outputting statements the person actually wrote, ‘generatively’ producing novel statements that bear some resemblance to statements the person actually wrote, or some combination of both.\n\nA chatbot of the dead that’s monetised or gamified could be dangerous in the hands of an unscrupulous corporation\n\nChatbots can be used on their own or combined with other forms of AI, such as voice cloning and deepfakes, to create interactive representations of the dead. When provided with the right data, many companies and platforms now have the technical capacity to generate a conversational AI version of your deceased loved one. In the future, these chatbots will likely become more common and sophisticated, involving much more than just text. Like human mediums and Ouija boards, these bots appear to meet one of our deepest desires: to speak with the dead once again.\n\nMany critics view this technological endeavour as an especially abject form of death denial. A common objection to these bots is that emotionally vulnerable users may become so invested in their interactions that they will conflate their chatbot with the deceased person, or lose sight of the fact that the person is gone. As the philosopher Patrick Stokes puts it in Digital Souls (2021), we may ‘become so used to avatars of the dead that we accept and treat them as if they’re the dead themselves.’ This sort of worry suggests, as Weizenbaum feared, that the salutary potential of chatbots is based in delusional thinking.\n\nAnother worry relates to chatbots’ lack of inner lives. Critics, like the philosopher Shannon Vallor in The AI Mirror (2024), argue that there is something defective about emotional bonds with entities that cannot reciprocate affection or interest, about love that is kept alive by ‘the economic rationality of exchange’ rather than a more precarious ‘union of loving feeling and action’. And these emotionally one-sided relationships create a risk of over-reliance, social isolation and exploitation. This risk is especially stark given that chatbots of the dead may be produced by companies with a financial incentive to manipulate users and maximise engagement. A chatbot of the dead that is highly monetised (unlock your chatbot’s ‘caring’ traits for a small fee!) or gamified (chat every day to level up!) could be a dangerous tool in the hands of an unscrupulous corporation.\n\nThese types of critiques often implicitly assume that the purpose of chatbots of the dead is to function as a kind of stand-in for a lost loved one, an ersatz companion of sorts. This use suggests that chatbots should be designed to be as realistic and fully formed as possible. And from this vantage point, there are two possible problems: either chatbots cannot live up to this standard, and therefore they are deceptive and defective, or they can live up to this standard, and therefore they will confuse, isolate or exploit those who use them.\n\nBut this implicit assumption is wrong. Chatbots of the dead should not be used as stand-ins or companions. Nor should they be built to be as realistic and fully formed as possible. Instead, chatbots of the dead should be made to serve a purpose that is more familiar and more complex, a purpose that can help us transcend our mortal condition, not through technological resurrection, but through imagination.\n\nAny griever, or any student of history, knows that relationships don’t end neatly with death; memories, imaginings, questions and answers continue to churn in the minds of the living, aided by artefacts and shared community. The secular space where we allow this reality to flourish is the arts.\n\n‘Everything is as it should be, nothing will ever change, nobody will ever die,’ writes Vladimir Nabokov in his memoir, Speak, Memory (1951). Conjuring memories of childhood for the reader, he evokes a vivid sensory picture:\n\nThis passage may not denote a specific event, bumblebee or book. More likely, this is a composite scene staged from different impressions. When Nabokov describes his uncle as ‘gloating’ over a book, he arrives at this description from childhood memories filtered through adult inferences – ‘gloating’ is not typically a child’s concept. Memory is a space to grasp meaning through a gauze of impressionistic details, and the arts are spaces where this stippled portrait of past life can live on.\n\nWe save these kinds of artefacts to nurture our imagination of the past and our ancestors\n\nWe live our lives through a multitude of meaningful fictions, personal memory being one of them. This form of memory is often built, like Nabokov’s scene, from a composition of impressions – blue roses on wallpaper, a bee, a book, an expression on a family member’s face – that we stitch into a narrative when recounting scenes from our lives. This stitching-together constitutes a kind of fiction that is not opposed to truth. In Mimesis as Make-Believe (1990), the philosopher Kendall Walton describes fiction as that which ‘is to be imagined’. The passage above prescribes that the reader imagine Nabokov’s room in Vyra. This room is thus part of a fictional world. Although a bookstore might classify Speak, Memory as ‘literary nonfiction’, the world it prescribes is, in Walton’s sense, fiction, just a different kind of fiction than what is prescribed by Nabokov’s novel Pale Fire (1962). Speak, Memory makes extensive use of imagination to make the reader feel the work’s proposition: that the author’s childhood was tinged with warmth and sadness. According to Walton’s framework, the passage from Speak, Memory and others like it function as ‘props’ that ‘mandate’ and ‘prompt’ imaginings. These imaginings can be described in terms of propositions (eg, a bumblebee bumps against the ceiling) that are true within a fictional world – ie, a world that a reader of Speak, Memory is to imagine. By inhabiting this kind of fictional world, we can seek truth about our past and our relationships. Art formalises these fictional worlds and the quests for truth within them.\n\nSalvaged artefacts, like diaries and letters, can also serve our quests. When Amy reads a letter from 1967, written by her grandmother to her grandfather away on business, she is transported in time, place and perspective. She imagines her grandmother Hannah writing the letter and her grandfather Fredric receiving it. She imagines how her grandfather feels learning about his wife’s 10 cent raise, the look on his face reading about how their daughter has grown. References are filled in with imagination, based on inference; the value of 10 cents in 1967, the age of Amy’s aunt. The letter is a historical artefact that serves as a Waltonian prop, determining content in the fictional world Amy constructs as she imagines the lives of her grandparents. It furthermore serves as a Waltonian prompt, prompting Amy to imagine that world and other related memories. We save these kinds of artefacts to nurture our imagination of the past and our ancestors. The more examples of ancestral voices we survey, the more enriched is our sense of their identities, the more insight we gain into our families and ourselves.\n\nBut even without tangible props or prompts, like photos and letters, we can nonetheless imagine based on looser connections. A vague story told in passing about a great-great-grandparent may serve as a prop, prompting a vision of the past – if the listener has interest in imagining it. A hat from the 1890s may prompt us to imagine an ancestor wearing it. The more detailed, specific and multi-modal our prompts and props, the closer the imagined representation of a past person may be to the once-living person.\n\nBut a representation will always remain a representation – regardless of its accuracy.\n\nLike memoirs, photographs, letters, hats and oral histories, chatbots of the dead can serve the manifold goals of our memory quests, giving context to our lives, relationships and identities as they help us forge connections across time. They can function as Waltonian props that help generate fictional worlds and prompt receptive users to enter these worlds. Therein, we can explore and continue the relationships that matter to us. Because chatbots are not people but props that help us generate imaginary conversations, the user’s interlocutor – a character based on the deceased – does not exist independently of the conversation. Chatbot users create their imaginary interlocutors through their interactions with their prop chatbots. In this way, chatbots bestow a special form of creative agency upon users, wherein users play an active role in creating fictional worlds and the fictional characters within them. Talking to a chatbot is thus like the art of improvisation. The chatbot user is simultaneously a creative artist and a receptive viewer who directs and is directed towards meaning, insight or amusement.\n\nThe complexity of these interactions is not unique. Humans are adept at roleplay: we joke, we pretend, we play. For that reason alone, Weizenbaum’s assessment of his secretary was patronising. It is uncharitable to the woman’s intelligence to assume she was, as Weizenbaum feared, delusionally attributing agency to the machine when she sought alone time with ELIZA. More likely, she was using ELIZA as it was designed: to imaginatively roleplay therapy. ELIZA was not a therapist; it was a prop, a partial entity that needed a user to animate it.\n\nThis imaginative potential is obscured when chatbots of the dead are envisioned as flimsy stand-ins for deceased loved ones. This vision is common in science fiction, like the Black Mirror episode ‘Be Right Back’ (2013) in which a grieving woman becomes attached to an AI that replaces her dead partner. And this stand-in vision is accepted by critics and also many users who, like the journalist Cody Delistraty in The Grief Cure (2024), set out to ‘re-create’ their loved ones ‘with as much fidelity as possible,’ only to feel ‘broken’ when ‘that alternate reality bursts’ and their loved one ‘dies once more’.\n\nChatbots of the dead, like participatory theatre, allow the audience to participate in fictional worlds\n\nThis is the wrong way to view chatbots of the dead. Instead, the potential of these technologies would be better understood if they were thought of more like artworks – or, rather, like theatrical performances. Engaging with a chatbot is a lot like attending a participatory theatre performance. In these performances, audience members play active roles in the story. Early forms of participatory theatre include performances at Dionysian festivals or medieval mystery plays. More modern examples include Bertolt Brecht’s attempts to break the ‘fourth wall’ during his plays, and the immersive theatrical performances where audiences freely explore the space of the theatre, interacting with actors.\n\nA chatbot based on someone’s data is like an improv actor who has studied a backstory or character sketch in order to performatively represent a character based on that person, like a Civil War soldier at a historical reenactment, an Elvis impersonator, or King Pentheus in Dionysus in 69 (1969), the participatory rendition of Euripides’ play The Bacchae. Chatbots of the dead, like participatory theatre, allow the audience to directly participate in these fictional worlds, thereby becoming imaginatively acquainted with the real person to whom the character corresponds.\n\nThis theatre analogy is not perfect. Unlike human actors, chatbots are not physically present. They do not make conscious choices about how to portray characters or co-construct meaning with their audience. Nevertheless, this framework is useful because it clarifies and accentuates the relation between chatbots and the deceased. Actors are neither identical to, nor replacements for, the real people they represent; instead, they convey information about them and a felt sense of their particularities and complexities.\n\nA discerning user should view chatbots similarly. Depending on how they are designed, chatbots can represent a person in many ways. The assumption that a chatbot delivers – or seeks to deliver – an authoritative replication of a deceased person makes as much sense as the assumption that an actor’s portrayal of a historical figure in a drama represents the sole faithful depiction of that figure. Just as the inspiration for a historical character may come from various sources, our personal data flows from different aspects of our identities. People do not really speak in a single voice: most people are different on social media than they are in text messages, for example. There is no one way to design a chatbot ‘actor’ because there is no best or definitive perspective on a person, no best or definitive fictional world in which to encounter someone’s legacy. There are countless useful, informative, intriguing, funny, strange, beautiful perspectives that a chatbot ‘actor’ might stage, just as there are countless ways a human actor can play a role, a writer compose a memoir, or a portraitist paint a picture.\n\nLet’s bring this vision to life with several imaginative possibilities. Imagine that you collaborate with engineers to create a chatbot of your deceased grandfather. You decide to build a selective chatbot to memorialise him via sentences he actually wrote or said. But you want the conversations to flow smoothly, so you include some generative functionality. You embark on the laborious task of deciding which salvaged writing to include in the dataset. It quickly becomes clear that sentences from cover letters are written in a professional voice that differs from the intimate voice of family letters. You also have audio recordings, but your grandfather’s spoken voice is different from the way he writes – a person’s multitudes are not easily sortable into clear categories.\n\nAnd so, the builder of this chatbot is forced to ask difficult questions about the deceased that call for care and attention: how do I want to remember them? How should a person be remembered? The deliberative effort that goes into building a chatbot highlights that the end product is a representation, inflected with the creator’s choices and perspectives. Ultimately, you decide you cherish your grandfather’s sense of humour, and you want your children to know about his experience fighting in the Second World War. You design two bots, trained on different material, to performatively present these two sides of your grandfather. The history bot also serves as a navigator of your family archive, linking to scans of primary documents.\n\nImagine another possibility: you’ve recently lost your partner in a tragic accident. Your loved ones have returned to their normal routines, but you’re still deep in mourning and seeking guidance and support as you plan for the future. Your partner is vivid in your mind, and it’s disorienting to live in a world in which their absence is accepted. So, you allow a tech company to use all available data from your partner to train a generative bot – you want it to be as expansive and conversant as possible. And here’s where this story diverges from Black Mirror. The tech company’s platform offers you a frame around the conversations: the bot periodically ‘breaks character’ to check in, prompting reflection on how the representation aligns with your memory, your emotional state, and the insights you’re gaining, helping you make adjustments to the bot as necessary. Using the bot becomes a process of shaping your grief and the representation. And, since you’re continuously reminded of the frame around the representation, you don’t see it as a replacement.\n\nA chatbot could play the role of a museum curator, speaking knowledgeably about a person’s archives\n\nImagine now that you are an adult who lost both your parents when you were younger. You have a family of your own, and lately you find yourself wondering about your parents’ choices. You’re interested in the wisdom of their shared perspective. Why did they make certain decisions for your family? So, you build a generative chatbot that is informed by data relating to both your parents’ thoughts on family and parenting. This includes excerpts from their writings, memories of things they said, along with passages from books and other media they valued that relates to these themes. You engage with this chatbot as a unique intelligence; it does not represent an individual, but neither is it a generic intelligence, like ChatGPT. It represents a specific collective wisdom.\n\nThese are just examples. The creative design possibilities are immense and must be explored in artistic practice. Practitioners should look to the arts and other cultural resources that help people deal with loss and memorialise history. For example, a chatbot could be designed to speak as a spiritual medium channelling the deceased from a spiritual realm in order to emphasise the separation of death and impart a sense of mysticism to the imaginative experience. Or chatbots could be designed to employ Brecht’s ‘alienation effect’, which involves acting techniques designed to inhibit immersion and promote aesthetic distance between an audience and the events on stage, making room for reflection. Alienation techniques include character breaks and metatextual discussion, as in the example involving a deceased partner. In this vein, a chatbot could be designed to play the role of a museum curator, speaking knowledgeably about the person they are representing and their archives.\n\nChatbots of public figures might inspire stylised voices of the deceased, eg, caricatures that, like Elvis impersonators, magnify idiosyncratic qualities for the purpose of parody, satire or celebration. A classroom might create a bot based on the narrator from a memoir; for example, students could continue a conversation with Nabokov after Speak, Memory ends. You might create a bot trained on your childhood journals, so you can converse with your past self. An abusive parent might be represented at their most rageful to help process childhood trauma in a therapy session. An anxious person might be represented in a way that allows users to read their worried thoughts to promote empathy and understanding. A chatbot might be trained on all available family records to speak in the voice of your ancestors. Another might be trained on all the known writing from a particular ancient village. Yet another might be trained on all the works of a prolific lost writer: Plato, James Baldwin, Ursula K Le Guin, or a group of writers from a particular point in history: the Beats, the Transcendentalists, the members of OuLiPo. The possibilities are endless. This technology can help us appreciate the composite nature of the characters we imagine in fictional worlds and the different purposes remembrance might play in our lives.\n\nIf these examples sound like they require a lot of thought and labour on the part of the bot’s initiators, that’s because they do. Good artistic representations require creative work. And this creative work is valuable for its own sake. In fact, the work that goes into producing a chatbot – which should involve many artistic choices as well as reflective engagement with archives and memory – may be more meaningful than any finalised product. Just as participatory theatre de-emphasises a complete and polished production in favour of process and iteration, building a chatbot requires constant fine-tuning. Thus, performance and creative development are part of the same process.\n\nThere is, in a sense, no chatbot finalised product that stands apart from users’ creative input, since chatbots are props that can bring a fictional world to life only in participation with users and their improvisational choices. This is one reason why, although chatbots can help ease the burden of navigating and accessing archives, they do not shrink or replace those archives. It is also why we should be suspicious of technology companies that offer us easy, prefabricated, supposedly complete chatbots. We should instead seek technology that enables us to create and fine-tune our own chatbots – without requiring extensive technical expertise. There are no shortcuts to thoughtful representations. And we should not try to circumvent the valuable creative process.\n\nThis shift from chatbots as companions to tools of artistic remembrance deflates many common concerns. From this vantage point, we can see that chatbot ‘actors’ do not necessarily aim at realism. They cannot and will not capture a deceased person fully. They do not delude users into behaving as if the dead are still alive or believing that they are talking to a real person. On the contrary, they underscore absence in the real world by engaging the memory of the dead in imagined spaces. Moreover, concerns about one-sided relationships with chatbot ‘actors’ are misplaced, since the natural object of a user’s attachment is the deceased person, and by extension the characters that represent them, rather than the tools that generate those characters. These tools – particular chatbots on particular platforms with evolving data sets – will typically be interchangeable and ephemeral in a way that the dead person is not. If these tools are made to serve artistic goals, rather than commercial ones, the attachments they foster will be nourishing rather than addictive.\n\nSome may fear the liberties AI will take with the legacies of the dead. To this we emphasise that these liberties are ours to authorise. Chatbots of the dead are creative representations that users should take an active role in shaping. Imagination is integral to all creative representations of the past because imagination is inextricable from memory and historical interpretation. We emphasise imagination not to abandon truth, but to show how truth can flow from our imaginations. We should judge AI representations by the same standards that we judge all truth-seeking artifice. An overly sentimentalised biopic may be worthy of the same critiques as a saccharine and evasive chatbot. Chatbots’ differing aesthetics will serve different purposes in our lives, from emotional release to therapeutic insight to intellectual enrichment to historical education. And like all aesthetic enterprises, the value of a chatbot is commensurate with the thought and effort – the creative choices – put into it by its authors.\n\nOur framework points towards a future where our technocultural resources can be more than tawdry distractions. They might offer a way past our modern problems with death. Rather than instruments that help us deny loss, chatbots of the dead can be a resource for reflecting upon our own mortality and those we have lost. They can provide a felt sense of the human community that preceded us, and they can support, or even promote, the loving bonds that do not disappear in grief. Chatbots of the dead exist within the universal register of art, imagination and spirit. This register plays an ineliminable role in human flourishing, and we are optimistic that chatbot ‘actors’ can be conducive to a form of flourishing peculiarly suited to our age.\n\nThe view of chatbots outlined in this essay is not a panacea. Even if our vision were adopted and the necessary infrastructure for creative use were made available, there would still be opportunities for misuse and exploitation. Chatbots of the dead will surely attract technology companies with economic incentives that are at odds with both the interests of users and the artfulness we have envisioned. There is a risk that the artistic spaces we envision will be colonised by bland, addictive, prefabricated, highly monetised, gamified commercial products, which serve only to benumb us from our spiritual problems. But art and creative spaces have long been a source of resistance to the domineering forces in human life. Let us not abandon new technology to the flattening effects of narrow commercial interests. Rather, let us assert AI’s potential to further the spiritual mission of humanity’s irrepressible creativity, our duties to history and memory, and our quests for insight and connection.\n\n",
    "scrapedDate": "2025-03-03T13:30:03.179Z",
    "wordCount": 5341
  },
  {
    "title": "Though natural selection favours self-interest, humans are extraordinarily good at cooperating with one another. Why?",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/commitment-and-cooperation-a-coevolutionary-relationship",
    "publishedDate": "",
    "content": "Every week at the office, you and your fellow employees have meetings to discuss progress on group projects and to divide tasks efficiently. Perhaps in the evening, you go home and cook dinner with your partner. At least once in your life, you might have seen a team of firefighters work together to extinguish a fire at a burning house and rescue those inside. You have probably also witnessed or participated in political demonstrations aimed at bettering the treatment of those in need. These are all examples of human cooperation toward a mutually beneficial end. Some of them seem so commonplace that we rarely think of them as anything special. Yet they are. It is not obvious that any of the other great ape species cooperate in such a way – spontaneously and with individuals they have never before met. Though there has been some evidence of cooperation in other great apes, the interpretation of studies on ape cooperation has also been contested. In the human case, cooperation is unequivocal.\n\nThe evolution of cooperation has been of interest to biologists, philosophers and anthropologists for centuries. If natural selection favours self-interest, why would we cooperate at an apparent cost to ourselves? You might say that none of these examples is costly; they all benefit the person cooperating as well as the recipient of the cooperation. This is true, but there is still a puzzle to solve. If I can reduce the cost of cooperating by deception – pretending to pull my weight in the group project or in the rescue mission – and still reap the benefits, why would I not do so? This is known as the ‘free-rider’ problem.\n\nThis kind of problem also arises in the very first forms of human cooperation. Group hunting is one such activity, possibly occurring as early as 2 million years ago. Imagine a group of our ancestors carrying spears, tracking a bovid animal like a wildebeest. If the hunt takes the form of an ambush, the group would need to coordinate with each other to surround the prey and attack it at the same time, without being noticed. If the hunt takes the form of a chase, the group would need to coordinate when they move forward and from what angle. They might also need to collectively figure out how to shepherd the target away from its herd. While some have found that chimpanzees cooperate in group hunting, this is not true of all chimpanzee groups. Indeed, others have argued that chimpanzee hunting is often characterised by individuals trying to maximise their own gain. That is, they do not work collaboratively with others nor do they bring food back to a central location for sharing, but rather fight over the spoils competitively. This usually occurs in the form of harassing the captor of the prey since the captor would not do well by attempting to protect a carcass that was too large to monopolise from the forces of several hungry predators.\n\nHowever, environmental changes approximately 2 million years ago meant that choosing to forage individually was no longer optimal and it became necessary to collaborate with others to secure better game. Global cooling and drying trends resulted in an expansion of open environments. This change in habitat was coupled with an increase in terrestrial monkeys such as baboons who competed with early hominins for their usual means of subsistence in fruits and vegetation, creating selection pressure for hominins to find a new foraging niche. So, early human group hunting looked quite different from that of the other great ape species. It involved coordination and cooperation as well as division of the spoils, even among those who did not engage in the hunt. As with our earlier examples, this kind of mutually beneficial cooperation would seem to involve an incentive to free-ride or deceive others, as an individual would do better by lagging behind in the hunt and ensuring the gain at minimal cost, if they could get away with it. Yet this is not typically what we see in practice. So what could explain why human group hunting is so cooperative?\n\nThe anthropologist Kristen Hawkes has argued that the reason men take part in a collaborative hunt is to signal their mating quality to potential partners. On this view, collaborative hunting is not primarily a form of providing food for one’s family but is instead a means of ‘showing off’ or sending a ‘costly signal’. In other words, if an individual’s quality is not directly observable, they are incentivised to advertise it either for sexual access or resource attainment. Such advertisements may take the form of risky displays or energy-intensive activities. Of course, it would also be profitable for ‘low-quality’ individuals to deceive others about their mating quality. The proposal is that, in general, the receiver of the signal is able to determine when senders are honest about their quality because of the cost entailed in signalling – a low-quality mate could not afford to engage in the risky or energy-intensive activity.\n\nHowever, there are many problems with this explanation for why men hunt. First, hunting is a collective activity where it is difficult to observe individual contributions that would correspond to mating quality. Second, it does not seem that costs are greater for the less-skilled – the whole group might do less well with lower-quality hunters but it’s unclear that the individual hunter does less well if he is low quality. Third, we would expect individuals to withdraw from costly hunting if they were less skilled – it would only advertise their low quality and they would pay the costs of signalling – but we do not see this in reality. Finally, in a study of 10 forager societies, it was found that, on average, male hunting generates 68 per cent of the group’s calories, compared with female foraging which generates 32 per cent; male hunting also provides 88 per cent of total protein sources compared with just 12 per cent from gathering. This shows that hunting is, in fact, a good means of provisioning for one’s family. So, it seems the costly signalling hypothesis is not well supported by ethnographic evidence. Provisioning offers an explanation for why men hunt, but we still face a free-rider problem. It does not necessarily require the effort of an individual hunter to ensure his family is provisioned as long as the hunting party meet a threshold level of engagement for a successful hunt, and the individual hunter’s lack of contribution goes unnoticed.\n\nMy theory for why cooperation in hunting is sustained is different. To get there, I will first have to introduce some game theory. Game theory is the study of strategic interactions between individuals. We represent a game of mutual benefit as a Stag Hunt. In this game, players have two options – hunting stag or hunting hare, and they choose simultaneously so they do not know what the other will do. Both individuals would do best by cooperating and jointly acquiring a stag. However, they also have the option of going on a solo hare-hunting excursion. If they do so, and the other person goes for the stag, the hare-hunter gets a reasonably good payoff, but the stag hunter gets nothing. If both go for the hare, each gets a lower payoff than they would if only one hunted hare, but at least they get something. So hunting hare is considered the ‘risk-dominant’ option – it has a lower payoff than if both people hunted stag, but is attractive because it ensures you never end up with nothing. Really, a group hunt is a Stag Hunt game involving multiple players, but I will use the simpler two-person example for now.\n\nIf we don’t end up cooperating, we lose out on future beneficial interaction\n\nHow can we ensure cooperation in a Stag Hunt? One option is via commitment. A commitment is a pre-play signal (a signal that occurs before the choices are made) that changes an individual’s incentives for some future course of action, as well as changing her partner’s expectation of her future course of action. The signal need not be verbal. A commitment in the Stag Hunt game might involve, for example, picking up one’s spear or simply going on the hunting excursion. This changes the receiver’s expectations of what the sender of the signal will do. If the receiver believes the sender is going to go stag-hunting rather than hare-hunting, the receiver is also incentivised to go stag-hunting, since this ensures a better payoff for both. The sender is also incentivised to follow through on his signalled commitment to choose to hunt the stag. If he subsequently chooses to go hare-hunting, not only will he receive a lower payoff, but he will also be subject to social sanctions imposed by the receiver for violating her expectations. That is, the commitment is made credible because there is a reputational cost to reneging. Someone who commits and does not follow through reveals themselves to be an unreliable partner and so will be less likely to be chosen for mutually beneficial interaction in the future, meaning there is a cost to reneging. I believe commitment is large part of the reason humans have become so profoundly cooperative compared with other great ape species.\n\nSo far, this has all been theoretical. I have shown how, in theory, commitments could solve a coordination dilemma in a Stag Hunt game, but I have not shown that this is actually the mechanism ensuring cooperation in early hominin group hunting. To do so, I must show, first, that there are future opportunities for beneficial interaction and, second, that reneging is punished by social exclusion. If these conditions are met, a signal to cooperate really does change sender incentives to cooperate since, if we don’t end up cooperating, we lose out on future beneficial interaction. To establish this empirically, I will use evidence from modern hunter-gatherer societies. Of course, the analogy between the practices of modern hunter-gatherers and our ancestors is not perfect. In particular, you might worry that I am crediting our ancestors with more advanced cognitive skills than were present at the time. However, this shouldn’t be a worry for our current case – all that is needed for reputation consequences to change the incentives of the sender of the commitment signal is that we have some concept of preferential partner choice, and this capacity is demonstrated even in chimpanzees.\n\nFirst, future opportunities for beneficial interaction exist because of the interconnected nature of hunter-gatherer communities. Individuals rely on one another for food and collective care of offspring, among other things. For example, recent anthropological evidence from modern forager societies reveals that, in some societies, newborns are held by alloparents 85 per cent of the time in the first days after birth. In this kind of social world, maintaining future opportunities for cooperative interaction is paramount.\n\nSecond, we must ask whether reneging in the hunt is punished by social exclusion. In fact, there is plenty of evidence about this from modern hunter-gatherer communities. For example, in a comprehensive overview of food-sharing published in 2004, Michael Gurven documents his fellow anthropologist Jon Altman’s earlier finding of collusion between two Gunwinggu family clusters in Australia to share less food with a third cluster who was not producing enough. The sanction induced higher production and sharing by the third cluster. Meanwhile in the Philippines, Marcus Griffin found that unproductive Agta civilians are socially ostracised until they are forced to relocate. And in his book The Netsilik Eskimo (1970), the anthropologist Asen Balikci even notes that ‘lazy hunters’ are the subjects of back-biting, ostracism and outright quarrel in the Netsilik community. So reneging on a signalled intent to cooperate really can incur a cost.\n\nAt this point, you might wonder what work the commitment signal is doing – the answer is that those who do not signal are not punished. Think, for example, of women and children who still receive food even though they did not go on the hunting excursion. If it could be shown that able-bodied men who do not signal are still punished for not hunting, then the explanation might be one of sex-based expectations rather than signalling, but there is as yet no evidence of this.\n\nI have shown how commitment can serve to secure mutually beneficial outcomes in group hunting – a commitment changes both others’ expectations of what the hunter will do, and the hunters’ own motivations to cooperate. However, I do not believe that the role of commitment in human cooperation stops here. Rather, I think there is a coevolution of commitment and cooperation over human history that explains why human cooperation is so distinctively proactive and widespread. That is, humans developed means of making commitments to cooperate with one another that allowed some forms of rudimentary cooperation to get off the ground and this, in turn, created the selective environment for the development of increasingly effective commitments, allowing even more effective cooperation on a wider range of issues, and so the feedback loop goes on. In what follows, I’ll illustrate a coevolutionary relationship of this sort by showing how group hunting and other early hominin shared activities played a role in the emergence of language, allowing us to make linguistic commitments to cooperation.\n\nCooperation in early hunter-gatherer societies allowed hominins to expand into new habitats and with reduced risk of predation from other species. The cultural learning skills that were developing in the context of weapon-making and food preparation also enabled us to make innovations in clothing, shelter and more sophisticated tools. These developments better protect against harsh habitats and thereby contribute to extending human lifespans. Furthermore, the division of labour in hunting and gathering provides a resource buffer when prey is not available – this increased means of subsistence allows the group to grow and specialisation in these skills allow it to be more productive. As Kim Sterelny notes in his book The Evolved Apprentice (2012), it is likely we therefore see a positive feedback loop between collaborative hunting, increased group size and the rate of innovation. Not only this, but greater access to animal resources and a smoothing of the food supply across seasonal variation meant that there was greater availability of energy for mothers, a relaxation of the energy constraints on development of larger brains, and elongated life histories with shorter interbirth intervals. All these features will have contributed to growth of the group size. Since residential camp sizes in the modern ethnographic literature are still quite small, when I say growth of the group, I mean the interconnected network of camps, sometimes called the ‘village’. Unlike nonhuman primates, hunter-gatherers exhibit a hierarchical social structure of co-residing families with friendship dyads across camps, frequently referred to as the multilevel society.\n\nIt is in the context of these larger, interconnected groups that we see selection pressure for a transition from communication that was context-dependent to abstract and decontextualised language, which, of course, is necessary for linguistic promising. Many researchers believe there is a causal link between joint activities and language evolution. One notable account is by the neuroscientist Michael Tomasello, in his book A Natural History of Human Thinking (2014). He argues that, in joint tasks such as foraging or hunting, participants require a shared common ground of experience in order to understand the communicative acts of others. For example, Sandy pointing to a banana tree would indicate to Betty that there were bananas there only if they were engaged in a joint task of banana-foraging. This kind of inference requires certain cognitive capacities. In particular, we need to be able to represent something from another’s perspective, think about what others know about what we know, and monitor our own communicative acts to make sure they convey what we want them to. Importantly, these kinds of cognitive capacities, which developed in the context of early hominin joint activities like hunting, are precursors to the cognitive capacities needed for language.\n\nOne crafts a spear head, the other crafts a shaft. To do so, they need some means of communicating\n\nMoreover, the multilevel society is one in which we were potentially required to engage in cooperative interactions with new partners on novel tasks. For example, it is only in the context of such societies that we might see across-camp trading of goods. This is a situation where we do not have a pre-established shared common ground, creating selection pressure for a group-wide means of communication. For this, we needed to be able to adopt a person-neutral perspective on a situation, use a culturally constructed system for representing our intentions, monitor our communicative acts, and adjust them according to the standards of our group. Crucially, the cognitive capacities that make language possible are built upon the cognitive capacities I mentioned earlier – those used in communication in early hominin shared activities. So we see how group hunting – and likely other forms of shared activity made possible by commitment – contributed to the emergence of language. First, in contributing to a growth of the group size, which provided that selective environment for language evolution. Second, in providing the cognitive precursors to abstract and decontextualised language.\n\nIn these larger groups, we need a form of committing that does not rely on prior common ground. Picking up one’s spear doesn’t signal cooperation in a new interaction with a new partner. But where did linguistic promising come from? I suggest it arose out of the pressure to make explicit the roles of interaction in complex shared activities. As a result of group hunting and the subsequent increase in productivity, humans were able to develop the more sophisticated tool-making and projective weaponry of the Middle Stone Age. With the possibility of more complex tasks involving several individuals, some means of communicating one’s intention in a joint task would have made everyone better off. For example, consider jointly crafting a spear. The individuals might do better if they could coordinate on doing complementary tasks – one crafts a spear head, while the other crafts a shaft. In order to do so, they need some means of communicating this to one another. Importantly, conventions about past roles taken on by the individuals won’t suffice, since the individuals might be interacting with each other for the first time. For role announcement to constitute a commitment, all that is required is that Betty forms clear expectations of Sandy’s future course of action. As long as this is the case, she may be prepared to exclude Sandy on the basis of unfulfilled expectations, and this raises Sandy’s cost of reneging.\n\nIn order to explain its emergence, linguistic commitment also has to offer some fitness advantages over and above prelinguistic commitment. In fact, it offers many. With language, commitments no longer depend on opportunities for shared activities. Commitments can be made among strangers as long as there are reputational effects that raise the cost of reneging. This will be true if the strangers share the same network of social interaction, even if the two interacting individuals know nothing about one another. Language is decontextualised, allowing us to make commitments in multiple contexts where a shared common ground does not exist. The ability to say: ‘I will help you’ signals cooperation in multiple contexts. In contrast, picking up one’s spear or showing up to the hunting excursion will not be sufficient signals of cooperation in a new context, like caring for another’s offspring.\n\nFurthermore, language allows us to make commitments that refer to things that are spatially and temporally remote. It also has scope to be more specific than nonlinguistic signals of cooperation. Communicating a commitment as specific as ‘I promise to collect figs while you forage for bananas’ allows the receiver to form more grounded expectations of cooperation concerning actions she may not directly monitor. Linguistic commitments allow us to make conditional commitments. In a multistage interaction, language allows the individual to say: ‘If you share x with me, I will share y with you in turn.’ We may also specify the penalties of reneging, which can strengthen the credibility of commitments. For example, Sandy may say to Betty: ‘I promise to collect figs and, if I fail to do so, you are entitled to take my bananas.’ As such, reputational consequences may come not only from reneging on the commitment itself but also reneging on the specified penalty, compounding the costs of defection.\n\nOf course, there are intermediate stages between prelinguistic commitment and linguistic commitment dependent on fully decontextualised, abstract language. A promise will become increasingly explicit as our communicative capacities develop. Showing up to the hunting excursion or picking up a spear might communicate your simple intention to hunt. Combining gestures will allow additional complexity – not only can you communicate what you intend to do, but also by what means, through stringing gestures together. For example, you can signal a commitment not only by showing up to the hunting excursion but also by miming your intended role in the hunt or pointing to where you intend to go.\n\nI have spelled out a coevolutionary link between human cooperation and commitment. Prelinguistic commitment ensured the stability of early hominin group hunting by changing receiver expectations and sender motivations, incentivising cooperation for both parties. These commitments were reputationally enforced. The early collaborative activities that they made possible contributed to the formation of larger, multilevel societies and the development of language. This provided us with the means to engage in a new and more effective form of commitment – linguistic promising.\n\nThe evolution of commitment had a profound impact on the evolution of human cooperation. Some cooperative activities could not have been achieved without the ability to communicate about specific divisions of labour, expectations and abstract events. Consider collective action problems such as building projects. Here, the ability to abstractly refer, and precision about the division of labour in financial and physical investment, is necessary for achieving a mutually beneficial end. However, communication alone is insufficient for individuals to trust one another when there is an opportunity to free-ride off the efforts of others. What is required is the ability to make a credible commitment to investment – that is, the ability to explicitly promise.\n\nBut how can we tell if my account is true or not? One might think that this kind of explanation is rather speculative and unconstrained – it is storytelling. An evolutionary explanation of this sort generally begins with a description of the ancestral state and a purported end state that we want to explain. Here, the end state is modern human cooperation. The explanation given takes a narrative form – the aim is to provide a synthesised description of an evolutionary process by appealing to incremental changes we could have made in response to social or ecological pressures in our environment. In their book From Signal to Symbol (2021), the philosophers Kim Sterelny and Ronald Planer have written about the conditions for a good evolutionary narrative, and I will briefly show that this account meets those conditions.\n\nSignalling one’s attitude toward a moral norm can commit one to acting in line with the attitude\n\nFirst, this hypothesis is gradualist and incremental – language capacities coevolved with the development of large, multilevel groups and with our commitment practices. In other words, incremental changes in group size were accompanied by incremental changes in the decontextualisation and abstractness of language and this was, in turn, accompanied by more incremental changes in the efficacy of commitments. Second, the changes I have posited in the social environment that supported the evolution of linguistic commitment are not improbable – they depend only on interaction with strangers and tasks that create a need for role announcement. Third, there is a principled choice of baseline capacities – the cognitive requirements for communication in shared activity are supported by empirical evidence from prelinguistic infants. Fourth, in relying only on growth in the group size and innovation in the complexity of tasks, the account is congruent with variability in hominin environments. Finally, the theory has testable implications.\n\nI hope to have offered a new explanation for the distinctive prosociality of humans – its origins lie in the coevolution of commitment and cooperation over historical time, beginning with early hominin group hunting. This coevolutionary relationship between commitment and cooperation likely happened many more times in our history. Notably, new forms of commitment involve those backed by moral norms and by institutional structures. For example, signalling one’s attitude toward a moral norm can commit one to acting in line with the attitude, and not doing so is frequently met with extreme ostracism. Legal commitments are not only reputationally enforced, but also physically and financially enforced, compounding the costs of defection and further incentivising cooperation.\n\nMy hypothesis is that this relationship between expanding cooperation and new forms of commitments is a uniquely human phenomenon and helps to explain the evolution of distinctively human prosociality. While prelinguistic commitments are available to other animals, linguistic, moralised and institutionalised commitments are what make the scope and scale of human cooperation unique.\n\n",
    "scrapedDate": "2025-03-03T13:30:04.846Z",
    "wordCount": 4184
  },
  {
    "title": "When I moved to India for work, I found that rape was a feature of the country, as deeply embedded as caste",
    "category": "Childhood and adolescence",
    "url": "https://aeon.co/essays/how-did-rape-become-a-feature-of-indian-society-like-caste",
    "publishedDate": "",
    "content": "It was a warm spring evening in Bengaluru. Purple jacaranda blooms leaned heavy against the dying light, and warm puffs of exhaust sat suspended in the air, as if urging the monsoon to break. Leaving the newsroom at the end of the day, I decided to stop by an office-warming party for a mapping company I was considering writing about, right by my apartment in upscale Indiranagar. Around two dozen people milled about the open-plan space, sipping beers and bobbing to the reggae playing in the background. This crew was distinct from the regular Bengaluru startup crowd: they had travelled to places like Poland and Peru for mapping projects, and there were quite a few women as well; the lead told me they wanted to reach gender parity soon. They seemed eager to get to know me as a fresh-faced foreigner new to the city, not as a reporter who might feature them in the paper.\n\nBefore I knew it, it was almost 10 pm. As I took my leave, my host offered to walk me home. It was rare for me to be out drinking midweek but I shook my head no: I lived just a few blocks away! ‘Are you sure? It’s pretty late,’ he said, with genuine concern in his eyes. I dismissed his worries with a flick of my hand. The gesture felt a little too intimate between a reporter and a potential source; besides, I had chosen to live in Indiranagar precisely because of its reputation for being safe: for the right to walk home without worry. Before he could press further, I made my way towards the exit and spent a few minutes getting lost inside the maze of shoes, my vision woozy around the edges.\n\nOnce I’d stepped out of the building, the euphoria of the party took all of two seconds to fade. The road ahead was pitch dark, deadly quiet. A gust of wind whipped against my torso, then another. I steeled my jaw, hunched my shoulders, and began marching the hundred or so metres back towards the bright-lit main road. I’d walked home after sundown a number of times, but this time, approaching the junction, I realised something was amiss: the streetlamp on the corner was not on. The road stayed dark. I figured there must have been another blackout – they had become more frequent as the days grew hotter.\n\nA low-pitched buzz was approaching from ahead – I could just make out the contours of a motorbike scooting towards me at full speed. Before I knew it, a hand had thwacked my chest. When I finally gathered myself to react, I spun around and saw his face, turned towards me from the receding vehicle, shiny with sweat in its red backlight. Tuft of hair on top, double chin bulging from the bottom, the offending arm held out like a weapon. A wicked smile that said: Gotcha. Did you think you were safe?\n\nIt was 2015, and I had moved to Bengaluru to work as the only foreign reporter at a national paper. India was fresh off Narendra Modi’s arrival to power after decades of Congress rule; the rupee was strong and projected to grow stronger in those innocent days, pre-demonetisation, pre-COVID-19 pandemic. The promise of political change and prosperity was thick in the air, overshadowing the looming threat of Hindutva violence. One of the world’s largest emerging economies seemed poised to turn a new page and I wanted to be there. I had just graduated from a prestigious US university with no specific aspirations other than to experience as much of the world as possible. When I received an offer from the paper, I took it.\n\nViolence against women was one of India’s defining features at the time, as an outraged global media gave extended coverage to the brutal gang-rape and murder of a young physiotherapy student, Jyoti Singh, on a night bus in Delhi in 2012. What became known as the ‘Nirbhaya’ case (for ‘fearless’, in Hindi) was just the tip of the iceberg. More than 34,000 rape cases were officially reported in India in 2015 – one woman raped every 15 minutes – and nearly 10 times as many crimes against women were logged. Born and raised in Hong Kong, I’d had the privilege of growing up taking my safety for granted. I’d ambled across town alone at all hours as a teenager, had walked home after midnight without a second thought: though casual sexism abounded, I’d never had my personal space impinged upon. The few instances of public sexual assault in the region were reported with dramatic condemnation in the news. It is common for Hong Kong natives to live abroad – in industrialised nations with high living standards. Meanwhile, my move to a developing country, and one as unsafe as India, was so far beyond the bounds of convention that some of my peers took me for a joke.\n\nDid the violent crime that claimed Nirbhaya happen here too? Or were things getting better?\n\nStill, I had spent the previous summer visiting the metropoles of Delhi, Mumbai and Chandigarh, and witnessed how the gender landscape was changing. For many women whose lives had previously been circumscribed to the family home, the smartphone revolution meant they now had unprecedented access to the wider world. A generation of women was becoming educated, completing high school and university at unprecedented rates, and eager to make their own living, heading for the big cities where they would find work and a hitherto unimaginable independence. By 2014, it was estimated that more than 30 per cent of Indian programmers were women.\n\nBesides, Bengaluru was meant to be different. The city, now known as the Silicon Valley of the Global South, had jumpstarted the country’s software boom in the 1980s, on the back of the success of Infosys, co-founded by N R Narayana Murthy (father-in-law to the former UK prime minister Rishi Sunak). Bengaluru had amicable weather, live music venues, even a flourishing craft beer industry. And it was south India: liberal, educated, peace-loving and English-speaking. I arrived to find women thronging its dusty avenues at all hours of the day, lingering in restaurants and negotiating with rickshaw drivers and subziwallas (vegetable sellers), in groups and alone. They were just as likely to wear jeans as salwar kameez. Some bleached their skin with Fair and Lovely creams, others threaded and pencilled-in their brows or coated their eyelids with kajal, a look I later attempted to adopt.\n\nIn the newsroom, I was placed within a predominantly female team. Krithika drove a scooter around the city for interviews and events, and before long I became a frequent rider on her backseat, catching the ends of her dupatta flying behind her helmet. Avani always had plans for a party or a festival, and the outfits to go with them. Shraddha, impeccably dressed, had her share of harassers and stalkers, but she deterred their efforts with masterful aplomb. Then there was Radha, the only female editor at the paper, who conducted herself with unfailing poise, sailing through the office among male reporters and executives in her carefully pressed, gold-bordered saris. These women appeared to have nothing to fear. They taught me everything about the country I would report on, and eased me into daily life in Bengaluru. I queried them on my worst fears: did the violent crime that claimed Nirbhaya happen here too? Or were things getting better? ‘That kind of thing won’t happen to you,’ they assured me in lowered voices. But with a few caveats: don’t wear anything too revealing, keep your valuables in a safe place, make sure you always have someone to call in case trouble strikes. Don’t go out on your own at night. Otherwise, do anything you want, go anywhere you want, wear whatever you want. This is Bengaluru, the safest city in India!\n\nThe assault on my way home was not my first brush with violence. A few months before, as my first New Year’s in the city approached, I canvassed the newsroom for celebration ideas. Most of my colleagues planned to stay in with friends and family. Some restaurants and hotels were advertising countdown parties, charging thousands of rupees a head. I wondered whether there was a central place where people counted down to midnight, like they do in Times Square in New York City. It seemed that Brigade Road, the shopping street just blocks from the newsroom, might offer what I was looking for: the area was blocked off from traffic every New Year’s for a mass celebration. No one on the team had ever attended, but I was keen to check it out. ‘Be careful na, it’s going to be very crowded,’ said Krithika, but I was not concerned: I had grown up in one of the most crowded cities in the world.\n\nMy friend Meghna and I started the night with a few drinks at her apartment, along with our friend Max, a tall American in his 30s, all of us recent arrivals. Before leaving for Brigade Road, I patted myself down – bank card in my bra, small wad of cash in my left pocket, phone in the other pocket – then smoothed a long kurta over my jeans.\n\nThe main roads were choked with cars and pedestrians hurrying towards their merrymaking destinations. To avoid them, we took a quieter path up Museum Road and around the moonlit curve of Rest House Road, past convent schools, consulates and the line of Tibetan malls I often visited for momos and noodle soup, finally arriving at the intersection with Brigade Road. The juncture was fenced off by a makeshift security checkpoint, guarded by two policemen wearing signature khaki uniforms and slouch hats. Beyond this barricade, Brigade Road was strung with coloured fairy bulbs and neon signs, just like every other night when I passed by on my way to the Metro. However, tonight the lights shone upon a roaring tide of humans, every single one male. Men in polos and plaid shirts, in gold necklaces and bandannas, bald and bearded, pot-bellied and skeletal, pushing and shoving. Drunk, belligerent, hungry for mayhem.\n\nIt was a roiling sea of testosterone and sweat and euphoria, lurching vaguely southward. Smelling sour. I took a moment to soak in the scene. This was Bengaluru’s version of the New Year crowd: it resembled nothing I had seen before. We had come all this way with a white man escorting us, what was there to do before midnight but paddle along? I turned to Meghna, who looked a little hesitant, but she nodded. ‘Hold on to me so we don’t get separated,’ she said, and we clasped hands. A policeman moved the barricade to let the three of us enter, his eyes idly surveying the mass, barely paying us attention.\n\nA dark face glared into mine from centimetres away, a bloodthirsty glint in his expression\n\nAt once we were swallowed up by the heaving stew and knocked about in its current. My legs buckled forward, my sandalled feet were stamped on before they could find firm ground, my shoulders were shoved side to side. The stench of alcohol and perspiration was overpowering. I reached for my back pocket to stop my phone being snatched, but my hand landed on something else, plastered around my body, in the small of my back, pressing against my shoulder blades, pinching at my chest, digging between my legs from behind.\n\nOther hands. Sweaty hands. A dark face glared into mine from centimetres away, a bloodthirsty glint in his expression: I am going to swallow you whole. I cried out for Meghna and turned to meet her eyes – as wide and stricken as mine were. They told me everything I needed to know.\n\nThe next instant, she was tearing through the mob back towards the barricade, yanking me along with her. We jostled against another dozen hands intent on folding us back into the riot, yet we somehow broke free and pried ourselves out, Max a few paces behind. We bolted all the way back to Meghna’s apartment, arriving right before midnight. The countdown roared through the window. Max cracked open a Kingfisher and handed it to me. I took the beer and headed to the bathroom, where I spent the first hour of the year crumpled on the floor, my face in my hands. We had been on Brigade Road for all of 30 seconds. It was 30 seconds too many.\n\nBack in the newsroom, I recounted what had awaited me on Brigade Road, careful not to blame my colleagues for suggesting the option. They reacted with expressions of open-mouthed awe, but I could tell they were not entirely surprised. ‘It isn’t your fault,’ they said, ‘but what were you wearing? Were you with anyone?’ Anger flared in my cheeks and quickened my breathing. My female colleagues – my guardians and friends – had not prepared me for what I’d experienced, and when things went south they resorted to finding fault in my behaviour: it felt like betrayal.\n\nI pulled in a lungful of office air. They had not meant to deceive me. Most of them had moved to Bengaluru from more violent reaches of the country and were enjoying greater independence than they had known elsewhere, without any clear idea of its limits. They were also upper class and upper caste, accustomed to unspoken norms of respectable behaviour that shielded and coddled them: the possibility of attending a public celebration with thousands of strangers would scarcely have crossed their minds. Meanwhile, I had been confronted with the scene and still misread it entirely. We were just beginning to fathom the gulf between us, the different expectations we carried when it came to our liberty and safety.\n\nIn Radha’s office, I repeated the story, this time careful to qualify who I was with and what I was wearing. She was pensive while I blew my nose into a napkin. ‘I am very sorry these kinds of things are still happening in this country. Everywhere, every day.’ Her face was grave but firm, her eagle eyes beady as ever. There was a finality to her words, as if she were delivering news of a death. ‘I do hope things will get better, but unfortunately this is the way things are for now. And there is nothing you nor I can do about it.’\n\nBefore I left, she added: ‘Just stay away from these situations, OK? You have no idea who those people are, what all they can do. They’re not educated, they don’t know how to behave. All you can do is stay away.’ Those people. The mass that was the lower class, impenetrable when it came to their caste, religion, language, values and norms of behaviour. As I was starting to learn, othering was a handy tool for my companions when confronted with the less savoury realities of their society, one whose lauded diversity can just as easily morph into social division.\n\nAs women in India make tentative strides towards greater liberty and individual rights, their progress has not been taken kindly by all sectors of a staunchly patriarchal, misogynistic society. Living in Bengaluru amid the changing winds and careless promises of the early Modi era, it was easy to forget that this was still a country ranked 130th out of 155 in the United Nations’ Gender Inequality Index (2015), and where, according to the UN Population Fund (2020), almost half a million baby girls went ‘missing’ every year through sex-selective abortions and infanticide. The average age of marriage for women hovered around 19, and, despite the gains made in education, the International Labor Organization (2014) estimated that only a quarter of adult women participate in the labour force – among the lowest rates in the world. While upper-class women have ascended to the C-suite, even the office of prime minister, male supremacy remains the status quo.\n\nAs India’s riches have grown over the past decade, they have coincided with historic levels of inequality, with the top 1 per cent accruing 40 per cent of the country’s wealth, while the bottom half continues to survive on less than $3 a day. Hundreds of millions of men continue to find themselves in a poverty trap, increasingly left behind by India’s generational growth story and, as their grip on entitlement start to waver, they feel even more threatened. It is easy to imagine how, when confronted with women’s onward march toward greater independence, men resort to violence to put women in their place and reassert their own power. If they control nothing else, they can control women’s bodies; and any female is a target – from infants to elderly widows, in public spaces, in the home.\n\nThere is a grievous lack of sex education in India, driven by conservative attitudes that regard any discussion of sex as taboo, while several populous states have banned sex education outright. The researcher Madhumita Pandey and the journalist Tara Kaushal, who each spoke to rape convicts in the aftermath of Nirbhaya, affirmed that none of their subjects had received sex education or understood the concept of consent; in many cases, they even failed to understand the crime they were being charged for. In this vacuum, many young men learn about sex through pornography and Bollywood movies that glorify men who stalk and aggressively pursue their romantic interests until they relent. While the global discourse around Nirbhaya condemned the country’s ‘rape epidemic’ and framed the rapists as ‘monsters’, inside the country itself there was an equally potent narrative that blamed the victim. One of the accused, sentenced to the death penalty, openly said that they had wanted to teach the female college student a lesson for staying out after hours with a male acquaintance.\n\nIt is a liberty with definite bounds, enjoyed only when supplementary arrangements are afforded\n\nIn my conversation with colleagues, interview subjects and local acquaintances, I heard approval for growing gender parity in the same breath as they lamented that society was simply not ready. Backed into a corner, the reformists concluded that women had to be protected for their own good. This was the overriding logic behind the various safeguards I discovered while living in Bengaluru: the gender-specific curfews in college hostels and paying-guest accommodations; the segregated security lines at the entrances to malls and hotels; the women’s sections on public transport that I never strayed from.\n\nAs more women enter the workforce, particularly in manufacturing, states have passed legislation limiting women from taking on night shifts and requiring employers to provide them with safe transportation home. In their groundbreaking book Why Loiter (2011), Shilpa Phadke, Sameera Khan and Shilpa Ranade focus on Mumbai, another purported safe haven for women, and point out that so-called safety for women is limited only to middle-class women, implicitly assumed to be ‘young, able-bodied, Hindu, upper-caste, heterosexual, married or marriageable’, and that their access to public space is conditional at best: ‘subject to [her] knowing the “limits”, restrictions that often do not apply in quite the same way to her brothers.’ It is a liberty with definite bounds, enjoyed only when supplementary arrangements are afforded. The problem is never with men, nor the society that continues to perpetuate masculine ideals of dominance and violence.\n\nTo me, it was a relentless cycle of violation and retreat, followed by advances that met with further violation. Social change is slow: women have to buy time until, one day, the values of equality and respect sufficiently penetrate the majority, and the guardrails can come off. But how can this happen when ‘those people’ continue to be othered and shut out of systemic reform?\n\nIn 2024, more than a decade after the Nirbhaya murder, another high-profile case shook the country: the rape and murder of a trainee doctor in Kolkata, in her college building after a long hospital shift. Her death was at first ruled a suicide – despite her body having been found with eyes, mouth and genitals bleeding. Later, after the principal of the medical college blamed the victim’s decision to rest in a seminar hall alone at night, cities around India erupted in protest, asserting women’s right to ‘Reclaim the Night’. The chief minister of West Bengal, Mamata Banerjee, one of only two female chief ministers in the country, led one of the rallies and offered to resign.\n\nI am curious as to why, out of tens of thousands of reported rapes every year, the Nirbhaya and the Kolkata cases alone have sparked this level of outrage. Both victims had been training to enter the medical profession, embodying the aspirational woman in the rising middle class, and yet education and hard work had not exempted them from being targeted. In the months since the Kolkata case, doctors and medical schools across India have staged numerous strikes demanding heightened security for medical workers, recycling the same worn logic for more protection, more gilded cages. They argue that hospitals should be safe places, islands of exemption from the broader, uglier reality. But where are the protests for the vast majority of rape victims, the less privileged majority who are somehow seen as less deserving of protection? Are they, too, simply ‘those people’?\n\nIn 2020, in the Hathras district of Uttar Pradesh, a 19-year-old Dalit (lower-caste) woman was raped by four upper-caste men and subsequently died of her injuries. The police initially declined to file an official report, then hastily cremated her body without the family’s consent. Protests were held, but these never garnered the attention given to the two students. Only one of the accused was handed a life sentence; the rest were acquitted. Or take the documented practice of rape carried out by the Indian military in the past three decades, during armed conflicts against Muslims and Indigenous Adivasi communities, as a weapon of power alongside murder, arson and mass displacement – crimes that are never called to account. The Free Press Journal last year reported that ‘in many cases, law keepers like the police were the perpetrators. Rapes in Kashmir and the northeast by men in uniform are routinely dismissed.’ This kind of impunity sanctions the use of rape by official state forces. Meanwhile, marital rape is yet to be considered an offence, despite a legal commission after the Nirbhaya case recommending its criminalisation, effectively granting men inalienable rights over their wives’ bodies. Indeed, it has emerged that the accused in the Kolkata case had a history of assault against his wives: no cases had been brought against him for his actions, and he had been allowed to continue his duties as a police volunteer.\n\nTaken in sum, an implicit hierarchy starts to take shape. As the writer and activist Meena Kandasamy described it in a blog post in 2014:\n\nBy attributing standalone cases to ‘those people’, educated Indians, women included, sidestep the root cause: an implicit belief system that maintains inequality between the sexes and castes, and holds the country back from broader progress.\n\nA wedding invitation meant I was to be initiated into wearing the sari. Traditional Indian clothing for women tends towards modesty, obscuring body parts deemed off-limits to the public eye. Long kurtas with churidars, salwar kameez sets, dupattas draped over the shoulder – all are composed of loose-fitting, flowy fabrics that are comfortable but not exceptionally flattering. The sari is a different affair: made of a tight, midriff-bearing blouse and a long cloth carefully wound and bound around the body, like a bow on a present, it accentuates the female figure while concealing less attractive bulges. It is said that no woman can look bad in a sari. In recent years, the outfit has been claimed as a symbol of liberation by South Asian women.\n\nI stood in my living room in my underwear, amid a whirlpool of stiffened silk, while Meghna inducted me into the delicate craft of draping, the way her mother and aunts had taught her. First, I was to tie a knot at the edge of the cloth and hold it in my fist, letting the rest of the fabric fall to the ground; then, wrap the cloth around the waist twice, effectively rendering the lower body immobile; next, I had to pleat the cloth carefully, back and forth, tucking the pleats into the front of the sari, before throwing the remaining cloth over my shoulder where the pallu could tumble down the back.\n\nAll night they danced. Hands placed delicately on their waists and the back of their heads, they thrust their hips\n\nA few clumsy tries later, I waddled gingerly towards the mirror to assess the effect. The blouse was form-fitting, and the pallu fell over the curve of my waist. The pleats fanned out prettily along the floor. I felt beautiful. But in my amateur attempt, I also felt constrained, devoid of agency, as though the bandages holding together my guise might at any moment fall apart to reveal my naked foreign self.\n\nYet at the wedding there was only ecstatic joy. After the pre-wedding rituals of the sangeet and the haldi, after the bride had led the groom around the sacred fire three times and uttered her vows to provide him with abundant health and bear his children, the lights dimmed and the dancefloor glowed, beckoning. The sweet sitar tune of ‘Om Shanti Om’ poured from the speakers, and women of all ages whirled into the centre of the room. All night they danced. Hands placed delicately on their waists and the back of their heads, they thrust their hips to the amplified bass, feet stamping. Rolls of flesh spilled from beneath the folds of wound fabric like blessed abundance; pallus unwound and tossed about. Heads were thrown back, braids flying, eyes closed in rapture, faces crinkled with laughter.\n\nHow carefree these women appeared, as if all worries had been strewn to the air, forgotten. A wedding is the most regimented of rituals, and yet it also grants women a space to celebrate the womanhood that cost them so many pains to share. Every culture burdens its women with a blanket of expectations regarding how to dress and speak and behave, and in that moment it was easy to believe that the blanket was resplendent in all the suffering it conferred, that it was very much worth it. I stood in my inexpert sari on the edge of the room, wondering whether it was only a matter of time before I could learn to shoulder the blanket with grace.\n\nThe authors of Why Loiter call for women to idle on Mumbai’s streets without clear purpose, just like their male counterparts, and claim full rights to the city even if that means courting physical risk: to them, the greater risk lies in not experiencing the pleasures of public space. The Bengaluru writer C K Meena endorses this idea: despite regularly encountering harassment in public spaces, she refuses to have her freedom of movement curtailed, and is instead determined to change male behaviour by putting up a fight, one instance at a time. As a young arrival in Bengaluru, armed with my privileged upbringing and Jane Jacobs-ian theories of ‘eyes on the street’ as a guarantor of personal safety, I would have agreed with them. I might have run into minor trouble, but that didn’t mean I had to shrink, or make my life smaller. Call it bloody-minded, but I did not move all the way here to live the life of a second-class citizen.\n\nAnother workday ended earlier than usual: the sun just setting, sending grapefruit streaks across the baby-blue sky. The jackfruit vendors were still in their daytime spots by the Indiranagar Metro when I emerged, a saccharine sweetness emanating from the sticky fruit laid out at their feet. I gave myself permission to relax a little, dropping my shoulders as I walked along the main road towards my apartment. Arriving at the intersection near the mapping company’s office, I paused for a moment and peered around the corner. I had remembered it as a desolate shroud of street, a world away from the rushed frenzy of the main road, but at a glance it resembled any other residential street in Indiranagar, indistinguishable from the one I lived on nearby. Would it carry any trace of the sinister for someone who’d not had the misfortune of being assaulted there? I wanted to believe I’d simply got caught in freak accident in an otherwise safe area. I wanted to not live in fear in a city I was trying to call home: it was not too much to ask.\n\nSo I turned into the street I’d avoided since my assault, watching the empty lane ahead darken with every second, and passing sand hills piled up by the construction sites on my left. At an angry revving from some distance behind me, I tensed and turned around slowly, waiting. A benign two-wheeler flew past, its occupants sitting stick-straight in their helmets. The coast was clear. I resumed walking. The next thing I knew, a force hit me on my right – a violent billow of wind, vaguely wet, knocking me off my bearings. I looked up just in time to see another motorbike whizz into the distance. A few stunned seconds later, I dug out my phone and turned on the flashlight. My right arm, shirt and skirt were spattered with a moist, brown goop. It was one more attempt to put me in my place, a place I refused to belong.\n\nThey estimated that 99 per cent of crimes in India remain unreported\n\nThis time, my newsroom collegues encouraged me to lodge a formal complaint with the police. ‘After Nirbhaya, they have been taking such cases much more seriously,’ they said. ‘Why don’t you give it a try?’ That case in Delhi had exposed just how inept the Indian police force was at handling sexual harassment, and a series of reforms had been swiftly announced, expanding the legal definition of rape to include all penetration without consent, and lengthening the jail sentence of convicts from seven to 10 years. For rapes that result in death or a vegetative state, the death penalty was introduced. Women police officers were purportedly now designated to file sexual assault reports at police stations. Fast-track courts were established.\n\nBut the Indian legal system has a reputation for woefully uneven implementation. The Indian Penal Code, imported from the British in 1860 and written in a colonial language spoken by only 10 per cent of the population, is vastly limited in reach and effectiveness. (It was replaced by the Bharatiya Nyaya Sanhita – the ‘Indian Justice Code’ – in July 2024, but even that retains much of the IPC’s offences and language.) Cases that do make it to court are often bogged down by poor conviction rates and a liberal bail regime, factors that a female police officer writing in The Indian Express attributed to the law’s failure to deter rape culture as a whole. In 2022, out of 26,508 rape cases presented for trial in Indian courts, only 5,067 led to convictions – a 19 per cent conviction rate, compared with over 60 per cent in the UK that year. An investigation by the Mint newspaper that crossed national health survey data with official crime statistics estimated that 99 per cent of crimes in India remain unreported.\n\nThe women around me grew up enduring an inborn hostility against their gender and spent their entire lives accommodating it. They’d become almost blind to the manoeuvring and compliance necessary to keep themselves safe, as they cheered each baby step towards progress, hoping that things would get better. Unlike me, they did not have an escape hatch. It was simply the most bearable way to survive, and to do so with dignity. Why could I not be as strong? Why did I not have a thicker skin?\n\nI wondered if the carefully constructed cocoons of my companions ever frayed, whether they too had experienced violations that justified their instincts towards self-preservation. I knew better than to ask: they had professional relationships and reputations to protect, and an admission of harassment could destroy careers and bring shame upon extended families. Writing in New Lines Magazine in 2024, Surbhi Gupta says that: ‘Historically, a family or community’s honour has been tied to women’s virginity in India, and South Asia at large …’ It is this weight of women’s dignity that gives sexual assault its potency. And the shame it bestows transcends class lines. As a free-wheeling foreigner tied to no one’s honour, I had much less to lose.\n\nA few blocks north of the Metro, I found the Indiranagar police station. The reception area was low-ceilinged and lit with a stark fluorescence – and empty, apart from a few officers sprawled on battered wooden chairs, playing games on their smartphones. I walked up to the man at the front desk and told him I wanted to lodge a sexual harassment complaint. He led me up to the main room, where another elderly male officer was seated at a large table. He had a bushy grey moustache and serious, benevolent eyes; his hands neatly folded on top of a wrinkled ledger book. I sat across from him, pressed my palms to the sides of my legs, and laid out the facts. The date, the time, the street. The first incident, then the second. The triviality of my complaint became painfully apparent as I spoke, my small voice barely projecting in the sparsely furnished room. Despite everything, I had not been seriously harmed and never had reason to fear for my life, while every day across the country women suffered truly horrific atrocities. But I had already come this far.\n\n‘Understood,’ the officer was saying. He looked at me in half-amusement, making no move to record my story. The thick black cover of his ledger book was streaked with scratches. ‘And do you have the licence plate of the vehicle, ma’am?’ It had all happened too quickly for me to register what was going on, let alone note anything down.\n\nThere are offices in the area full of working women. Any man riding by could do whatever they want\n\n‘You know, it’s almost impossible for us to track that down, ma’am.’ I stared at him wordlessly. His whiskers were still. ‘Would you like to file an FIR complaint, ma’am?’ I considered this. The first information report I could provide was scant; besides, filing a complaint would result in an official record, traceable to my identity and my visa. I did not want to invite further trouble, or have anything further to do with the police.\n\nWould it be possible at all to install a streetlight around there? I asked instead, my tone pleading. The street is pitch black at night. There are offices in the area full of working women. Any man riding by could do whatever they want. It was a weak ask. Yet it would probably involve six government departments to get a streetlight up and running, none of which had to do with the police. ‘That would be very difficult,’ the officer confirmed, with a dance of his eyebrows. He leaned back in his chair. ‘Very complicated, I tell you. But maybe, we can send more officers to patrol the area.’\n\nA few days later, I spotted two police officers standing by the streetlamp next to the house. They slouched, chatting at leisure, not paying me any heed as I tramped past.\n\nTwo years after my New Year’s experience on Brigade Road, the Bengaluru event made the global news. Women who might have been Meghna and me had come to celebrate, and were molested en masse. CCTV footage showed mobs of revellers running down the streets and circling any lone women in the crowd, grabbing their necks, backs and waists. Hauling women into side streets, they forced their heads down to their crotches, then threw them to the ground while bystanders gaped. Responding to the ensuing outrage, the state home minister said: ‘Such incidents do happen on New Year[’s] day and on Christmas.’ He said enough police had monitored the event. The police insisted that no cases had been filed. By then, I was preparing to leave Bengaluru for places where the spectre of violence no longer hung over my head. But I felt heavy, as a now-familiar soup welled up within me – a stew of anger, despair and helplessness. But not surprise.\n\n",
    "scrapedDate": "2025-03-03T13:30:06.568Z",
    "wordCount": 6094
  },
  {
    "title": "Nothing lasts forever: not humanity, not Earth, not the Universe. But finitude confers an indelible meaning to our lives",
    "category": "Childhood and adolescence",
    "url": "https://aeon.co/essays/how-humanity-moved-from-eternal-to-bookended-time",
    "publishedDate": "",
    "content": "Do you recall the first time you knew you would die? It’s a milestone, realising your time is limited. That things happened before you, and will happen afterward, in your absence.\n\nAs we grow up, the understanding of death comes in stages, but it culminates in acknowledgment of one’s own – unavoidable yet unpredictable – mortality. Sometime between the ages of six and 10, children become aware that their time is inescapably bounded.\n\nRoughly the same might be said of humanity’s self-knowledge. Only recently has the human collective begun accepting the fact it is itself mortal. We now appreciate that events unfolded for aeons before us and that our species can disappear, never to return. One day, the cosmos will persist without human witness, nor any inherent tendency to manifest things we cherish.\n\nThe anti-war campaigner Jonathan Schell called this realisation the ‘second death’. Growing up, each of us comes to terms, psychologically, with a ‘first death’ – our own – but, beyond this, lurks the realisation that humankind itself hasn’t always existed and won’t be around forever.\n\nFor most of history, such understanding was lacking. People could defang – or outright deny – the possibility of beginnings and ends greater than those of our own biographies by appealing to eternity. Before we found evidence to prove otherwise, it was permissible to presume that, beyond tangible scales, time has no true bounds. For millennia, people have found comfort in this, because nothing dies in eternity. Given eternal time, every possibility – no matter how wildly improbable – will repeat and recur limitlessly. Outside our island of perceptible time – within eternity’s boundless ocean – it remained plausible to assume that all deceased things will eventually resurrect.\n\nIt’s now clear humanity lacks the luxury of eternity. We know this because evidence has accumulated to show that there are greater, even more encompassing mortalities than our own. We now understand Earth and its life had their origins and, one day, they will be cremated by our ageing Sun. A ‘third death’, then. Beyond that, even the Universe itself has its bounds: it began with a bang, and the consensus view is that, in the distant future, it will likely have its end. Thus, a ‘fourth death’. Multiple grander mortalities, expanding concentrically outward.\n\nWe are only just coming to terms with this – this supremacy of finitude. It marks a historic reorganisation of our sense of orientation that may, one day, be judged comparable to that of the Copernican Revolution. Just under 500 years ago, Nicolaus Copernicus initiated a string of discoveries eventually proving our planet is not the centre of a tidy, manageable cosmos. Instead, Earth pirouettes around a mediocre star within an ungraspably vast Universe. It took generations for people to start noticing – and giving names to – what Copernicus had wrought. Similarly, we are only now waking up to the significance of the nested mortalities we live within. With the most seismic revolutions, it takes time for the dust to settle before we can glimpse the landscape transformed.\n\nHowever, whereas Copernicus made us feel insignificant in space, placing bookends on time stands set to reverse this, by insinuating that our acts might just be more cosmically resonant than we previously dared presume. One revolution, undone by another.\n\nWhy? I argue that abandoning eternity is ultimately galvanising: for it implies that what we do in our moment on Earth genuinely matters beyond our own fleeting lives. Only by finding time’s cosmic bookends has modern science thrown into crisp relief the truth that what happens next might resonate indelibly. Because, if time has extremities, then history will never repeat; and if what’s happening here and now will never reoccur, certain decisions can never be taken back or reversed.\n\nHistorically speaking, when people have encountered something whose edges they cannot directly see, they have tended – ahead of contrary evidence – to assume it simply lacks limits. Staggered by size into stupor, we tend to conflate extremely large things with limitless things. It proved no different with time, beyond perceptible scales. For much of human history, what terra incognita was for geography, eternity was for chronology. It was the amorphous unknown lurking beyond what was concretely measured.\n\nWhile the Abrahamic religions may have appeared to place boundaries on time millennia ago – with their teachings of genesis and apocalypse – eternity nonetheless saturates these creeds. Worldly time was conceived of as but a brief hiatus, environed by two eternities: a vanishing, Biblical ‘vale of tears’ cleft between the boundless precedent of an uncreated divinity and, afterward, the unending afterlife promised for the souls He created.\n\nPerhaps one reason why the idea of eternity has persisted for so long is that it offers consolation. In infinite time, all possibilities inevitably cycle without limit, such that unprecedented change and irrecuperable loss evaporate. The only trajectory is an orbit. Everything gained is eventually lost; everything lost, eventually regained.\n\nHowever, it’s also the case that the evidence required to banish eternity has accumulated only gradually. Because knowledge is tethered to observation, we know only so far as we – or our tools – can see. Thus, before humans invented clever ways to survey scales greater than the immediate, it was impossible to disqualify hunches that – beyond the purview of what’s humanly tangible – time lacks proportion and eternity reigns.\n\nBecause imagination fails to circumscribe such aeons, it’s easy to assume they lack circumference entirely\n\nPeople speak of the ‘god of the gaps’, alluding to how the supernatural slowly recedes as we learn more about the cosmos around us. Something similar has happened with eternal time. As we’ve probed and pried farther, we’ve revealed beginnings – and possible ends – for Earth, life and the Universe, replacing eternity with finite timescales at increasingly encompassing scales. Yet, with each advance, eternity has merely ceded ground, retreating into the darkness outside the flame lit by our knowledge.\n\nEven in the present day, it’s often forgotten that the unthinkably vast durations revealed by modern science – described by the term ‘deep time’ – are distinct from eternities. Because imagination fails to circumscribe such aeons, it’s easy to assume they lack circumference entirely.\n\nIf accepting such immense finitudes is difficult now, it would have been unthinkable earlier in human history, before radio telescopes and radiocarbon dating. A millennium ago, it would have been evident human lives are not eternal. However, based on available evidence, it would have remained underdetermined as to whether, beyond this familiar span, things don’t just boundlessly cycle and repeat.\n\nThis mattered because it influenced how people behaved. If events inevitably reset, regardless of what happens in the present, everything accomplished now will eventually wash away. Assuming this, the Roman statesman Cicero concluded there was little point attempting to impart legacies that are ‘durable’, much less ones that are indelible.\n\nBefore an archaeological record was compiled, cross-referenced and communicated across continents – rather than remaining constrained within one – it wasn’t possible to falsify hunches that human history has been cycling on Earth, forever, without definitively beginning or drastically changing.\n\nWe envision a time before which no humans practised farming or writing, and a time after which some started\n\nSimilarly, before the globe had been fully mapped, people could plausibly believe that everything locally chronicled and accomplished had already taken place – and would later ceaselessly repeat – on other, uncontacted landmasses.\n\nIn Leviathan (1651), Thomas Hobbes spoke of the shift from a warring ‘state of nature’ to that of the ‘social contract’. This might now sound like what we understand as the transition from prehistory to history. We envision a time before which no humans, anywhere, practised farming or writing, and a time after which some started. But this wasn’t what Hobbes had in mind. He clarified that he believed the precivilisational state ‘was never generally so, over all the world’.\n\nFor Hobbes, global history represents a revolving, never a rupture. Regions might slither between various situations, but, from the planet’s perspective, these will all have already been manifested elsewhere. Assuming this, humankind’s collective future cannot come to look different from its extended past.\n\nHobbes’s contemporaries – from Francis Bacon to Edmond Halley – remarked that, though new things appeared to be being invented, they couldn’t disprove a lingering suspicion all these feats hadn’t already been innumerably achieved before – by forgotten civilisations, throughout time immemorial. Eternity resided over the horizon, in undiscovered lands.\n\nIn the 18th century, these perspectives began to change – but not without a fight.\n\nThroughout the 1700s, naturalists began looking to Earth to answer the question of the present’s location in time. Geology had begun consolidating as a science. Ahead of digging into the planet, the first geologists assumed they might find proof of nothing really changing throughout Earth’s history. They expected remains of everything currently alive on the surface to be located even in the deepest strata. Some predicted that, if we dug to Earth’s core, we would find human remains.\n\nWhen scientists began peeling Earth open, revealing the fossil record’s testimony, this isn’t what they found. Not only did the eldest strata entirely lack organic remains, they also noticed the skeletons changed drastically – becoming less familiar – the further back in time they looked. No one could find fossilised humans in the older strata, which suggested creatures like us were embarrassingly recent additions. (There was one false positive, but that turned out to be a giant salamander’s skeleton.)\n\nHomo diluvii testis: a fossil of a giant salamander found in a limestone quarry in Öhningen, Germany, and, in 1726, believed to be the spine and pelvis of a child who had been trampled during the Biblical flood. Courtesy BnF, Paris\n\nThe time was ripe for the first physical theory comprehensively predicting both the past and the future of the Earth system. This came from the French polymath Georges-Louis Leclerc, Comte de Buffon in the 1770s. He conjectured our planet was irreversibly losing an original and finite fund of internal heat. A molten birth, hurtling inexorably to frozen death.\n\nRetreating to his basement forge in Burgundy, Buffon heated iron balls then measured their cooldown times, before scaling them up to the globe’s size. From this, he estimated Earth was around 74,832 years old; that life emerged upon it 35,983 years ago; and that it would remain habitable for another 93,291 years. (Rounding wasn’t yet scientific convention.) Nonetheless, eternity wouldn’t depart without a struggle. Proof was building that Earth was far older and, with time’s limits seemingly bursting, some saw evidence of limitlessness.\n\nHutton insisted the past and future of the Earth system was not just enormously long but limitlessly long\n\nNot long after Buffon’s investigations, the Scottish geologist James Hutton argued that, in explaining Earth’s features, only currently observable causes should be conscripted. Overapplying this rule, he became convinced our planet had never looked drastically different from how it looks today, nor could it change in future. This inspired his declaration that geologists find ‘no vestige of a beginning, – no prospect of an end.’ He even questioned evidence there had been a time when our planet entirely lacked life, insisting the Earth system never undergoes unprecedented changes.\n\nBewitched by the aeons required to sculpt vast mountains using only mundane processes – like erosion or sedimentation, which work at a piecemeal pace – Hutton insisted the past and future of the Earth system was not just enormously long but, instead, limitlessly long. Sometimes hailed as the ‘discoverer of deep time’, this is misleading. Hutton was drunk on boundlessness – which is shallow because, in eternity, nothing truly changes – not meaningful depth.\n\nThis way, eternity briefly recurred: by receding into the mists of geological time. But, despite Hutton’s efforts, Buffon’s view – that Earth’s biography has determinate bookends in time – slowly won the day. Of course, Buffon’s methods were rudimentary, the results false, but it was an important start. His experiments – attempting to simulate Earth’s evolution in miniature – provide the common ancestor of today’s sophisticated climate models.\n\nUltimately, Buffon’s search for our planet’s biographical bookends provided the extremities within which life’s story could begin to be understood as unfolding – and in a way that could admit genuine firsts and irreversible losses.\n\nNo longer was it possible to hold that everything humanly or biologically possible has already occurred, given a boundless past. This way, bookending the past liberated the future. Hope for entirely new things – never before achieved, anywhere – could take hold.\n\nThe same, however, applied for global challenges entirely unprecedented: including disasters, more destructive than any previously recorded.\n\nIn 1777, Buffon asked himself what would happen if all organisms on Earth were instantaneously annihilated by some disaster. To his mind, life would spontaneously reappear, briskly repopulating the planet with, he assumed, the same species as before.\n\nThough the biography of Earth had been granted its bookends, the same hadn’t yet been confirmed for its myriad species. Planets were inchoately understood as things with a definite birth, a bounded lifespan and a foreseeable death; but it wasn’t yet definitively accepted that species also experience such milestones.\n\nThere wasn’t yet consensus on how species originate, so there couldn’t yet be conclusive grasp that, once lost, they are gone forever. In the earlier 1800s, naturalists continued to imagine that complex creatures could simply pop into existence without forebears. Hutton’s followers imagined dinosaurs one day, spontaneously, returning. Others theorised that the first humans were generated, effortlessly, from sea slime: no parents necessary.\n\nDarwin proved we were all born of time, not slime\n\nCharles Darwin changed this. After On the Origin of Species (1859), it became undeniable that nature cannot generate complicated creatures from scratch. The existence of us, today, required the precedent of countless yesterdays: building laboriously from simpler into less simple ancestors.\n\nThis confirmed why members of the same species cannot spawn anywhere, anytime, effortlessly, without connection to countless parents and precursors stretching back, uninterrupted, to life’s beginning. Darwin proved we were all born of time, not slime. Crucially, this also established why, if every member of a species is destroyed, that lifeform isn’t returning anywhere, ever again.\n\nThis way, Darwin proved that species, too, have bookended biographies – that they are born, persist, and perish – and that their stories unfold firmly within the horizon of consequential action. Deeds and decisions in the present can extinguish them, which leaves an indelible mark on Earth’s future, because they won’t ever be returning.\n\nThe idea of eternity clung on elsewhere though, beyond Earth, as our awareness of the voluminousness of the cosmos grew. In the wake of the Copernican Revolution, other planets became the arena wherein expectations of endless recurrence, and thus immortality, could be transposed.\n\nAfter astronomy revealed our planet is but one among uncountably many, it made people feel small. But it didn’t initially make them feel lonely. Far from it. Responding to the cosmic vastitudes revealed throughout the 1600s, Blaise Pascal admitted that the ‘eternal silence of these infinite spaces terrifies me’. But people forget what else he said: what terrified him wasn’t the prospect we were alone, but the opposite. He hated the ignominy of being unnoteworthy, or the idea of countless populated globes that ‘know nothing of us’. Because Pascal assumed all worlds host the same animals Earth houses – down to the ‘mites’ – such that all Earthly things must cosmically recur ‘without end and without cessation’. What alarmed him was how mundane this extramundane churn of living globes makes us.\n\nPascal was far from alone. Confronted by a Universe far larger than had previously been permitted by limited imaginations, people simply assumed it lacked limits entirely. Contemporaries, like the theologian Henry More, concluded that there will be ‘endless’ Earths and humankinds. Dizzyingly, he pictured infinite Adams coupling with innumerable Eves. Nothing in infinity is unique, nothing mortal.\n\nInitially, Copernicus’s revolution didn’t shrink humanity’s ego. It inflated it to cosmic catchments. And these assumptions would last, undefeated, until at least the early 20th century.\n\nConsider the words of Arnold Taylor, a reverend of a leafy English village, who in 1901 posted a letter to the editors of a science magazine, seeking consolation. Taylor had been spooked reading a book claiming there might not be humans on other planets. Such a statement, our reverend felt, was surely too ‘sweeping’?\n\nA century ago, many assumed the inhabitants of other planets would be humans\n\nTroubled, Taylor asked what astronomers – or, as he had it, ‘the planetoscopists’ – thought on the matter. Taking Mars as an example, he said he assumed the consensus was that it was ‘probable that human beings may be living there’.\n\nWhen it came to ‘planetoscopy’, Taylor had familial skin in the game. Decades earlier, his great-uncle W R Dawes had drawn the first detailed maps of Mars: highlighting what he mistook for waterways. This was taken as confirmation our neighbouring planet was inhabited.\n\nThis wasn’t an isolated view. A century ago, many like Taylor assumed the inhabitants of other planets would be humans; that everything happening here was also going on over there.\n\nBy this time, time’s bookends had expanded to subsume the entire solar system. Thanks to thermodynamics, it was now scientifically accepted that our Sun would one day definitively die, erasing the possibility of living worlds pirouetting around it. But what of systems beyond? Though stars might experience bookended biographies – ageing and dying – the Universe containing them was not thought to suffer such inconveniences. It was largely assumed that the cosmos, at large, was limitless and ageless.\n\nAccordingly, even though Darwin’s theory made it wildly improbable, people could still seek immortality for our species in the stars. Given an eternity of chances, even the most unlikely confluence of conditions will innumerably repeat. Hence, why, in the 1929 words of Robert A Millikan, a Nobel Prize-winning physicist, there will always be, somewhere, an Earth – ‘it matters not which’ – whereupon ‘some billion years hence the development of man still may be going on.’\n\nLemaître theorised the Universe was birthed by titanic detonation\n\nIn an eternal, infinite universe, such resurrection is inevitable. Even Darwin himself once alluded, furtively, to an evolutionary ‘fresh start’ following our Sun’s death. Such a thought was comforting, some began remarking, given that humans had begun messing with atoms.\n\nBut then the Universe itself started gaining bookends in time. Peering into the world’s largest telescope, throughout the 1920s, Edwin Hubble noticed something bemusing about other galaxies outside our Milky Way. They are rapidly flinging away from us. Our Universe is expanding.\n\nExtrapolating backwards, this implied an explosive beginning. Inferring forwards, it implied a rarefied end, with matter and energy eventually centrifuging into nihility. The Universe – as one giant physical whole – was gaining a beginning and an end.\n\nThe Belgian physicist Georges Lemaître pieced it all together first. He theorised the Universe was birthed by titanic detonation. In 1931, Lemaître proposed our cosmos isn’t unborn and undying, but can be compared to a fireworks display. Standing on a ‘well-chilled cinder’, we peer into space, witnessing the explosion’s ember-scattering aftermath.\n\nIn 1946, Lemaître published a book summarising his vision. Just three years later, speaking on BBC Radio, the astronomer Fred Hoyle absentmindedly referred to Lemaître’s theory as the ‘Big Bang’. The name stuck.\n\nFrom 1948 onward, Hoyle himself launched a counterstrike against this bookending of universal chronology. Like Hutton before him, he tried to defend eternity from time’s creeping encroach – albeit now at cosmological, rather than geophysical, scales. Hoyle sought theories that could explain cosmic expansion without postulating cosmic beginnings. But the Big Bang, eventually, started winning the day.\n\nThis had profound ramifications. Previously, in an eternal Universe, it was possible to assume that life had never begun, but simply circulates – and always has done – from system to system. Earlier generations of scientists speculated about ‘astroplankton’, or ‘space-spores’, which could accomplish this job of spreading life unceasingly.\n\nThis allowed conjecture that ‘organic beings are eternal like matter itself’. It also meant people could find comfort in claims that all life, throughout the Universe, is ‘related’: one circulating, immortal family; not isolated, unrelated refugees. What’s more, if you hold that life never properly began, it’s hard to imagine it ever ending. Something that’s everywhere, everywhen, is impossible to exterminate.\n\nWe must confront the possibility that life began and is thus a fragile thing\n\nThe notion of an eternal Universe had long underwritten recurrent hopes that our own species, regardless of improbability, would reoccur. Hence, why, as speculators had previously assuaged, the stars are ‘future regions of our immortality’: where there will always be ‘other suns, other earths, other humanities’; our ‘successors’ in mind’s ‘universal and eternal history’.\n\nThe discovery that the Universe itself appears to have bookends obliterated this. Noticing this in 1953, the anthropologist Loren Eiseley spelled it out. The old ‘idea of an eternal universe’, he wrote, allowed ‘an infinity of time in which man might arise again and again’. This provided assurance (or, better, insurance).\n\nYet, Eiseley realised, building evidence for the Big Bang demolishes such hope. No longer can we assume that life eternally circulates: drifting as ‘spores’ from ‘the wreckage of burned-out systems to systems beginning anew’. We must confront the possibility that it began and is thus a fragile thing: with more or less of a foothold in existence, depending on how widely distributed it is.\n\nThe Russian-born American cosmologist George Gamow concurred. In 1941, he announced that the Big Bang means the problem of life’s definite origin ‘has to be faced anew’. There was a time when the cosmos had no life in it, anywhere, and a time after which it did. This way, biology – as a cosmic phenomenon – became a biographical, bookending thing too. It is something that emerged and must eventually die out. Conclusions from thermodynamics continued to support this. One very distant day, our cosmos will spend all its useful energy, making complex life impossible, everywhere.\n\nThe Big Bang provoked even more disquieting questions. As Enrico Fermi asked in 1950: ‘Where, then, is everybody?’ He was talking about aliens. Previously, it was possible to assume interstellar journeys were impossible. If the Universe was eternal, we would already have been visited.\n\nBut suddenly it became permissible instead to conjecture we live in the cosmic epoch before which life, of whatsoever sort, has spilled outward: flooding throughout the firmament. Eiseley pointed this out, noting it now seemed ‘there has not been a sufficient period for all things to occur even behind the star shoals of the outer galaxies’.\n\nOnce again, bookending the past opened radical possibilities for the future. For Buffon’s generation, this unfolded at planetary scale; now it was unfolding at cosmological scale. It became at least plausible to propose that the Universe’s future might look unrecognisably different from its past and present. Might biology be the spark that inflicts the change?\n\nThe evolution of brains like ours, on Earth, can hardly be assumed to be an inevitability\n\nHumans were, at this time, themselves making strides towards building rockets. From this, further conundrums bubbled forth. Are we the first to have thought about hurling spaceships at other stars, and are thus radically alone in an otherwise insensate cosmos? Or, have countless others tried and failed? Indeed, atoms and rockets can lead to global suicide as well as interstellar emigration.\n\nLaunched in the 1960s to answer these riddles, the Search for Extraterrestrial Intelligence (SETI) has since found nothing; only cosmic silence. What’s more, as the second millennium shaded into the third, numerous discoveries have confirmed the evolution of brains like ours, on Earth, can hardly be assumed to be an inevitability. We are the offspring of chance: a fluke product of the ‘roulette wheels that are galaxies’, as Stanisław Lem put it.\n\nPerhaps this explains why we can’t tune into any sidereal hubbub. Perhaps we are simply the first, improbable beings to pose the question: Could we be the first?\n\nIf Earth’s life is unlikely and unprecedented, its ruination could therefore be a loss for the wider cosmos itself. Without predecessors, who’s to say what we might be capable of ultimately? There’s no precedent to learn from, but also no prior indication of limits on what might yet be achieved.\n\nDemonstrating that time has bookends took millennia of enquiry, but its implications are bedding in only now. It was only by discovering that time is unfathomably deep – but not limitless – that our species has realised we don’t have the excuse or exculpation of eternity. There will be no reruns, no other Earths. Dying here means dying forever. And for all we know, there are no other intelligences in the Universe ready to take the baton. No longer can we rest assured, knowing that mishaps down here may be compensated by identical histories, elsewhere, throughout eternally churning star shoals. As Eiseley put it: ‘the same hands will never twice build the golden cities of this world’.\n\nWhen we become aware of our own individual mortality – the ‘first death’ – we discover we have one, single opportunity with our own lives. We now must accept the same applies to life itself, cosmically speaking.\n\nThe yearning for immortality hasn’t left us, of course, but we now must look for it in Many-Worlds interpretations of quantum physics and other increasingly exotic, unreachable places. Where centuries ago, it was possible to hold that one’s history might repeat on other continents – and, later, on other planets – we now must seek the comforts of recurrence in parallel universes.\n\nThe degree to which we have banished eternity outward, from unexplored landmasses to the outskirts of spacetime, is the extent to which the stakes – of what unfolds here and now – have heightened.\n\nIf orienting ourselves in space generated the Copernican worldview, gaining our bearings in time is forging something new. The tenor of the former was mediocrity; the lesson of the latter is precarity.\n\nIt is the dying of the world that secures the immortality of our influence\n\nWhat’s more, this new sense of our orientation – within time’s wider expanses – appears to have coalesced just in time. Up until recently, thinkers took deep time as indication of the present’s triviality. Within such magnitudes, ‘now’ seemingly shrinks to nullity, with its legacies being laundered away in the longer term.\n\nBut such a view is wrong. What’s currently unfolding might leave legacies that cannot be taken back, and were not inevitable, but still will be felt aeons hence. Time isn’t just deep; it’s deeply fragile. This dizzying knowledge needs, urgently, to sink in. Either we apply it now, just in time, and secure our future, or there might not be one. We don’t have the luxury of infinite retries.\n\nSome may find it depressing to consider that, inevitably, everything dies. But they are missing one truth: it is only in a mortal world that consequences can resonate indelibly. After all, thermodynamics teaches that useful energy is finite in the Universe, and performing actions – of whatever sort – depletes this fund. Accordingly, the amount of actions that will ever get accomplished is finite, no matter how enormous this finitude might be. Selecting one action over any others possible, therefore, leaves an indelible cosmic legacy, regardless of how trifling. Because, though legacies can be reversed, this itself also expends effort, and energy isn’t infinite.\n\nSo, though the first lesson is that existence itself is bookended, the second – more profound – lesson is that this makes actions enduring in a newly cosmical sense. It is the dying of the world that secures the immortality of our influence.\n\nThis applies to modest goals as much as to hubristic, grandiose ones. We might call it the energetic imperative. Don’t let energy go to waste. Channel it towards what is beautiful, joyous, vivacious, ebullient! Because every moment we don’t, this ageing Universe forever becomes a less cacophonous, colourful place than it could otherwise have been.\n\nWhich is why this supremacy of finitude – this circumambience of grander mortalities than our own – is, in fact, galvanising. Just like the child coming to accept her ‘first death’, apprehending this might simply be part of growing up.\n\nUltimately, eternity intoxicates and stultifies, whereas finitude edifies by showing us that our decisions – right now – do matter. If immortality’s price is agency’s liquidation, I’d choose agency every time.\n\n",
    "scrapedDate": "2025-03-03T13:30:08.262Z",
    "wordCount": 4739
  },
  {
    "title": "In order to bridge the yawning gulf between the humanities and the sciences we must turn to an unexpected field: mathematics",
    "category": "Medicine",
    "url": "https://aeon.co/essays/to-better-understand-the-world-follow-the-paths-of-mathematics",
    "publishedDate": "",
    "content": "In 1959, the English writer and physicist C P Snow delivered the esteemed Rede Lecture at the University of Cambridge. Regaled with champagne and Marmite sandwiches, the audience had no idea that they were about to be read the riot act. Snow diagnosed a rift of mutual ignorance in the intellectual world of the West. On the one hand were the ‘literary intellectuals’ (of the humanities) and on the other the (natural) ‘scientists’: the much-discussed ‘two cultures’. Snow substantiated his diagnosis with anecdotes of respected literary intellectuals who complained about the illiteracy of the scientists but who themselves had never heard of such a fundamental statement as the second law of thermodynamics. And he told of brilliant scientific minds who might know a lot about the second law but were barely up to the task of reading Charles Dickens, let alone an ‘esoteric, tangled and dubiously rewarding writer … like Rainer Maria Rilke.’\n\nSixty-plus years after Snow’s diatribe, the rift has hardly narrowed. Off the record, most natural scientists still consider the humanities to be a pseudo-science that lacks elementary epistemic standards. In a 2016 talk, the renowned theoretical physicist Carlo Rovelli lamented ‘the current anti-philosophical ideology’. And he quoted eminent colleagues such as the Nobel laureate Steven Weinberg, Stephen Hawking and Neil deGrasse Tyson, who agreed that ‘philosophy is dead’ and that only the natural sciences could explain how the world works, not ‘what you can deduce from your armchair’. Meanwhile, many humanities scholars see scientists as pedantic surveyors of nature, who may produce practical and useful results, but are blind to the truly deep insights about the workings of the (cultural) world. In his best-selling book The Fate of Rome (2017), Kyle Harper convincingly showed that a changing climate and diseases were major factors contributing to the final fall of the Roman Empire. The majority of Harper’s fellow historians had simply neglected such factors up to then; they had instead focused solely on the cultural, political and socioeconomic ones.\n\nIn my own book, The Oracle of Numbers: A Short Philosophy of Mathematics (2023), currently only available in the original German, I tried to counter this intellectual parochialism. During my academic training in mathematics, physics and philosophy, I witnessed many instances of this narrow-mindedness and always wondered why highly intelligent people in these fields guarded themselves against major insights from the other fields. I wanted to motivate them, and the inquisitive general public, to open their minds and see that the neverending quest for a better understanding of the world follows many paths.\n\nLudwig Wittgenstein once said: ‘I want to show the colourfulness of mathematics.’ In that spirit, I placed mathematics at the centre of my project because, in my view, mathematics searches along more of these many paths than any other intellectual discipline. It is connected on a deep level both with the natural sciences and the humanities. It bridges the gulf between them, and it does so by putting certain metaphysical and epistemological dogmas into question, as will become clear in the following.\n\nThe divide between the two cultures is not just an academic affair. It is, more importantly, about two opposing views on the fundamental connection between mind and nature. According to one view, nature is governed by an all-encompassing system of laws. This image underlies the explanatory paradigm of causal determination by elementary forces. As physics became the leading science in the 19th century, the causal paradigm was more and more seen as the universal form of explanation. Nothing real fell outside its purview. According to this view, every phenomenon can be explained by a more or less complex causal chain (or web), the links of which can, in turn, be traced back, in principle, to basic natural forces. Anything – including any aspect of the human mind – that eludes this explanatory paradigm is simply not part of the real world, just like the ‘omens’ of superstition or the ‘astral projections’ of astrology.\n\nOn the opposing view, the human mind – be it that of individuals or collectives – can very well be regarded separately from its physical foundations. Of course, it is conceded that the mind cannot work without the brain, so it is not entirely independent of natural forces and their dynamics. But events of cultural significance can be explained as effects of very different kinds of causes, namely psychological and social, that operate in a sphere quite separate from that of the natural forces.\n\nThese divergent understandings underpin the worldviews of each culture. Naive realists – primarily natural scientists – like to point out that nature existed long before humankind. Nature is ordered according to laws that operate regardless of whether or not humans are around to observe. So the natural order of the world must be predetermined independently of the human mind. Conversely, naive idealists – including social constructivists, mostly encountered in the humanities – insist that all order is conceptual order, which is based solely on individual or collective thought. As such, order is not only not independent of the human mind, it’s also ambiguous, just as the human mind is ambiguous in its diverse cultural manifestations.\n\nMathematics mediates a conciliatory view that avoids the mistake of the naive realist and the naive idealist\n\nThe clash of cultures between the humanities and the natural sciences is reignited over and over because of two images that portray the interrelationship of mind and nature very differently. To achieve peace between the two cultures, we need to overcome both views. We must recognise that the natural and the mental order of things go hand in hand. Neither can be fully understood without the other. And neither can be traced back to the other.\n\nThe naive realist and the naive idealist fall prey to the same error, albeit in opposite directions – which gets us some way to the essence of the disagreement between the two cultures. Both confuse determination with explanation. ‘Determination’ refers to the emergence of a real-world phenomenon through a more or less complex web of cause-and-effect relationships. For example, when particle physics tells us that the Northern Lights result from solar winds colliding with Earth’s atmosphere, we get an explanation of the luminous phenomenon by having its main causally determining factor pointed out to us. Similarly, when psychological research informs us about the potentially long-lasting effects of trauma, we can explain the behaviour of victims of childhood abuse, to a certain extent, as a result of how repressed memories influence their actions. Now, the realist and the idealist – the scientist and the literary intellectual – both agree that explaining a phenomenon always means, in essence, revealing such determining causal relationships. Their views differ only in that for the realist the causal network is rooted in a firm natural basis, whereas for the idealist the basis is conceptual and therefore dependent on a contingent cultural embedding.\n\nThe best mediator of a conciliatory view that avoids the mistake of the naive realist and the naive idealist is mathematics. Mathematics gives us shining proof that understanding some aspect of the world does not always come down to uncovering some intricate causal web, not even in principle. Determination is not explanation. And mathematics, rightly understood, demonstrates this in a manner that lets us clearly see the mutual dependency of mind and nature.\n\nFor mathematical explanations are structural, not causal. Mathematics lets us understand aspects of the world that are just as real as the Northern Lights or people’s behaviour, but are not effects of any causes. The distinction between causal and structural forms of explanation will become clearer in due course. For a start, take this example. Think of a dying father who wants to pass on his one possession, a herd of 17 goats, evenly to his three sons. He can’t do so. This is not the case because some hidden physical or psychological forces hinder any such action. The reason is simply that 17 is a prime number, so not divisible by three.\n\nThose who remember mathematics only as a dull school subject that is mainly about applying unmotivated formulas in the context of uninspired exercises will naturally be sceptical about my claim that mathematics can bridge the gulf between the two cultures. The usual presentation of mathematics to the broader public makes it appear, at best, as a useful auxiliary discipline for the natural and technological sciences, or, at worst, as a curious collection of cute logical tricks. How could such a discipline possibly contribute to a better understanding of the interrelationship between mind and nature?\n\nA first idea is given by Riemannian geometry, the foundations of which were laid in the mid-19th century by Bernhard Riemann, building on the work of his teacher Carl Friedrich Gauss. Gauss was fascinated by the intrinsic curvature at any point on the interior of a smooth surface. What does this mean? Take a flat sheet of paper. It has zero curvature everywhere, and this remains so even after it is rolled up into a play telescope; the apparent curvature is merely extrinsic, it exists only relative to the surrounding three-dimensional space. In contrast, the surface of a sphere has a non-zero intrinsic curvature everywhere. The surface doesn’t just look crooked, it really is crooked – in and of itself, so to speak.\n\nRiemann elevated Gauss’s concept of intrinsic curvature to more than two dimensions. Ever since, one can meaningfully and precisely ask whether the ‘space around us is curved in itself’. Before Riemann, this would have been a meaningless sequence of words, at best with some associative force in the hands of a gifted poet. Riemann’s geometry made conceivable what was not only unimaginable before, but unthinkable in the truest sense of the word.\n\nThe example of Riemannian geometry not only demonstrates the ability of mathematics to broaden our intellectual horizons with new perspectives. It also shows how this intellectual-aesthetic advantage can yield unexpected scientific, and even practical, benefits: it provided the conceptual resources Albert Einstein needed to develop his general theory of relativity. Thus, Einstein was able to see gravity not as a force causing material objects to attract each other, but as a purely geometrical, ie, structural, element of the Universe. The Universe is curved in itself. And, since the Big Bang, it has been expanding, but not into a pre-existing surrounding space. The Big Bang was no explosion that ejected matter outwards (causal); there was and is simply no ‘outside’. Rather, the Universe is expanding in the sense of a certain continuous change of its intrinsic curvature (structural). It is thanks to Einstein, and Edwin Hubble, that we know this and can use it for useful things like the GPS. But it is only thanks to Riemann that we can think it at all.\n\nMind and world are no separate spheres that must first be connected. Rather, both depend on each other\n\nThe difference between causal and structural explanations becomes clearer with regard to Immanuel Kant’s conception of ‘transcendental idealism’, as laid out in his Critique of Pure Reason (1781). For Kant, empirical experience is possible only within a given conceptual framework, which in turn does not arise from empirical experience, but rather constitutes the human mind. And a crucial part of this framework is Euclidean geometry. The world is not ‘in itself’ populated by spatially and temporally delimited objects that interact with each other in a variety of ways. According to Kant, the world is structured this way because the human mind is able to grasp it in this way, and in this way only. Euclidean geometry, so he believed (he couldn’t know anything about Riemannian geometry yet), determines the spatial dimensions of this order. Thus, geometry is just as much a study of an elementary trait of our mind as of the space around us.\n\nKant’s transcendental idealism doesn’t only suffer from the fact that Euclidean geometry turned out to be not quite as constitutive as he thought. More severely, his conception of empirical knowledge, as an act of understanding through which conceptually formed ‘judgments’ miraculously emerge from mere ‘sensations’, remains completely obscure, as even well-meaning readers of the Critique must admit. But we can attribute at least one fundamental insight to Kant: mind and world are no separate spheres that must first be connected, so that the question arises as to how exactly this might be achieved. Rather, both depend on each other. Just as the world does not simply prescribe spatial, temporal and other structures that our mind then has to decipher, the mind is not free to impose any structure on the world at will. This is most impressively demonstrated by mathematics, which, despite the lack of empirical restrictions, does not fall into wild speculation.\n\nMathematics eludes the causal paradigm of explanation. Its explanations are of a very different kind than those in terms of cause and effect, as another look at the relationship between geometry and physics shows. Ten years before his general theory of relativity, Einstein had in 1905 presented to an astonished world its preliminary form, the special theory of relativity. And in this theory, a special form of Riemannian geometry devised by Hermann Minkowski plays a key role, with rather peculiar consequences. The most notorious is the so-called ‘twin paradox’. A pair of twin brothers have applied to take part in a space mission, but only one of them has been selected. The aim of the mission is to travel to the farthest regions of space, thanks to the latest rocket technology at nearly the speed of light. After the chosen brother returns to Earth, still a young man, he visits his brother and reencounters an old man.\n\nIt is quite common to explain this ‘paradox’ as a consequence of the accelerating forces acting on the travelling brother in his rocket. (See, eg, Richard Feynman’s Lectures on Physics.) And indeed, it seems very plausible, even inevitable, to assume that some physical cause must be responsible for the age difference. But that is false, the true reason lies in the structural framework within which the forces occur: the geometry of spacetime. Space and time cannot be considered separately, rather they are intertwined in an overarching common structure. And within this structure, the path, or ‘world line’, of the astronaut on the mission is simply shorter than that of his brother on Earth. This is not a cause, but – in Kant’s terms – a basic aspect of our transcendental explanatory framework.\n\nMathematics highlights the limits of natural scientific explanation. This becomes even clearer when we consider how the idea of an all-explanatory physical theory or ‘world formula’ came about in the first place. In other words, how did scientists come to believe, or at least hope, that a mathematical description of nature on the most fundamental level exists, with which every worldly phenomenon is explainable in the sense that its entire causal history can be derived from basic laws, at least in principle? The starting point was the mechanistic view of the world, according to which all physical systems consist of tiny, indivisible particles that either interact with each other like billiard balls through direct collision, or by means of remote forces. It was Isaac Newton’s mathematical model of classical mechanics that gave this idea its standing, in conjunction with his law of gravity. Newton’s model allowed him to explain an astonishing wealth of very different phenomena, such as the movements of the planets around the Sun and the falling of apples from trees, in a uniform way.\n\nBefore Newton, nobody thought of an ‘all-explaining’ physical theory. There were various natural phenomena for which individual explanations were sought. For example, Ptolemy’s ‘epicycle theory’, which was based on complex nested circular movements, was responsible for the movements of the planets around Earth. Completely different explanations were given for the falling of objects on Earth. But then Newton appeared on the scene with his model – and with him ‘Laplace’s demon’. This figure, presented by Pierre-Simon Laplace in 1814, not only knew Newton’s laws, but also the locations and velocities of all particles in the entire Universe at any given time, as well as the forces acting on them. According to Laplace, this enabled the demon to calculate the exact state of the entire Universe for any point in time, be it in the future or the past.\n\nOf course, to Laplace and his contemporaries, the demon was a mere fictional figure, but one with a true core. If a physical system can be regarded as ‘closed’, ie, as one whose interactions with its environment are negligible, a scientist can in principle predict the behaviour of the system with arbitrary accuracy. All she has to know with sufficient accuracy are the forces acting within the system, as well as the initial spatial distribution of the elementary system components and their velocities at the relevant point in time. Such a demonic scientist would possess what Dr Frankenstein dreamed of: ‘absolute’ control and penetration of the functioning of any physical system, including humans and their life and thought processes.\n\nThe key to an explanation is the central limit theorem, a fundamental result of mathematical probability theory\n\nSo it may seem, but it is a fallacy. Laplace’s demon can predict the behaviour of any physical system exactly, and it can also deduce in detail the state of the system at any time in the past. But even the demon does not understand every aspect of the system’s behaviour. Again, determination is not explanation. Take, for example, the height of every adult in Paris. Laplace’s demon can calculate the exact body lengths of all adult Parisians. And it can explain for each of them exactly how their genes, diet and other environmental influences led to them reaching their respective heights. As another example, if we throw lots of coins in the air at once and note the number of coins showing heads after landing on the floor, and we repeat this many times, the demon can predict for each iteration and each coin whether it will show heads or tails. And the demon can explain in detail how each outcome was causally determined.\n\nBut there is a fact the demon cannot explain. If we plot in a diagram the frequencies with which the different body lengths or counts of heads occur, the result is, in both cases, an approximate bell curve. Why? The forces acting in both cases are obviously very different. Nevertheless, the overall distributions of the heights and of the heads’ counts are very similar. Laplace’s demon can’t explain this fact, at least not if its theoretical knowledge is limited to the causally determining laws of nature.\n\nThe key to an explanation is the central limit theorem, a fundamental result of mathematical probability theory. The theorem states that, under appropriate conditions, a certain version of the mean of a sample converges to the so-called standard normal distribution (visually the bell curve). But probability theory is not entailed by those natural laws. In particular, the central limit theorem doesn’t follow from them. Laplace’s demon couldn’t even formulate the theorem. Very different concepts are needed to state the theorem than those of differential calculus, which are essentially sufficient for the laws of nature, at least in their Newtonian form.\n\nPhysics has undergone many changes since Laplace, most notably the emergence of relativity theory and quantum mechanics. Today we know that the world is not populated by tiny, indivisible billiard balls from which all material objects are composed in more or less complex ways. And Newton’s model is flawed in its geometric foundation alone. But that doesn’t change the argument. Even if these two currently incompatible pillars of physics – relativity and quantum physics – can someday be brought together in a unified theory, there will still be mathematical explanations of physical phenomena (eg, the approximate normal distribution of body lengths in a population or of the counts of heads in repeated coin tosses) that do not follow from the unified theory.\n\nIn his ‘two cultures’ speech, Snow located mathematics clearly in the camp of the sciences. But, as we have seen, mathematics doesn’t adhere to the explanatory paradigm of causal determination. This distinguishes it from the natural sciences. Nevertheless, mathematics tells us a lot about nature. According to Kant, it does so because it tells us a lot about the human mind. Mind and nature are inseparable facets of the world we inhabit and conceive. So, why should the humanities not also count as a science? They can tell us just as much about that one world on a fundamental level as the natural sciences. Mathematics demonstrates this clearly.\n\nSome naive realists like to make a clever move here. As persistent adherents of scientism – ie, the metaphysical doctrine that only the concepts and methods of enquiry used by the natural sciences, or more specifically physics, can account for what’s real – they simply attribute to the unified theory everything that could possibly prove to be a ‘useful’ part of mathematics now or in the future. But this semantic sleight of hand does not solve the basic problem of scientism: the world is structured in diverse ways, and in many different degrees of abstraction. On the most fundamental level are relations that determine the spatial and temporal order of events. Within this geometric structure, intricate and diverse causal structures arise that allow us, if well enough understood, to explain many of the phenomena emerging from them. But there are also more abstract structures, for instance those of probability theory, or for that matter psychology, sociology, linguistics, etc. And there is no scientific criterion, understood in the narrow sense of scientism, that could tell us which of these structures really do shape the world and which ones we only choose to see the world through.\n\nThose naive realists who claim that only the natural sciences can capture the structure of the world face a dilemma. Either they reduce the role of mathematics to a minimum and claim that only those mathematical models are permissible that refer to structures determined by the natural sciences beforehand. Then, however, they are committed to an implausible position according to which higher-level abstract relations as they are established, for example, by the central limit theorem are mere ‘projections’ onto the world – not an essential part of it. Or else they recognise such relations as ‘scientific’ too. But then they have to present a convincing argument as to why, of all non-empirical, conceptual-analytical disciplines, whose standards of validity are fundamentally different from those of the natural sciences, only mathematics should be admitted into the illustrious circle of the ‘true sciences’.\n\nA simulation is not an explanation. It allows us to predict, but it doesn’t let us understand\n\nI am not aware of any such argument. There are indeed rational standards by which we can distinguish true from merely imagined structures; standards as to what constitutes an objective measurement, a reliable observation, a valid deduction, or a cogent argument. However, these standards are much more complex and discursive than proponents of scientism want to admit. To simply state from the pulpit that only those structures are true that they have to accept as true in order to avoid the first alternative of the dilemma, the reduction of the role of mathematics to a completely implausible minimum, is not very convincing.\n\nMathematics undermines the causal explanatory paradigm not only in its natural scientific manifestations, but also in its uses in the humanities. We give explanations for a wide variety of phenomena by hidden causes way too often and way too fast, where the simple admission to having no explanation would not only be more honest, but also wiser. Wittgenstein spoke of the disease of wanting to explain. This disease shows itself not just in our private everyday exchanges and in the usual public debates, but also in scholarly discourse of the humanities. When confronted with individual or collective human thinking and behaviour, it is tempting to assume just a few underlying factors responsible for the thinking and behaviour. But, more often than not, there really is no such neat, analysable set of factors. Instead, there is a vast number of natural, psychological and societal factors that are all equally relevant for the emergence of the phenomenon one wants to explain. Perhaps a high-end computer could incorporate all these factors in a grand simulation. But a simulation is not an explanation. A simulation allows us to predict, but it doesn’t let us understand.\n\nThe aim of the humanities should not be to identify causes for every phenomenon they investigate. The rise and fall of empires, the economic and social ramifications of significant technological innovations, the cultural impact of great works of art are often products of irreducibly complex, chaotic processes. In such cases, trying to mimic the natural sciences by stipulating some major determining factors is a futile and misleading endeavour.\n\nBut mathematics shows that beyond the causal chaos there can be order of a different kind. The central limit theorem lets us see and explain a common regularity in a wide range of causally very different, but equally complex, natural processes. With this and many other examples of structural mathematical explanations of phenomena in the realm of the natural sciences in mind, it seems plausible that mathematical, or mathematically inspired, abstraction can also have fruitful applications in the humanities.\n\nThis is by no means meant to promote an uncritical imitation of mathematics in the humanities and social sciences. (The overabundance of simplistic econometric models, for instance, is a huge warning sign.) Rather, it is meant to motivate scholars in these fields to reflect more upon where and when causal explanations make sense. Complexity can’t always be reduced to a graspable causal explanation, or narrative. To the contrary, often the most enlightening enquiries are not those that propose new factors as the true explainers, but those that show by meticulous analysis that far more factors are crucially in play than previously thought. This, in turn, should motivate scholars to seek aspects of their subject of interest beyond causality that are both relevant and amenable to structural forms of explanation. Besides probability theory, chaos theoretical methods and game theory come to mind as mathematical sub-disciplines with potentially fruitful applications in this regard.\n\nThe mental world and the physical world are one and the same. The respective sciences deal with different aspects of this one world\n\nHowever, the main point of our discussion is not that mathematical applications in the humanities might bridge the gap between the natural sciences and the humanities. The point is that mathematics, not really belonging to either camp, shows them to be on an equal footing from the start. The natural scientific paradigm of explanation is not the role model any respectable form of enquiry has to follow. Mathematics shows that natural causes can’t explain every phenomenon, not even every natural phenomenon and not even in principle. So, there is no need for the humanities, the ‘sciences of the mind’, to always strive for explanations by causes that can be ‘reduced’ to more elementary, natural forces. Moreover, mathematics shows that causality, of any kind, is not the only possible basis on which any form of explanation ultimately has to stand. Take for example the semantic relationships between many of our utterances. It is not at all clear that these can be explained in terms of psychological causes, or any other causes. It is not unreasonable to believe that the world is irreducibly structured, in part, by semantic relations, just as it is structured by probabilistic relations.\n\nThis insight indicates a possible reconciliation of the natural sciences and the humanities. It implicitly refers to something that Richard Rorty explicitly expresses in Philosophy and the Mirror of Nature (1979) as follows:\n\nThe divide between the natural sciences and the humanities does not stem from the supposed fact that only those mental phenomena are real that are explainable in natural-scientific terms. Nor is the divide due to some extra-natural mental order, determined by causal relationships of a very different kind than those studied in the natural sciences. The mental world and the physical world are one and the same world, and the respective sciences deal with different aspects of this one world. Properly understood, insofar as they deal with the same phenomena, they do not provide competing but complementary descriptions of these phenomena.\n\nMathematics provides the most impressive proof that a true understanding of the world goes beyond the discovery of causal relationships – whether they are constituted by natural or cultural forces. It is worth taking a closer look at this proof. For it outlines the bond that connects mind and nature in particularly bright colours. Kant understood this bond as a ‘transcendental’ one. The late Wittgenstein, on the other hand, demonstrated its anchoring in language – not in the sense of a purely verbal and written practice, but in the sense of a comprehensive practice of actions the mental and bodily elements of which cannot be neatly separated. In the words of Wittgenstein, ‘commanding, questioning, recounting, chatting are as much a part of our natural history as walking, eating, drinking, and playing.’\n\nMathematics too is part of this practice. As such, like every science, it is inseparably rooted in both nature and the human mind. Unlike the other sciences, this dual rootedness is obvious in the case of mathematics. One only has to see where it resides: beyond causality.\n\n",
    "scrapedDate": "2025-03-03T13:30:10.367Z",
    "wordCount": 4915
  },
  {
    "title": "For most of history, the Moon was regarded as a mysterious and powerful object. Then scientists made it into a destination",
    "category": "Computing and artificial intelligence",
    "url": "https://aeon.co/essays/how-the-scientists-of-the-1960s-turned-the-moon-into-a-place",
    "publishedDate": "",
    "content": "On 25 May 1961, the US president John F Kennedy announced the Apollo programme: a mission to send humans to the Moon and return them safely to Earth within the decade. The next year, the American geologist Eugene M Shoemaker published an article on what it would take to accomplish the goal in American Scientist. It is an extraordinary document in many ways, but one part of his assessment stands out. ‘None of the detailed information necessary for the selection of sites for manned landing or bases is now available,’ Shoemaker wrote, because there were ‘less than a dozen scientists in the United States’ working on lunar mapping and geology.\n\nIt can be easy to take our maps, images and story of the Moon for granted. But over the past six decades, our cultural and scientific relationship with the Moon has been radically altered. Multiple robots landed on the Moon last year, and more are on the way. The Moon is a place and a destination – but this was not always the case.\n\nTo geographers and anthropologists, ‘place’ is a useful concept. A place is a collision between human culture and physical space. People transform their physical environment, and it transforms them. People tell stories about physical spaces that make people feel a certain way about that space. And people build, adding to a space and transforming it even further.\n\nSome scholars have started using these concepts to think about extraterrestrial locations. In her book Placing Outer Space (2016), the Yale anthropologist Lisa Messeri observes that scientists often think about planets, both in our solar system and beyond, as places. Sometimes this is explicit, as in the case of a series of talks given by Carl Sagan titled ‘Planets Are Places’. In other cases, scientists express a sense of place indirectly through their practices and language. Messeri observes that planetary scientists conduct place-making primarily through ‘narrating, mapping, visualising, and inhabiting’ other worlds. ‘Importantly,’ Messeri writes, ‘one can be (or can imagine being) in a place. Place suggests an intimacy that can scale down the cosmos to the level of human experience.’\n\nThis idea stuck with me as I surveyed the history of lunar science and the history of exploration. It seemed that, even before the emergence of planetary science as a recognisable discipline, people were slowly but surely applying the practices of place-making to the Moon.\n\nCenturies ago, a major reconceptualisation took place that made it possible for many to imagine the Moon as a world in the first place. New technologies enabled early scientists to slowly begin the process of mapping the lunar surface, and to eventually weave narratives about its history. Their observations and theories laid the groundwork for others to imagine the Moon as a rich world and a possible destination.\n\nThen, in the 1960s, the place-making practices of these scientists suddenly became practical knowledge, enabling the first visitors to arrive safely on the lunar surface.\n\nFor much of history, the Moon was a mythological and mathematical object. People regarded the Moon as a deity or an abstract power and, at the same time, precisely charted its movement. It seemed to influence events around us, and it behaved in mysterious ways.\n\nThe Egyptians and the Babylonians were eager to understand and predict the motions of celestial objects, including the Moon. The Babylonians created extensive tables of measurements showing the position of the Sun and Moon at different times of the year. Using these tables, they could predict the appearance of new moons and the timing of eclipses. Theirs was one part of an evolving and interconnected mathematical tradition involving Egyptian, Greek, Arab and Hindu scholars.\n\nTo Aristotle, the Moon was embedded in a giant sphere with the other planets and the stars\n\nThis mathematical understanding was not divorced from the mythological. For many people in these cultures, predicting the motion of these objects meant being able to access religious truth or the ability to predict events here on Earth. Understanding the Moon mathematically meant gaining astrological or religious understanding.\n\nGreek scholars applied mathematical analysis to the Moon, but they also began to theorise about physical mechanisms for the motions of the planets. To the philosopher Aristotle, the Moon was embedded in a giant sphere with the other planets and the stars. Earth sat at the centre of these nested spheres, which moved around Earth. These spheres were composed of a fifth element, aether, and they were incapable of any sort of change. Even if they were somehow worlds in themselves, in the Aristotelian model, the Moon and its sphere were an impenetrable barrier between our world and the celestial realm. The universe was literally divided into the sublunary and the superlunary.\n\nIn this view, the Moon is a physical object of sorts. But its nature makes it entirely divorced from our understanding, and completely inaccessible. Both of these qualities preclude the Moon from being considered a ‘place’ in any practical sense.\n\nThere were some who thought about trips to the Moon. Stories in religious traditions across the world tell of people travelling to the Moon. There were some thinkers before and after Aristotle who imagined that there were more worlds than just Earth. The ancient atomists discussed the possibility of worlds other than Earth, while other Greeks discussed the possibility of life on the Moon. This included Plutarch, who wrote about the Moon as both mythical and a physical object. But, to the extent that the Moon was thought about as a place, the notion was largely speculative or religious. And, by the medieval era, influential scholars rejected the idea.\n\nIn the Islamic empires and in Europe, the Aristotelian notion gained hold over the scholarly worldview. The Greek astronomer Ptolemy and others tried their best to modify this model to get better predictions of the motion of celestial objects. The more they did this, the more complex their suggested mechanisms became. The Moon was largely treated as one component of a complex geometry problem. Then, in the 17th century, European astronomers broke free from Aristotle’s grip, and laid the theoretical foundations for the Moon to become a place.\n\nThe first part of the story has been told often and well. The German astronomer Johannes Kepler built on the Copernican hypothesis that the Sun, not Earth, was at the centre of the system, overturning the Aristotelian notions of celestial mechanics and physics. Then the Pisan polymath Galileo’s view of the Moon through his telescope revealed an imperfect and rugged lunar surface, and new objects that did not seem to orbit Earth.\n\nFrom Sidereus Nuncius (‘Starry Messenger’, 1610) by Galileo Galilei. Courtesy the National Central Library of Florence\n\nAs they prepared new maps in the 20th century, scientists looked back on the work that had preceded theirs, enabled by early telescopes of eras past. In their book Mapping of the Moon: Past and Present (1974), the astronomer Zdeněk Kopal and the cartographer Robert W Carder present a sequence of what they considered important milestones in telescopic mapping.\n\nGalileo made illustrations of the Moon at the start of the 17th century, competently displaying the textures and shadows that he saw through the telescope. The English astronomer Thomas Harriot also made illustrations of his view through the telescope in 1609, and produced more map-like drawings of the visible features. But Harriot did not include any topography or texture. Later maps would combine these techniques, and add more elements that contributed to the sense of the Moon as a place. It was also around this time that astronomers started naming the features they saw.\n\nBy Thomas Harriot, 26 July 1609. Courtesy Lord Egremont/West Sussex Archives/Petworth House\n\nA Jesuit priest from Ferrara, Italy, invented the major lunar place names that we use today. His predecessors had created names based on their benefactors and confined to specific kingdoms. But Giovanni Battista Riccioli gave the ‘seas’, or ‘maria’, poetic Latin names. He named the various craters after philosophers and scientists from antiquity through to the modern era. If you look at his map from 1651, drawn up by Francesco Maria Grimaldi, you see familiar names such as Copernicus and Ptolemy, Mare Imbrium and Mare Tranquillitatis.\n\nAll this was hard to swallow prior to the 18th century, and it took philosophical and religious arguments to convince people in Europe. A contemporary of Galileo, the natural philosopher and bishop John Wilkins, laid out the case for this claim in his treatise titled The Discovery of a World in the Moone (1638). He crafted a fascinating argument, built on the observations of Kepler and Galileo, and couched in the rhetoric of his time.\n\nEarly telescopic observers explored the Moon long before the Apollo astronauts arrived\n\nHis first proposition is simply that ‘the strangeness of this opinion is no sufficient reason why it should be rejected.’ For context, he described other ideas once considered ‘ridiculous’ but later shown to be true. And because he worried that religious arguments would doom the idea of the Moon as another world, he addressed the problem of incorporating Aristotelian doctrine into the Christian concept of the universe.\n\nAfter addressing these potential objections, Wilkins laid out his full argument in 12 more propositions. Depending on how you read them, about five are based on direct observation. He included evidence that the Moon has a solid body, that there are mountains, valleys and an atmosphere. The rest of the propositions are largely speculations about what this world might be like.\n\nWilkins imagined that such a world would not have been created by God without the intention that it support life. Wilkins interpreted the darker and lighter spots on the Moon as oceans and land, respectively. To Wilkins, this implied that the land must be inhabited, but he did not believe we could know much about the people who lived there.\n\nThe work is a fascinating look at the rapidly shifting intellectual currents of the 17th century. Galileo’s observations and Wilkins’s argument show that this idea was beginning to be taken seriously. In the coming centuries, with the telescope at the forefront, astronomers began to map the surface of the Moon, charting the mountains and the ‘seas’ of a new world.\n\nBut it’s Johann Tobias Mayer, an 18th-century German cartographer, whom Kopal and Carder credited as ‘the veritable father of lunar cartography’. Mayer used measurements with a micrometer to create a precise coordinate system for his mapping effort. The full versions of his maps, published after Mayer’s death, feature crisscrossing lines of longitude and latitude. By this point, maps of the Moon were beginning to look very much like maps of Earth.\n\nFrom Opera Inedita (1775) by Johann Tobias Mayer. Courtesy the Linda Hall Library\n\nLunar maps quickly became more detailed and complex and, as they did, the lunar surface seemed more like an intelligible and accessible place. In the 19th century, the invention of photography took this project even further.\n\nBy the 1850s, astronomers were taking useful photographs of the Moon on collodion plates. As the photographic process improved, high resolution telescopic imagery of the lunar surface became more common and accessible. At the end of the century, the Lick Observatory in California published an atlas constructed from photographs obtained with their 36-inch refractor telescope.\n\nThe German astronomer Johann Krieger’s use of these images demonstrates both their power and their limitations. While photographs were useful, Krieger knew that he could see far more detail through his telescope. He sought to create his own maps and, rather than starting from scratch, he decided to draw the missing details on top of the Lick Observatory photographs. The result was a series of partially photographic representations of various craters. The effort may have killed him, however. Krieger died in 1902, apparently of exhaustion.\n\nIn their own way, these early telescopic observers and their successors explored the Moon long before the Apollo astronauts arrived. The astronomers measured mountains on another world. They broke down the Moon into smaller regions, mapping them in detail using photographic plates in combination with their own direct observations.\n\nWith the Moon now appearing more like a tangible place and potential destination every day, popular culture climbed on board. The novelist Jules Verne began describing trips to the Moon, and movies from the French film industry depicted a lunar landing.\n\nYet the Moon itself was relatively neglected in the scientific world. Mapping efforts did not extend to serious analysis of what was on the surface, much less how to get there. The Moon was still out of reach, and largely shrouded in mystery.\n\nAt the end of the Second World War, it still was not clear to scientists how lunar craters were formed. Many believed that craters were the result of volcanic activity or other internal processes in the Moon. The fact that craters were almost universally circular on the Moon was a crucial part of the lunar puzzle.\n\nAs late as the 1950s, scientists still did not know much for certain about the Moon’s geology, and disagreed over whether you could apply geology to it at all. It had maps and place names, but within the scientific community, it did not have a story.\n\nIn his book To a Rocky Moon (1993), the American geologist Don Wilhelms argued that two scientists provided the essential clues for unravelling the mystery. The first was Grove Karl Gilbert of the United States Geological Survey (USGS), who studied craters on Earth, and suggested (all the way back in 1893) that large impacts caused the craters on the Moon. The second was Ralph Baldwin, who worked with bomb fuses during the Second World War, and independently came to the same conclusion based partly on his observation of craters created by explosives. He theorised that objects might impact the Moon with enough force to create the circular craters we see today.\n\nThen the Cold War motivated an entirely new type of lunar exploration. Rocket launches became a way for the US and the Soviet Union to signal to one another – and to the world at large – information about their military capabilities and the economic systems that supported them. But these demonstrations needed payloads and destinations that signalled peaceful intentions. It was here that scientists found an opportunity.\n\nAt first, it was physicists who were most interested in sending experiments to space on rockets, but geoscientists gave them a new destination. In 1959, the chemist Harold Urey convinced NASA’s new assistant director for space sciences, Homer Newell, that the Moon would be an interesting scientific target. Robots could give the geoscientists the closer look they needed, and successful trips to the Moon would give the US points in the Cold War. The US and the Soviet Union both began launching a series of robotic probes to the Moon, but the Soviets took an early lead when their Luna probes made impacts and captured the first images of the far side of the Moon.\n\nThe frontier narrative that Apollo spawned was a story based on American westward expansion\n\nThe US responded with the Ranger and Surveyor programmes. Ranger probes were sent to impact the Moon’s surface, taking pictures and measurements on their way. The Surveyor probes soft-landed on the lunar surface, where they conducted experiments and took pictures. The Ranger and Surveyor images were immediately subject to intensive analysis, as Wilhelms explained, and were used to modify theories of lunar geology.\n\nFinally, with the Kennedy mandate gaining momentum, the preceding centuries of mapping – and the work of the scant few lunar geologists on the job – gained new significance. The mathematics of the Moon’s motion, the topography of its surface and the geological story of its evolution became directly and immediately relevant to national interests.\n\nLunar Orbiter became the first robotic probe designed to pave the way for human travel. NASA sent a series of five robots to orbit the Moon in 1966 and 1967 to search for landing sites. Engineers at Kodak equipped them with a clever mechanism to remotely transmit images taken on film. These images became the basis for new maps of unprecedented scope and detail.\n\nThe geoscientists went to work analysing the images, further altering their stories about the Moon’s history. But Kennedy’s deadline was rapidly approaching. NASA needed the geologists to focus on the selection of landing sites for the Apollo programme.\n\nThey used techniques that had roots in early topographical analysis to find flat spots that made for safe landing sites. And they used new geological narratives to find interesting sites that gave later Apollo missions more scientific value. The Egyptian American geologist Farouk El-Baz was one of the scientists who analysed Lunar Orbiter images for NASA. In his papers and the memoranda of the Apollo Site Selection Board, one can see, for the first time, theories of extraterrestrial geology being used to find destinations for practical human travel.\n\nEl-Baz and other geologists used the images from Lunar Orbiter to train the Apollo astronauts. They carefully studied the images, choosing landmarks that the astronauts could use to orient themselves once in orbit. The geologists of the USGS took the astronauts out to sites on Earth to prepare them for the work they would do on the Moon.\n\nSome of the astronauts were not particularly interested in the geological work, or did not consider it an especially significant part of their mission. But, to the geoscientists, it was everything. The photographs and samples returned from the Apollo missions provided essential ‘ground truth’ that helped scientists test the geological theories developed using views from above. By the 1980s, they had incorporated what they had learned into a consensus theory about the Moon’s geological history.\n\nIn Placing Outer Space, Messeri discusses the frontier narrative that Apollo spawned. It was a story based on American westward expansion, and it captivated the political, scientific and popular audiences of the time. Shoemaker lamented that NASA abandoned a frontier-style project of scientific exploration after the Apollo programme. Messeri cautions against carrying that type of narrative forward into our new epoch of space exploration, and suggests that we should seek more cooperative and constructive visions of our relationship with outer space.\n\nPerhaps there is an alternative suggested in this history. There are more ways to explore than simply having boots on the ground. NASA may not have fulfilled Shoemaker’s vision of human presence on the lunar frontier, yet it gave us a programme of scientific exploration that may have been difficult for people to imagine at the time – and continually rooted in the technologies and techniques being deployed in support of the Moon landings.\n\nRobotics advanced so rapidly in the decades following Apollo that satisfying levels of ground truth could be obtained by landers and rovers on other worlds. The robotic exploration of the solar system in the second half of the 20th century provides us with a new model of exploration, an idea that historians of exploration such as Stephen J Pyne have begun to investigate. NASA and its robots have, themselves, become symbols of a new type of exploration, with the values of curiosity and perseverance providing names for the flagship vessels of exploration in the 21st century. And as our expansion into space accelerates, our views of the past will continue to shape the explorations and narratives that come next.\n\n",
    "scrapedDate": "2025-03-03T13:30:12.077Z",
    "wordCount": 3214
  },
  {
    "title": "For Mary Midgley, the Western philosophical tradition is shaped by the fact that its greatest practitioners were bachelors",
    "category": "Thinkers and theories",
    "url": "https://aeon.co/essays/for-mary-midgley-philosophy-must-be-entangled-in-daily-life",
    "publishedDate": "",
    "content": "In the 1950s, the philosopher Mary Midgley did something that, according to philosophical orthodoxy, she wasn’t supposed to do. In a BBC radio script for the Third Programme (the precursor of BBC Radio 3), she dared to point out that almost all the canonical figures in philosophy’s history had been unmarried men. To most, Midgley’s attempt to discuss the relationship status of our most cherished philosophers would have been discarded as irrelevant, even scandalous. Surely ideas are timeless, outliving any particular philosopher and their marital status (or lack thereof)? At the time, Midgley’s comments were seen as inappropriate, and she received an aggressive response from her editor who questioned whether these were philosophical points at all. The script ‘Rings and Books’ was never broadcast – it was rejected on the grounds that it was a ‘trivial, irrelevant intrusion of domestic matters into intellectual life’.\n\nFor more than 60 years, Midgley’s script fell into oblivion. Eventually, it was discovered buried under a pile of dusty notes in Midgley’s home when she was in her 90s. Even though Midgley’s move in the would-be radio script was frowned upon, she wanted her listeners to reflect upon this ‘trivial’ fact of the matter – because, she felt, it was important. It told us something about philosophy itself. As it turned out, Midgley’s answer would anticipate an important insight later developed by some of the most famous feminist thinkers in the 2000s. But back in the 1950s, this went unnoticed.\n\nMidgley began her philosophical career as an undergraduate at the University of Oxford in the early years of the Second World War – she was there from 1938 to 1942. During this time, she got to know several women who went on to become prominent philosophers after the war, including Elizabeth Anscombe, Iris Murdoch (both were also regulars on the BBC) and Philippa Foot. All four had wildly different characters but became fast friends and lifelong philosophical interlocutors.\n\nAfter the war, Midgley began a doctorate degree at Somerville College, Oxford, studying Plotinus – best known as a defender of Platonic philosophy. But she quickly regretted her decision. Writing many years later in an article for The Guardian entitled ‘Proud Not to Be a Doctor’, Midgley describes her ‘luck’ in having dropped out of her PhD, even if that meant she would never receive ‘those magic letters that qualify one to teach in universities’ (namely, ‘P’, ‘h’ and ‘D’). Specifically, Midgley felt she had dodged a bullet and avoided falling into the philosophical narrow-mindedness that was fashionable in 1940s Oxford.\n\nMidgley didn’t begin publishing the books and articles she would later become known for until 59\n\nIn her career after Oxford, Midgley became a prolific writer, professional philosopher and, unlike many academics, popular with a public audience. Over the 99 years of her life, in more than 200 publications, she engaged in deep philosophical problems of identity, evil and human nature, as well as topics of public concern like environmentalism, religion and science (including a bust-up with Richard Dawkins who is said to still ‘feel the cruelties of Midgley’s original assault as painfully as a pea beneath the mattress of adulation on which he now reclines’). Midgley appeared on the radio and wrote articles for The Guardian with provocative titles like ‘Sneer Tactics’ and ‘Santa Pause’ (the former outlines the morally right circumstances in which one can make a joke at another’s expense, and the latter offers a Christmas wish for the correct use of the comma in contemporary literature). With all her philosophical faculties intact, Midgley published her final book, What Is Philosophy For?, just three weeks before her death in 2018 (the following year, she would have turned 100 and received a letter from Queen Elizabeth II).\n\nPublic and philosophical success came late in Midgley’s life, making her career path somewhat unusual compared with other philosophers, whose careers usually began when they left university. In fact, Midgley didn’t begin publishing the books and articles she would later become known for until the age of 59. Later, she explained that: ‘I needed time to think. Before then, I did not know what I thought clearly enough to want to go public with it.’ And the Midgley who wrote ‘Rings and Books’ in her 30s had not become the Midgley that many now recognise as a leading figure of 20th-century philosophy. When she made that ever-so scandalous observation – that most of our remembered philosophers were bachelors – Midgley did so from the living room of her town house in Newcastle, surrounded by her family, far beyond the walls of any university environment.\n\nMidgley’s observation that ‘none of [the greatest] philosophers … had any experience of living with women or children’ led her to make an important insight about the concept of philosophy itself – one that was perhaps available only to someone from outside the conventional academy. To the BBC, these were the words of a mother, not a philosopher. But from a contemporary point of view, it’s clear that Midgley was presenting an important insight – perhaps just one that the world wasn’t ready to hear in the 1950s.\n\nPhilosophy was human work for Midgley, something that humans need to do for life to go well. And this philosophy can go well or badly. In her words, philosophy is ‘the way in which we service the deep infrastructure of our lives – the patterns that are taken for granted because they have not really been questioned.’ Here philosophy is, at least in part, a personal practice of scrutinising our tightly held beliefs, patterns of thought and assumptions – something akin to critical thinking, perhaps.\n\nPhilosophers must question concepts at work in society, especially those that have become blocked or damaged (Midgley compares the work of a philosopher with that of a plumber). To return to Dawkins, the idea that human behaviour could possibly be determined by a so-called selfish gene struck Midgley as a dangerous ideology, not a piece of science. She saw the personification of the gene as reductive and encouraging of a ‘grand conclusion that the cosmos is both a random, meaningless jumble and also a callous, brutal fate-figure that manipulates us’. Midgley thought the idea of purpose wouldn’t go away, despite Dawkins’s sweeping rhetoric (it’s worth mentioning that Dawkins took the time to meticulously respond to each of her criticisms, which he took to be unfair caricatures of the concept of selfish genes). Without the lure of simple selfishness, Midgley thought, we can see human nature in its amazing complexity, including a vast array of motives, drives, needs and desires (one of which may indeed be selfishness).\n\nTrends in philosophy today may be a consequence of the fact that most of our ‘great’ philosophers were bachelors\n\nThe question of what philosophy is and what it is for preoccupied Midgley her entire life. And while ‘Rings and Books’ was deemed ‘trivial’ and ‘irrelevant’ by the BBC, it is as much a part of Midgley’s discussion of the very nature of philosophy as any of her other essays and books.\n\nLet’s begin with Midgley’s depiction of what she calls a ‘philosophical adolescent’:\n\nHere Midgley describes a fairly prevalent caricature of a philosopher; an individual who develops her ideas in private, free from the distraction of children, dependents, partners and pets. It is a lifestyle born of many privileges: quiet spaces, prolonged periods for deep thought, plentiful resources. Virginia Woolf noted that deep thinking requires ‘a room of one’s own’, and Midgley is making a similar point. But Midgley is also critical of this picture of the philosophical life. The lives and concerns of others do not seem to factor into it – all one needs is one’s own mind. But rather than hold up this model as one of a wizened intellectual, Midgley compares this kind of lifestyle to that of a teenager.\n\nThough certainly simplistic (and possibly insulting to actual teenagers), there is some truth in Midgley’s account of the lifestyles of many famous philosophers. Again, Woolf observed the struggle for women to find ‘a room of one’s own’ amid the battle to lead a life beyond domestic duties. This historical fact, we might think, is what prevented women from attaining the conditions deemed necessary for producing books that now ground entire traditions of philosophy, like René Descartes’s Meditations – or Immanuel Kant’s body of writing (Kant lived alone and virtually never left his hometown of Königsberg). If Midgley is right, many common trends in philosophy today may be the consequence of the fact that most of our ‘great’ philosophers were bachelors.\n\nMidgley is doing more here than simply pointing out an interesting piece of trivia. Her claim, developed in the rest of ‘Rings and Books’, is that this observation is of philosophical significance. She argues that the lifestyle of the ‘philosophical adolescent’ must have impacted their conception of what counts as ‘good’ or ‘proper’ philosophy. As she writes in her memoir The Owl of Minerva (2005):\n\nMidgley thought that by missing out on close meaningful relationships in their personal lives, many philosophers were encouraged to think of philosophy itself in a particular way – as the opposite of intimate and relational: abstract and remote. In short, Midgley thinks forms of social detachment may foster forms of philosophical detachment.\n\nIn the Western tradition, an over-abstract approach to philosophy is familiar. Bertrand Russell (who, in a striking departure from the trend that Midgley highlights, was married four times) argued that a good philosopher:\n\nLet’s consider an example offered by Midgley. Descartes is typically credited with, if not quite having founded, then as having done much to establish a long tradition of philosophical thinking called epistemology – the study of knowledge. In his Meditations, Descartes attempted to establish what, if anything, he can know for certain. Sitting down by his fireplace, he calls into question anything in his mind that might be subject to doubt. The existence of his friends, his family and everything else in the external world were all vulnerable – that is, doubtable – as was his own body. Descartes wondered: how can we really know that these things exist? Could they not be mere illusions, or dreams?\n\nMidgley sees us as entangled in our close relations with others\n\nDescartes concludes that the only thing I can know with certainty is that I am thinking (and therefore that I exist) – or, as philosophers know it, Cogito, ergo sum. Summarised by Midgley: ‘Here I am, said Descartes, a soul, an isolated thinker.’ But, despite its prominence in canonical histories of Western thought, Midgley was not overly impressed by this argument. In fact, she thinks this Cartesian move is born from the philosophical ‘adolescence’ that Descartes never grew up from. As she puts it:\n\nMidgley’s use of ‘normal’ here is no doubt reminiscent of a dated view about the nuclear family hailing from a time when many women carried out most of the domestic duties. But even aside from this, Midgley’s claim is important. As knowers, Midgley sees many of us (if not all) as already entangled in our close relations with others. From here, we can learn about the world, including the certain existence of other people. And Midgley thought that the experiences of those caring for children, in particular, could help illuminate the extent to which we really are entangled with one another’s existences.\n\nHer point is essentially this: certain philosophical problems seem important only because of the kinds of lives lived by the philosophers who thought about them. With Descartes still firmly in her crosshairs, Midgley points to the example of the so-called ‘problem of other minds’ – the epistemic problem of working out whether we can really know that anyone other than ourselves exists. Midgley argues that someone who has been pregnant, ie, had another someone living inside them, would never consider this an important question worthy of deep, philosophical contemplation. She writes:\n\nThe perspective of a pregnant person here is offered not as an ‘irrelevant’ or ‘trivial’ fact about a philosopher, but a special kind of advantage when it comes to knowledge. Over time, the parent is able to learn something through their body, something beyond the mere fact that I am a thinking thing. Today, notions of the embodied mind and proprioception are commonplace, but in the 1950s it was relatively rare to even mention any part of the body except the brain in a philosophical context.\n\nThe kinds of questions that Midgley was attempting to get her listeners to think about in ‘Rings and Books’ – questions like ‘How do our lives affect our philosophical views?’ – did not go away. Midgley’s script may have fallen into oblivion, but the spirit of her ‘trivial, irrelevant intrusion of domestic matters into intellectual life’ lives on. In contemporary philosophy, the idea that knowledge is situated within a particular ‘social location’ – meaning someone’s gender, age, ethnicity, background, even relationship status – is a central notion in feminist philosophy. And this is not a fringe area of philosophical scholarship. As an undergraduate in philosophy today, you’re as likely to study feminist philosophers such as Miranda Fricker, Kristie Dotson and Alison Wylie as you are Descartes. The idea that knowledge might be situated, known as ‘standpoint epistemology’, can be helpfully understood in contrast to the traditional approach to knowledge that Descartes is representative of.\n\nAccording to traditional epistemology, what can be said to ‘really’ exist must be objective – existing independently of a given knower or their body. This is why Descartes searched for knowledge accessible to any thinking person, a ‘view from nowhere’, available no matter a person’s social location. But feminist theories of knowledge argue that we shouldn’t think of objectivity in this narrow way. Instead, knowledge should be seen as socially entangled with our emotions, interests, relationships, background beliefs and bodies. And factors such as these should not be discarded as inappropriate or merely subjective.\n\nThe knowledge of those playing domestic roles in a household was bound up with tasks like ‘constantly lending eyes and hands to the child that requires them’, to use Midgley’s own example. But the perspective of an embodied person is so unfamiliar to the philosophical tradition that it has been often deemed an unphilosophical perspective – or, as Midgley put it, ‘irrational’. ‘Rings and Books’ tells us, rather radically for the 1950s, that the entry of women into the dominant ways in which we come to know about the world offers us new answers to the old questions that ‘great’ philosophers have grappled with – answers that Midgley argues are often ‘closer to the facts’.\n\n‘Rings and Books’ is a script about epistemic injustice that was never aired because of epistemic injustice\n\nPerhaps it was writing from outside the conventional academy that allowed Midgley to make observations and interventions that wouldn’t have been tolerated within – in this case, by drawing attention to the dominance of unmarried philosophers who seemed to neglect bodily connections with our nature as social animals.\n\nWhat of the suggestion that knowledge gained during pregnancy constitutes an irrational topic? Where does this kind of assumption come from?\n\nIn 2007, Fricker developed the term ‘epistemic injustice’ (epistemic meaning relating to knowledge). For her, epistemic injustice occurs when a person, or group of people, are excluded from practices that shape meaning and norms in society. Exclusion can happen on the basis of prejudicial beliefs, eg, about a person’s accent, skin colour or gender.\n\nConsider the following example. Our legal system creates and sustains our laws. Historically, women were not permitted to practise law and, until relatively recently, sexual acts that involve harm to women were not illegal. What’s more, women who reported such crimes were usually seen as overly emotional or said to be ‘flirting’ with their abusers. The concept of ‘sexual harassment’, and the terminology that would allow women to talk about it, didn’t even exist, making it harder for women themselves to understand their experiences as ones of real harm. In other words, women’s knowledge of their own harm was deemed trivial, not because it wasn’t real, but on the basis of sexist prejudices.\n\nIt is well known by now that the history of philosophy was dominated by men, but Midgley, like Fricker, points to something deeper. She shows that philosophy – in particular, epistemology – has been developed to favour certain ways of knowing. We’ve been taught that proper knowledge is available to us only once we’ve freed ourselves from our bodies and biases. Equipped with this picture, it is easy to conclude that knowledge generated in other, unfamiliar ways, is ‘unphilosophic’ or ‘improper’.\n\nBut, as Midgley highlights, women philosophers were not present in the development of this tradition. So, no wonder it seems unfamiliar. What Midgley offers us is a way of seeing assessments of certain knowledge as ‘irrational’ as a distinct harm – an assessment made on prejudiced beliefs about the identity of a person. As Midgley puts it: ‘[Any] account of human knowledge which women’s whole experience falsifies is inadequate and partial and capricious.’ In other words, we can find a case of epistemic injustice at work in our very concept of epistemology.\n\nLet’s now return to the place where this essay began – the BBC’s decision not to air ‘Rings and Books’ because it was a ‘trivial, irrelevant intrusion of domestic matters into intellectual life’. These words bring to mind an image of a confused person, trying unsuccessfully to contribute to a conversation for which they have no relevant knowledge. Of course, Midgley would go on to prove her editor wrong on this assessment. But what is striking is that this seems very clearly, in hindsight, to be a case of epistemic injustice at work. And so, what we find in the case of ‘Rings and Books’ is a script about epistemic injustice that was never aired because of epistemic injustice. In a sense, back in the 1950s, Midgley’s script was rejected for the very reason she wrote it in the first place: to highlight the inadequacy of the view that embodied ways of knowing do not constitute ‘proper’ ways of knowing. The irony of this would not have been lost on her.\n\n",
    "scrapedDate": "2025-03-03T13:30:13.762Z",
    "wordCount": 3031
  }
]